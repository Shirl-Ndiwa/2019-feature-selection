%  LaTeX support: latex@mdpi.com
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[remotesensing,article,accept,moreauthors,pdftex]{Definitions/mdpi}

% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal and change "submit" to "accept". The document class line would be, e.g., \documentclass[preprints,article,accept,moreauthors,pdftex]{mdpi}. This is especially recommended for submission to arXiv, where line numbers should be removed before posting. For preprints.org, the editorial staff will make this change immediately prior to posting.

%--------------------
% Class Options:
%--------------------

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%=================================================================
% MDPI internal commands
\firstpage{1}
\makeatletter
\setcounter{page}{\@firstpage}
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2021}
\copyrightyear{2021}
\externaleditor{Academic Editor: {Hanna Meyer}
} % For journal Automation, please change Academic Editor to "Communicated by"
\datereceived{12 September 2021}
\dateaccepted{23 November 2021}
\datepublished{}
\hreflink{https://doi.org/} % If needed use \linebreak
%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, soul, multirow, microtype, tikz, totcount, changepage, paracol, attrib, upgreek, cleveref, amsthm, hyphenat, natbib, hyperref, footmisc, url, geometry, newfloat, caption

% \usepackage[noadjust]{cite}
% \usepackage[hidelinks]{hyperref}
\usepackage[nolist]{acronym}

\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{adjustbox}

% \usepackage{supertabular}
\usepackage{supertabular,booktabs}

% line breaks in table cells
\newcommand{\specialcell}[2][l]{%
  \begin{tabular}[#1]{@{}l@{}}#2\end{tabular}}

\usepackage{csquotes}
\usepackage[utf8]{inputenc}
% enables custom font styles which are not available in IEEEtran by default
% https://tex.stackexchange.com/questions/331228/there-is-no-bold-text-italics-in-ieeetran

%\renewcommand{\sfdefault}{cmss}
%\renewcommand{\rmdefault}{cmr}
%\renewcommand{\ttdefault}{cmt}

\usepackage[detect-all]{siunitx}

\usepackage[pdftex]{graphicx}
% declare the path(s) where your graphic files are
\graphicspath{{./pdf/}{./jpg/}}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
\DeclareGraphicsExtensions{.pdf,.jpg,.png}

% suppress fancyhdr warnings
\setlength{\headheight}{23.90979pt}

% *** MATH PACKAGES ***
%
\usepackage{amsmath}
\newcommand*\mean[1]{\bar{#1}}

%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Monitoring Forest Health Using Hyperspectral Imagery: Does Feature Selection Improve the Performance of Machine-\linebreak Learning Techniques?}

% MDPI internal command: Title for citation in the left column
\TitleCitation{Monitoring Forest Health Using Hyperspectral Imagery: Does Feature Selection Improve the Performance of Machine-Learning Techniques? }

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0003-0748-6624} % Add \orcidA{} behind the author's name
\newcommand{\orcidauthorB}{0000-0001-7834-4717} % Add \orcidB{} behind the author's name
\newcommand{\orcidauthorC}{0000-0003-2567-8689} % Add \orcidB{} behind the author's name
\newcommand{\orcidauthorD}{0000-0002-6390-5873} % Add \orcidB{} behind the author's name
\newcommand{\orcidauthorE}{0000-0001-6002-6980} % Add \orcidB{} behind the author's name
\newcommand{\orcidauthorF}{0000-0001-6640-679X} % Add \orcidB{} behind the author's name

% Authors, for the paper (add full first names)
\Author{\hl{Patrick Schratz} %MDPI: Please carefully check the accuracy of names and affiliations.
 $^{1,}$*\orcidA{}, \hl{Jannes Muenchow} %MDPI: This author is different from redmine, please confirm.
 $^{1}$\orcidB{}, Eugenia Iturritxa $^{2}$\orcidD{}, José Cortés $^{1}$\orcidC{}, Bernd Bischl $^{3}$\orcidE{} and~Alexander~Brenning $^{1}$\orcidF{}}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Patrick Schratz, Jannes Muenchow, Eugenia Iturritxa, José Cortés, Bernd Bischl and Alexander Brenning}

% MDPI internal command: Authors, for citation in the left column
\AuthorCitation{Schratz, P.; Muenchow, J.; Iturritxa, E.; Cortés, J.; Bischl, B.; Brenning, A.}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad \hl{GIScience Group,} %MDPI: Please check if the addresses are arranged from small to large. The same to aff. 3.
 Department of Geography, Friedrich Schiller University Jena, \hl{Germany;} %MDPI: please add city and post code. The same to affs. 2 and 3.
 malnamalja@gmx.de~(J.M.); jose.cortes@uni-jena.de~(J.C.); alexander.brenning@uni-jena.de~(A.B.)\\
$^{2}$ \quad \hl{NEIKER} %MDPI: please check if this address is correct.
 Tecnalia, \hl{Spain;} eiturritxa@neiker.eus\\
$^{3}$ \quad \hl{Computational Statistics Group,} Department of Statistics, Ludwig-Maximilians-Universität München, \hl{Germany;} bernd.bischl@stat.uni-muenchen.de
}

% Contact information of the corresponding author
\corres{Correspondence: patrick.schratz@uni-jena.de}

% Current address and/or shared authorship
% \firstnote{Current address: Affiliation 3}
% \secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes

%\simplesumm{} % Simple summary

% Abstract (Do not insert blank lines, i.e. \\)
\abstract{This study analyzed highly correlated, feature-rich datasets from hyperspectral remote sensing data using multiple statistical and machine-learning methods.
The effect of filter-based feature selection methods on predictive performance was compared.
In addition, the effect of multiple expert-based and data-driven feature sets, derived from the reflectance data, was investigated.
Defoliation of trees (\%), derived from in situ measurements from fall 2016, was modeled as a function of reflectance.
Variable importance was assessed using permutation-based feature importance.
Overall, the support vector machine (SVM) outperformed other algorithms, such as random forest (RF), extreme gradient boosting (XGBoost), and lasso (L1) and ridge (L2) regressions by at least three percentage points.
The combination of certain feature sets showed small increases in predictive performance, while no substantial differences between individual feature sets were observed.
For some combinations of learners and feature sets, filter methods achieved better predictive performances than using no feature selection.
Ensemble filters did not have a substantial impact on performance.
The most important features were located around the red edge.
Additional features in the near-infrared region (800 nm--1000 nm) were also essential to achieve the overall best performances.
Filter methods have the potential to be helpful in high-dimensional situations and are able to improve the interpretation of feature effects in fitted models, which is an essential constraint in environmental modeling studies.
Nevertheless, more training data and replication in similar benchmarking studies are needed to be able to generalize the results.}

% Keywords
\keyword{hyperspectral imagery; forest health monitoring; machine learning; feature selection; model comparison}

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

\begin{acronym}

	% geringerer Zeilenabstand

	%\setlength{\itemsep}{-\parsep}
	\acro{AGB}{above-ground biomass}
	\acro{ALE}{accumulated local effects}
	\acro{ALS}{airborne laser scanning}
	\acro{ANN}{artificial neural network}
	\acro{AUROC}{area under the receiver operating characteristics curve}
	\acro{BRT}{boosted regression trees}
	\acro{CART}{classification and regression trees}
	\acro{CNN}{convolutional neural networks}
	\acro{CV}{cross-validation}
	\acro{DAP}{digital aerial photogrammetry}
	\acro{ENM}{environmental niche modeling}
	\acro{FFS}{forward feature selection}
	\acro{FPR}{false-positive rate}
	\acro{FS}{feature selection}
	\acro{GAM}{generalized additive model}
	\acro{GBM}{gradient boosting machine}
	\acro{GLM}{generalized linear model}
	\acro{ICGC}{Institut Cartografic i Geologic de Catalunya}
	\acro{IQR}{interquartile range}
	\acro{LiDAR}{light detection and ranging}
	\acro{LOWESS}{locally weighted scatterplot smoothing}
	\acro{MARS}{multivariate adaptive regression splines}
	\acro{MBO}{model-based optimization}
	\acro{MEM}{maximum entropy model}
	\acro{ML}{machine learning}
	\acro{NDII}{normalized difference infrared index}
	\acro{NDMI}{normalized difference moisture index}
	\acro{NIR}{near-infrared}
	\acro{NRI}{normalized ratio index}
	\acro{OLS}{ordinary least squares}
	\acro{OMNBR}{optimized multiple narrow-band reflectance}
	\acro{PCA}{principal component analysis}
	\acro{PDP}{partial dependence plots}
	\acro{PISR}{potential incoming solar radiation}
	\acro{PLS}{partial least squares}
	\acro{POV}{proportion of variance explained}
	\acro{RBF}{radial basis function}
	\acro{RF}{random forest}
	\acro{RMSE}{root mean square error}
	\acro{RR}{ridge regression}
	\acro{RSS}{residual sum of squares}
	\acro{SAR}{synthetic aperture radar}
	\acro{SDM}{species distribution modeling}
	\acro{SMBO}{sequential model-based optimization}
	\acro{SVM}{support vector machine}
	\acro{TPR}{true-positive rate}
	\acro{VI}{vegetation index}
	\acro{XGBoost}{extreme gradient boosting}
	\acro{PC}{principal component}
\end{acronym}

\begin{document}
\section{Introduction}

The use of \ac{ML} algorithms for analyzing remote sensing data has seen a huge increase in the last decade~\cite{lary2016}.
This coincided with the increased availability of remote sensing imagery, especially since the launch of the first Sentinel satellite in the year 2014.
At the same time, the implementation and usability of learning algorithms has been greatly simplified with many contributions from the open-source community.
Scientists can nowadays process large amounts of (environmental) information with relative ease using various learning algorithms.
This makes it possible to easily extend benchmark comparison matrices of studies in a semi-automated way, possibly stumbling upon unexpected findings, such as process settings, that would not have been explored otherwise~\cite{ma2015}.

% link to forest health (defoliation) analysis and show exemplary studies

ML methods in combination with remote sensing data are used in many environmental fields, such as vegetation cover analysis and forest carbon storage mapping~\cite{mascaro2014, urban2018}.
The ability to predict these environmental variables in unknown regions qualifies ML algorithms as a helpful tool for such environmental analyses.
One aspect of this research field is to enhance the understanding of biotic and abiotic stress triggers, for example, by analyzing tree defoliation~\cite{hawrylo2018}.
Defoliation is known to be a proxy for pathogen and insect damage~\cite{pollastrini2016}.
While common symptoms are observable across species, some effects and their degree of severity are species-specific~\cite{gottardini2020}.
Defoliation has also been shown to increase predisposition of tree death from secondary biotic factors up to ten years after the actual defoliation event~\cite{oliva2016}.
Other approaches for analyzing forest health include change detection~\cite{zhang2016} or describing the current health status of forests on a stand level~\cite{townsend2012}.

% introduce veg indices
Vegetation indices have shown the potential to provide valuable information when analyzing forest health~\cite{jiang2014, adamczyk2015}.
Most vegetation indices were developed with the aim of being sensitive to changes in specific wavelength regions, serving proxies for underlying plant physiological processes.
In some cases, indices developed for different purposes than the one to be analyzed (e.g., defoliation of pine trees) may help to explain complex underlying relationships that are not obvious at first.
This emphasizes the need to extract as much information as possible from the available input data to generate promising features that can help in improving our understanding of the modeled relationship~\cite{thenkabail2018}.
A less known index type that can be derived from spectral information is the \ac{NRI}.
In contrast to most vegetation indices, \ac{NRI}s do not use an expert-based formula following environmental heuristics; instead, they make use of a data-driven feature engineering approach by combining (all possible) combinations of spectral bands~\cite{thenkabail2000}.
When working with hyperspectral data, thousands of \ac{NRI} features can be derived this way.

% mention why FS is used / important
Even though \ac{ML} algorithms are capable of handling highly correlated input variables, model fitting becomes computationally more demanding and model interpretation more challenging.
Feature selection approaches can help to address this issue, reducing possible noise in the feature space, simplifying model interpretability, and possibly enhancing predictive performance~\cite{cai2018}.
Instead of using wrapper feature selection methods, which add a substantial overhead to a nested model optimization approach, especially for datasets with many features, this study focuses on (ensemble) filter methods, which can be directly integrated into the hyperparameter optimization step during model construction.

% clarify the focus of this study
Overall, this study aims to show how high-dimensional datasets can be handled effectively with ML methods while still allowing the interpretation of the fitted models.
The predictive power of non-linear methods and their ability to handle highly correlated predictors is combined with common and new approaches for assessing feature importance and feature effects.
However, this study focuses mainly on investigating the effects of filter methods and feature set types on predictive performance rather than interpreting individual feature effects.

Considering these opportunities and challenges, the research questions of this study are as follows:

\begin{itemize}

	\item How do different feature selection methods influence the predictive performance of ML models of the defoliation of trees?

	\item Do different (environmental) feature sets show differences in performance?

	\item Can predictive performance be substantially improved by combining feature sets?

	\item Which features are most important and how can these be interpreted in this context?

\end{itemize}

% Explain what is new in this study and link to methodology
In recent years, various studies that have used hyperspectral data to analyze pest/fungi infections on trees have been published.
Pinto et al. \citep{pinto2020} successfully used hyperspectral imagery to characterize pest infections on peanut leaves using random forest, while Yu~et~al.~\cite{yu2021} aimed to detect pine wilt disease in pine plots in China using vegetation indices derived from hyperspectral data.
Other studies which applied hyperspectral data for forest health monitoring are~\cite{lin2014,kayet2019,dash2017}.
Building upon these successful applications of hyperspectral remote sensing usage in the field of leaf and tree health monitoring, this work analyzes tree defoliation in northern Spain using airborne hyperspectral data.
The methodology of this study uses ML methods in combination with feature selection and hyperparameter tuning.
In addition, feature importance was analyzed.
Incorporating the idea of creating data-driven \ac{NRI}s, this study also discusses the practical problems of high dimensionality in environmental modeling~\cite{trunk1979, xu2016}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Materials and Methods}

\subsection{Data and Study Area}

Airborne hyperspectral data with a spatial resolution of one meter and 126 spectral bands were available for four Monterey Pine (\textit{Pinus radiata D. Don}) plantations in northern Spain.
The trees in the plots suffer from infections with pathogens such as \textit{Diplodia sapinea (Fr.) Fuckel}, \textit{Fusarium circinatum Nirenberg \& O'Donnell}, \textit{Armillaria mellea (Vahl) P. Kumm}, \textit{Heterobasidion annosum (Fr.) Bref}, \textit{Lecanosticta acicola (Thüm) Syd.}, and \textit{Dothistroma septosporum (Dorogin) M. Morelet} causing (among others) needle blight, pitch canker, and root diseases~\cite{mesanza2016, iturritxa2017}.
The first two fungi are mainly responsible for the foliation loss of the trees analyzed in this study~\cite{iturritxa2014}.
In situ measurements of defoliation of trees (serving as a proxy for tree health) were collected by visual inspection by experts. \textit{Defoliation} in percent was used as the response variable (Figure~\ref{fig:defol-distr}).

It is assumed that the fungi infect the trees through open wounds, possibly caused by previous hail damage~\cite{iturritxa2014}.
The dieback of these trees, which are mainly used as timber, causes substantial economic losses~\cite{ganley2009}.

\begin{figure}[H]
	%\centering
	%\begin{center}
		\includegraphics[width=0.7\textwidth] {defoliation-distribution-plot-1.pdf}
		\caption{Response variable ``defoliation of trees'' for plots~Laukiz1, Laukiz2, Luiando, and Oiartzun. \texttt{n} is the total number of trees in each plot, and $\bar{x}$ the mean defoliation. Values for Laukiz1, Luiando, and Oiartzun were observed in 5\% intervals; for Laukiz2, defoliation was observed at multiple heights and then averaged, leading to smaller defoliation differences than 5\%.}\label{fig:defol-distr}
	%\end{center}
\end{figure}

\subsubsection{In Situ Data}

The \textit{Pinus radiata} plots of this study, namely Laukiz1, Laukiz2, Luiando, and Oiartzun, are located in the northern part of the Basque Country (Figure~\ref{fig:study_area}).
Oiartzun has the largest number of observations ($n = 559$ trees), while Laukiz2 is the largest in area (1.44~ha).
All plots besides Luiando are located within 100~km from the coast (Figure~\ref{fig:study_area}).
A total of 1808~observations are available (Laukiz1: 559, Laukiz2: 451, Luiando: 301, Oiartzun: 497).
Field surveys were conducted in September 2016 by experienced forest pathologists.
Defoliation was measured in 5\% steps through visual inspection with the help of a score card.
For Laukiz2, values at three height levels (bottom, mid, and top) were available and averaged into an overall defoliation value, resulting in values that are not multiples of 5\% (e.g., 8.33\%).
The magnitude of human observer errors in such surveys, including the present one, is not precisely known and has being discussed for many years~\cite{innes1993}.
\hl{Ref.}%MDPI: Reference citation can't be in the begining of sentence, we add Ref., please confirm.
~\cite{maclean1982} estimated human observer errors in defoliation surveys to range between 7\% and 18\%.

% start a new page without indent 4.6cm
\vspace{-12pt}
\end{paracol}
\nointerlineskip
\begin{figure} [H]
\widefigure
	%\begin{center}
		%\centering
		\includegraphics[width=\textwidth] {study-area-hyperspectral.pdf}
		\caption{Study area maps showing information about location, size, and spatial distribution of trees for all plots (Laukiz1, Laukiz2, Luiando, and Oiartzun). The background maps give a visual impression of the individual plot area but do not necessarily represent the plot's state during data acquisition.}\label{fig:study_area}
	%\end{center}
\end{figure}
\begin{paracol}{2}
%\linenumbers
\switchcolumn


%\begin{figure}[H]
%	%\begin{center}
%		\includegraphics[width=0.7\textwidth] {defol-grid-3000px.jpg}
%		\caption{Example trees showing different levels of defoliation as interpreted by the surveyor: 10 \% (top-left), 20 \% (top-right), 40 \% (bottom-left), 60-70 \% (bottom-right).}
%		\label{fig:defol-trees}
%	%\end{center}
%\end{figure}

\vspace{-12pt}
\subsubsection{Hyperspectral Data}

The airborne hyperspectral data were acquired by an AISA EAGLE-II sensor during two flight campaigns on 28 September and 5 October 2016 at noon.
All preprocessing steps (geometric, radiometric, and atmospheric) were conducted by the \ac{ICGC}.
The first four bands were corrupted, leaving 122 bands with valid information.
Additional metadata information is available in Table~\ref{tab:hyperparameter_metadata}.

% parameter limits

\begin{specialtable}[H]
\setlength{\tabcolsep}{11.3mm}
	%\centering
	\caption[t]{Specifications of hyperspectral data.}
	\begingroup
	\begin{tabular}{ll}
	\toprule
		\textbf{Characteristic}         & \textbf{Value}                               \\\midrule

		Geometric resolution   & 1~m                                 \\
		Radiometric resolution & 12 bit                              \\
		Spectral resolution    & 126 bands (404.08~nm--996.31~nm) \\
		Correction:            & Radiometric, geometric, atmospheric\\\bottomrule
	\end{tabular}
	\endgroup\label{tab:hyperparameter_metadata}
\end{specialtable}

\subsection{Derivation of Indices}

To use the full potential of the hyperspectral data, all possible vegetation indices supported by the R package \texttt{hsdar} (89 in total) as well as all possible \ac{NRI} combinations were calculated.
NRIs follow the \ac{OMNBR} concept of data-driven information extraction from narrow-band indices of hyperspectral data~\cite{thenkabail2000,thenkabail2018}.
While various index formulations, such as band ratios or normalized ratios, are available, indices involving the same bands are strongly correlated.
Since the widely\ used NDVI index belongs to the group of normalized ratio indices (NRIs), which are implemented in the \texttt{hsdar} R package, we used the following normalized difference index (NDI) formula to combine all pairs of reflectances:
\begin{equation}
	NRI_{i,j} = \frac{band_{i} - band_{j}}{band_{i} + band_{j}}
\end{equation}
where \(i\) and \(j\) are the respective band numbers.

%\bigbreak{}

To account for geometric offsets within the hyperspectral data, which were reported by \ac{ICGC} to be potentially up to one meter, a buffer of one meter radius around the centroid of each tree was used when extracting the reflectance values.
A pixel was considered to fall into a tree's buffer zone if the centroid of the respective pixel was touched by the buffer.
The pixel values within a buffer zone were averaged and formed the final reflectance value of a single tree, and they were used as the base information to derive all additional feature sets.
In total, \(\frac{121*122}{2} = 7381\) NRIs were calculated.

\subsection{Feature Selection}

High-dimensional, feature-rich datasets come with several challenges for both model fitting and evaluation:

\begin{itemize}
	\item Model fitting times increase.
	\item Noise is possibly introduced into models by highly correlated variables~\cite{johnstoneiainm.2009}.
	\item Model interpretation and prediction become more challenging~\cite{johnstoneiainm.2009}.
\end{itemize}

To reduce the feature space of a dataset, several conceptually differing approaches exist: wrapper methods, filters, penalization methods (lasso and ridge), and \ac{PCA}~\cite{bommert2020, das2001, guyon2003, jolliffe2016}.
In contrast to wrapper methods, filters have a much lower computational cost, and their tuning can be added to the hyperparameter optimization step.
In addition, filters are less known than wrapper methods, and, in recent years, ensemble filters, which have shown promising results compared to single filter algorithms, were introduced~\cite{drotar2017}.
These two points mainly led to the decision to focus on filter methods for this work and evaluate their effectiveness on highly\ correlated, high-dimensional datasets.
Due to this focus, only this subgroup of feature selection methods is be introduced in greater detail in the following sections.

\subsubsection{Filter Methods}

% Filter methods
The concept of filters originates from the idea of ranking features following a score calculated by an algorithm~\cite{guyon2003}.
Some filter methods can only deal with specific types of variables (e.g., numeric or nominal).
Filters only rank features; they do not decide which covariates to drop or keep~\cite{drotar2015}.
The decision of which features to keep for model fitting can be integrated into the optimization phase during model fitting, along with hyperparameter tuning.
Thus, the number of top-ranked features to be included in the model is treated as an additional hyperparameter of the model.
This hyperparameter is tuned to optimize the model's performance.

% Ensemble filter methods
Beyond the use of individual filter methods to rank and select features, recent studies have shown that combining several filters by using statistical operations such as "minimum" or "mean" may enhance the predictive performance of the resulting models, especially when applied to multiple datasets~\cite{abeel2010, drotar2017}.
This approach is referred to as "ensemble filtering"~\cite{dietterich2000}.
Ensemble filters align with the recent rise of the ensemble approach in ML, which uses the idea of stacking to combine the predictions of multiple models, aiming to enhance predictive performance~\cite{polikar2012, feurer2015, bolon-canedo2019}.
In this work, the Borda ensemble filter was used~\cite{drotar2017}.
Its overall feature order is determined by the sum of filter ranks of all individual filters in the~ensemble.

% Ensuring a fair weighting in the ensemble
Filter methods can be categorized based on three binary criteria: multivariate or univariate feature use, correlation or entropy-based importance weighting, and linear and non-linear filter methodology.
Care needs to be taken to not weigh certain classes more than others in the ensemble, as, otherwise, the ranking will be biased.
In this study, this was taken care of by checking the rank correlations (Spearman's correlation) of the generated feature rankings of all methods against each other.
If filter pairs showed a correlation of 0.9 or higher, only one of the two was included in the ensemble filter, selecting it at random.

\subsubsection{Description of Used Filter Methods}

Filter methods can be classified as follows (Table~\ref{tab:filter-methods}):

\begin{itemize}
	\item Univariate/multivariate (scoring based on a single variable/multiple variables).
	\item Linear/non-linear (usage of linear/non-linear calculations).
	\item Entropy/correlation (scoring based on derivations of entropy or correlation-based approaches).
\end{itemize}
\vspace{-6pt}

% filter methods
\begin{specialtable}[H]
\setlength{\tabcolsep}{5.2mm}
	%\centering
	\caption{List of filter methods used in this work, their categorization, and scientific reference.}
	\label{tab:filter-methods}
	\begingroup\footnotesize
%	\begin{adjustbox}{width={0.7\textwidth},totalheight={\textheight},keepaspectratio}
		\begin{tabular}{lll}
				\toprule
			\textbf{Name}                                         & \textbf{Group} & \textbf{Ref. }              \\\midrule

			Linear correlation (Pearson)                 & univariate, linear, correlation   &~\cite{pearson1901} \\
			Information gain                             & univariate, non-linear, entropy   &~\cite{quinlan1986} \\
			Minimum redundancy, maximum relevance        & multivariate, non-linear, entropy &~\cite{zhao2013}    \\
			Carscore                                     & multivariate, linear, correlation &~\cite{zuber2011}   \\
			Relief                                       & multivariate, linear, entropy     &~\cite{kira1992}    \\
			Conditional minimal information maximization & multivariate, linear, entropy     &~\cite{fleuret2004}\\\bottomrule
		\end{tabular}
%	\end{adjustbox}
	\endgroup
\end{specialtable}

The filter "Information Gain" in its original form is only defined for nominal response variables:
\begin{equation}
	H(Class) + H(Attribute) - H(Class, Attribute)
\end{equation}
where \(H\) is the conditional entropy of the response variable (class or Y) and the feature (attribute or X).
$H(X)$ is Shannon's entropy~\cite{shannon1948} for a variable X, and $H(X, Y)$ is a joint Shannon's entropy for a variable X with a condition to Y.
$H(X)$ itself is defined as
\begin{equation}
	H(X) = - \sum_{i=1}^{n} P(x_i)\log_bP(x_i)
\end{equation}
where $b$ is the base of the logarithm used, most commonly 2.

In order to use this method with a numeric response (percentage defoliation of trees), the variable was discretized into equal bins \texttt{\(n_{bin}\) = 10} and treated as a categorical variable.

\subsection{Benchmarking Design}

\subsubsection{Algorithms}

The following learners were used in this work:

\begin{itemize}
	\item  Extreme gradient boosting (XGBoost);
	\item  Random forest (RF);
	\item  Penalized regression (with L1/lasso and L2/ridge penalties);
	\item  Support vector machine (SVM, radial basis function Kernel);
	\item  Featureless learner.
\end{itemize}

RF and SVM are well-established algorithms and widely used in environmental remote sensing.
Extreme gradient boosting (commonly abbreviated as XGBoost) has shown promising results in benchmarking studies in recent years.
Penalized regression is a statistical modeling technique capable of dealing with highly\ correlated covariates by penalizing the model coefficients~\cite{hastie2001}.
Common penalties are "lasso" (L1) and "ridge" (L2).
Ridge regression does not remove variables from the model (penalization to zero), but it shrinks them towards zero, keeping them in the model.
A featureless learner was included for a baseline comparison.

In total, the benchmarking grid consisted of 156 experiments (6 feature sets $\times$ 3~ML algorithms $\times$ 8 feature-selection methods and for the L1/L2 models, 6 feature sets~$\times$~2~models.
The selected hyperparameter settings are shown in Appendix~\ref{appendixA} Table~\ref{tab:hyperparameter_limits}.
All code and data are included in the research compendium of this study (\url{https://doi.org/10.5281/zenodo.2635403}, \hl{accessed on).} %MDPI: Please add access date after the URL; like “http://xxxxxxx (access on 8 January 2021)”, please check all URL in manuscript and add the specific date.


\subsubsection{Feature Sets}

Three feature sets were used in this study, each representing a different approach to feature engineering:

\begin{itemize}
	\item The raw hyperspectral band information (HR): no feature engineering %chktex 13;
	\item Vegetation indices (\ac{VI}s): expert-based feature engineering;
	\item Normalized ratio indices (\ac{NRI}s): data-driven feature engineering.
\end{itemize}

The idea of splitting the features into different sets originated from the question of whether feature-engineered indices derived \textls[-5]{from reflectance values have a positive effect on model performance.
Peña et al. 2017~\cite{pena2017} is an exemplary study that used this approach in a spectro-temporal setting.}
Benchmarking learners on these feature sets while keeping all other variables, such as model type, tuning strategy, and a partitioning method, fixed makes it possible to draw conclusions on their individual impact.
Each feature set has distinct capabilities that differentiate it from the others.
This can have both a positive and negative effect on the resulting performance, which is one question this study aims to explore.
For example, feature set VI misses certain parts of the spectral range, as the chosen indices only use specific spectral bands.
Feature set NRI will introduce highly\ correlated features, for which some algorithms may be more suitable than others.

In addition to these individual feature sets, the following combinations of feature sets were also compared:

\begin{itemize}
	\item HR + VI %chktex 13;
	\item HR + NRI;
	\item HR + VI + NRI.
\end{itemize}

Some individual features were removed before using the datasets for modeling when being numerically equivalent to another feature based on the pairwise correlation being greater than $1 - 10^{-10}$.
This preprocessing step reduced the number of covariates for feature set~VI to 86 (from 89).
This decision was made to prevent numerical issues that may occur in the subsequent tuning, filtering, and model fitting steps when offering features with a pairwise correlation of (almost) one.
The remaining features were then used as input for the filter-based feature selection within the CV.


\subsubsection{Hyperparameter Optimization}

Hyperparameters were tuned using \ac{MBO} within a nested spatial \ac{CV}~\cite{mlrmbo, binder2020, schratz2019}.
In MBO, first, \textit{n} hyperparameter settings are randomly chosen from a user-defined search space.
After these \textit{n} settings have been evaluated, one new setting, which is evaluated next, is proposed by a fitted surrogate model (by default, a kriging method).
This strategy continues until a user-defined stopping criterion is satisfied~\cite{hutter2011, jones1998}.

In this work, an initial design of 30 randomly drawn hyperparameter settings in combination with a stopping criterion of 70 iterations was used, resulting in a total budget of 100 evaluated hyperparameter settings per fold.
The advantage of this tuning approach is a substantial reduction of the tuning budget that is required to find a setting close to the global optimization minimum.
\ac{MBO} may outperform methods that do not use information from previous iterations, such as random search or grid search~\cite{bergstra2012}.
Tuning ranges used in this work are shown in Table~\ref{tab:hyperparameter_limits}.

To optimize the number of features used for model fitting, the percentage of features was added as a hyperparameter during the optimization stage~\cite{binder2020}.
For \ac{PCA}, the number of principal components was tuned.
%The RF hyperparameter \texttt{\(m_{try}\)} was re-expressed as $m_{try} = p^x_{try}$.
The RF hyperparameter \texttt{\(m_{try}\)} was re-expressed as $m_\textrm{try} = p_\textrm{sel}^t$, a function of the number of selected features, $p_\textrm{sel}$.
It was thus tuned on a logarithmic scale by varying $t$ between 0 (i.e., $m_\textrm{try} = 1$) and 0.5 (i.e., $m_\textrm{try}=\sqrt{p_\textrm{sel}}$).
This was necessary to ensure that \texttt{\(m_{try}\)} did not exceed the number of features available after optimizing the feature percentage during tuning.

\subsubsection{Spatial Resampling}

A spatial nested cross-validation on the plot level was chosen to account for spatial autocorrelation within the plots and assess model transferability to different plots~\cite{schratz2019, sperrorest}.
The \ac{RMSE} was chosen as the error measure.
Each plot served as one cross-validation fold, resulting in four iterations in total.
The inner level of cross-validation for hyperparameter tuning also used plot-level cross-validation.

\subsection{Feature Importance and Feature Effects}

Estimating feature importance for datasets with highly correlated features is a challenging task for which numerous model-specific and model-agnostic approaches exist~\cite{friedman2001, hastie2001, greenwell2018}.
The strong correlations among features make it challenging to calculate an unbiased estimate for single features~\cite{molnar2019}.
Methods such as \ac{PDP} or permutation-based approaches may produce unreliable estimates in such scenarios because unrealistic combinations of feature values are created~\cite{molnar2019}.
The development of robust methods that enable an unbiased estimation of feature importance for highly correlated variables is subject to current research~\cite{brenning2021}.

In this work, permutation-based feature importance was calculated to estimate feature importance or effects~\cite{apley2019}.
With the limitations in mind when applied to correlated features, the aim was to get a general overview of the feature importance of the hyperspectral bands while trying to avoid an over-interpretation of results.
The best-performing algorithm on the HR task (i.e., SVM) was used for the feature importance calculation.

%\subsection{Linking feature importance to wavelength regions}

To facilitate interpretation, the ten most important indices of the best performing models using feature sets HR and VI were linked to the spectral regions of the hyperspectral data that went into their calculation.
The aim was to visualize the most important features along the spectral curve of the plots to better understand which spectral regions were most important for the model.

\subsection{Research Compendium}

All tasks of this study were conducted using the open-source statistical programming language R~\cite{rcoreteam2019}.
A complete list of all R packages used in this study can be found in the linked repositories mentioned in the next paragraph.
Due to space limitations, only selected packages with high impact on this work are explicitly cited.

The algorithm implementations of the following packages were used: xgboost~\cite{chen2016} (\textit{\hl{Extreme Gradient Boosting}}), %MDPI: Is the italic necessary?
 kernlab~\cite{kernlab} (support vector machine)m and glmnet~\cite{glmnet} (penalized regression).
The filter implementations of the following packages were used: praznik~\cite{praznik} and FSelectorRcpp~\cite{fselectorrcpp}.
Package mlr~\cite{mlr} was used for all modeling related steps, and
drake~\cite{drake} was used for structuring the work and for reproducibility.
This study is available as a research compendium on Zenodo (\url{10.5281/zenodo.2635403}, \hl{accessed on).} %MDPI: Please add access date after the URL; like “http://xxxxxxx (access on 8 January 2021)”, please check all URL in manuscript and add the specific date.
Apart from the availability of code and manuscript sources, a static webpage is available at (\url{https://pat-s.github.io/2019-feature-selection}, \hl{accessed on),} %MDPI: Please add access date after the URL; like “http://xxxxxxx (access on 8 January 2021)”, please check all URL in manuscript and add the specific date.
which includes additional side analyses that were carried out during the creation of this study.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

\subsection{Principal Component Analysis of Feature Sets}

PCA was used to assess the complexity of the three feature sets.
Depending on the feature set, 95\% of the variance is explained by two (HR), twelve (VI), and 42 (NRI) \ac{PC}s.
HR features are therefore highly redundant, while the applied feature transformations enrich the data set, at least from an exploratory linear perspective.

\subsection{Predictive Performance}

% - RMSE around 27
% - MBO tuned penalized methods are substantially worse
Overall, the response variable \enquote{tree defoliation} could be modeled with an \ac{RMSE} of 28 percentage points (p.p.) (Figure~\ref{fig:perf-result}).
SVM showed almost no differences in RMSE across feature sets whereas other learners (RF, SVM, XGBoost, lasso and ridge) differed up to five p.p. (Figure~\ref{fig:perf-result}).
SVM showed the best overall performance with a mean difference of around three p.p. to the next best model (XGBoost) (\hl{Table}%MDPI: we adjust the order of Tables 3, 4 and Tables 5, 6, please confirm.
~\ref{tab:best-learner-perf}).
Performance differences between test folds were large: Predicting on Luiando resulted in an RMSE of 9.0 p.p. for learner SVM (without filter) but up to 54.3 p.p. when testing on Laukiz2 (Table~\ref{tab:svm-single-fold-perf}).

% feature sets
% - no substantial difference between feature sets
% - combining feature sets has only small impact
% - expert based feature sets show no better result than data-driven ones
% - penalized methods seem to have problems with VI feature set
The combination of feature sets showed small increases in performance for some learners.
XGBoost scored slightly better on the combined datasets HR-NRI, NRI-VI, and HR-NRI-VI compared to their standalone variants (NRI and VI) (Figure~\ref{fig:perf-result}).
However, the best performances for RF and XGBoost were scored on NRI and HR, respectively.
RF showed a substantial performance increase when using only NRI compared to all other feature sets, whereas for XGBoost, the worst performances were associated with the VI- and NRI-only feature sets (Figure~\ref{fig:perf-result}).

% learner
% - SVM Info Gain is the overall winner
% - Combined sets do not help for XBoost and Ridge
% - RF NRI quite strong
SVM combined with the \enquote{Information Gain} filter achieved the best overall performance (RMSE of 27.915 p.p.) (Table~\ref{tab:perf-top-10}).
%L1 penalized regression showed slightly better results in three out of six tasks when being optimized using the package internal 10-fold CV compared to MBO (RMSE of 50 vs 58).
Regressions with ridge (L2) and lasso (L1) penalties showed their best performances on the NRI feature set (Table~\ref{tab:best-learner-perf}).
Combining feature sets for lasso and ridge did not help to increase performance, and while there was no substantial difference for lasso, the performance of ridge improved by around two percentage points.
XGBoost showed very poor performances for some feature sets and fills the last\ ten places of the ranking (Table~\ref{tab:perf-worst-10}).
Especially when combined with PCA, the algorithm was not able to achieve adequate performances.

% filters
% - influence varies based on algorithm and task (SVM no influence, RF medium, XGB medium)
% - no substantial decrease in performance when using filters
% - no advantage of ensemble borda filter compared to others
The effects of filter methods on performance differed greatly between the algorithms:
SVM showed no variation in performance across filters (Figure~\ref{fig:filter-effects-no-filter}).
The use of filters for RF resulted in a substantial increase in performance in all tasks, especially on the HR feature set where all filters showed an improved score compared to using no filter (Figure~\ref{fig:filter-effects-no-filter}).
XGBoost's performance depended strongly on feature selection. In two out of six tasks (HR, VI), using no filter resulted in the worst performance.
XGBoost showed the highest overall differences between filters for a single task---for feature set HR, the range is up to 13~p.p. (\enquote{CMIM} vs. \enquote{no filter}) (Figure~\ref{fig:filter-effects-no-filter}).

% filter vs. no filter
% - Substantial effect for XGBoost and RF
% - SVM unaffected
When comparing the usage of filters against using no filter at all, there were no instances in which a non-filtered model scored a better performance than the best filtered one (Figure~\ref{fig:filter-effects-no-filter}).
For SVM, all filters and \enquote{no filter} achieved roughly the same performance on all tasks.

% Borda vs. other filters
% - most often among the first best filters for each task
% - never the best
The Borda ensemble filter was not able to score the best performance in any learner/\linebreak filter setting (Figure~\ref{fig:filter-effects-borda}).
For RF and XGBoost, it most often ranked within the better half among all filters of a specific task.

% Selected percentage of features across plots
The number of features selected during model optimization strongly varied across learners and plots.
RF selected the least features of all three learners, and with the exception of Oiartzun, only one or two variables were selected.
SVM used 210 features or more in all instances and selected between 16\% (Laukiz1) and 81\% (Oiartzun) of the features (Table~\ref{tab:tune-perc-sel-features}).
XGBoost also favored using several hundred features with the exception of Laukiz2, for which only 14 (0.96\%) were selected.



% latex table best performance per learner
%\input{performance-best-per-learner.tex}
% latex table generated in R 4.0.4 by xtable 1.8-4 package
\begin{specialtable}[H]
\setlength{\tabcolsep}{6.8mm}
%\centering
\caption{The overall best individual learner performance across any task and filter method for RF, SVM, XGBoost, lasso, and ridge, sorted ascending by RMSE (p.p.) including the respective standard error (SE) of the cross-validation run. For exttt{regr.featureless}, the task is not applicable and was therefore removed.}
\label{tab:best-learner-perf}
\scalebox{0.9}{
\begin{tabular}{rlllrr}
\toprule
 & \textbf{Task} & \textbf{Model} & \textbf{Filter} & \textbf{RMSE} & \textbf{SE} \\
  \midrule
1 & NRI-VI & SVM & Info Gain & 27.915 & 18.970 \\
  2 & NRI & RF & Relief & 30.842 & 12.028 \\
  3 & HR & XGBoost & Info Gain & 31.165 & 15.025 \\
  4 & NRI & Lasso-MBO & No Filter & 31.165 & 15.025 \\
  5 & NRI & Ridge-MBO & No Filter & 31.165 & 15.025 \\
  6 & - & regr.featureless & No Filter & 31.165 & 15.025 \\
   \bottomrule
\end{tabular}
}
\end{specialtable}
\vspace{-12pt}

% latex table top 20 absolute performances
%\input{performance-svm-single-plot.tex}
% latex table generated in R 4.0.4 by xtable 1.8-4 package
\begin{specialtable}[H]
\setlength{\tabcolsep}{18.7mm}
%\centering
\caption{Test fold performances in RMSE (p.p.) for learner SVM on the HR dataset without using a filter, showcasing performance variance on the fold level. For each row, the model was trained on observations from all others plots but the given one and tested on the observations of the given plot.}
\label{tab:svm-single-fold-perf}
\begin{tabular}{rrl}
\toprule
 & \textbf{RMSE} & \textbf{Test Plot} \\
  \midrule
1 & 28.12 & Laukiz1 \\
  2 & 54.26 & Laukiz2 \\
  3 & 9.00 & Luiando \\
  4 & 21.17 & Oiartzun \\
   \bottomrule
\end{tabular}
\end{specialtable}
\vspace{-12pt}

% latex table top 10 absolute performances
%\input{performance-top-10}
% latex table generated in R 4.0.4 by xtable 1.8-4 package
\begin{specialtable}[H]
\setlength{\tabcolsep}{6mm}
%\centering
\caption{Best ten results among all learner--task--filter combinations, sorted in decreasing order of RMSE (p.p.) and their respective standard error (SE).}
\label{tab:perf-top-10}
\begin{tabular}{rlllrr}
\toprule
 & \textbf{Task} & \textbf{Model} & \textbf{Filter} & \textbf{RMSE} & \textbf{SE} \\
  \midrule
1 & NRI-VI & SVM & Info Gain & 27.915 & 18.970 \\
  2 & NRI & SVM & CMIM & 28.044 & 19.101 \\
  3 & VI & SVM & Relief & 28.082 & 19.140 \\
  4 & NRI-VI & SVM & Borda & 28.102 & 19.128 \\
  5 & HR & SVM & CMIM & 28.119 & 19.123 \\
  6 & HR & SVM & MRMR & 28.119 & 19.123 \\
  7 & VI & SVM & Info Gain & 28.121 & 19.123 \\
  8 & NRI & SVM & PCA & 28.121 & 19.123 \\
  9 & HR-NRI & SVM & PCA & 28.121 & 19.123 \\
  10 & HR-NRI-VI & SVM & PCA & 28.121 & 19.123 \\
   \bottomrule
\end{tabular}
\end{specialtable}
\vspace{-12pt}

% latex table worst 10 absolute performances
%\input{performance-worst-10}
% latex table generated in R 4.0.4 by xtable 1.8-4 package
\begin{specialtable}[H]
\setlength{\tabcolsep}{5.7mm}
%\centering
\caption{Worst ten results among all learner--task--filter combinations, sorted in decreasing order of RMSE (p.p.) and their respective standard error (SE).}
\label{tab:perf-worst-10}
\begin{tabular}{rlllrr}
\toprule
 & \textbf{Task} & \textbf{Model} & \textbf{Filter} & \textbf{RMSE} & \textbf{SE} \\
  \midrule
1 & VI & XGBoost & No Filter & 45.366 & 6.672 \\
  2 & HR & XGBoost & No Filter & 44.982 & 5.378 \\
  3 & VI & XGBoost & PCA & 44.539 & 8.187 \\
  4 & HR & XGBoost & PCA & 44.032 & 6.183 \\
  5 & NRI & XGBoost & PCA & 43.433 & 9.543 \\
  6 & HR-NRI & XGBoost & PCA & 43.220 & 2.557 \\
  7 & HR-NRI-VI & XGBoost & PCA & 41.076 & 9.862 \\
  8 & VI & RF & CMIM & 39.980 & 10.144 \\
  9 & VI & RF & Info Gain & 39.623 & 10.616 \\
  10 & NRI & XGBoost & Pearson & 39.492 & 11.548 \\
   \bottomrule
\end{tabular}
\end{specialtable}

% latex table percentage of selected features during tuning
%\input{tune-perc-sel-features.tex}
% latex table generated in R 4.0.4 by xtable 1.8-4 package
\begin{specialtable}[H]
\setlength{\tabcolsep}{9.2mm}
%\centering
\caption{Selected feature portions during tuning for the best performing learner--filter settings (SVM Relief, RF Relief, XGBoost CMIM) across folds for task HR-NRI-VI, sorted by plot name. "Features ({\#})" denotes the absolute number of features selected, and "Features ({\%})" refers to the percentage relative to the overall features available in the training sets for each plot (Laukiz1 = 1249, Laukiz2 = 1357, Luiando = 1507, Oiartzun = 1311). Results were estimated in a separate model tuning step, not within the main cross-validation comparison.}
\label{tab:tune-perc-sel-features}
\begin{tabular}{llrl}
  \toprule
\textbf{Learner} & \textbf{Test Plot} & \textbf{Features (\%)} & \textbf{Features (\#)} \\
  \midrule
\multirow{4}{*}{\specialcell{RF \\ Car}} & Laukiz1 & 0.00245 & 1/1249 \\
   & Laukiz2 & 0.00359 & 1/1357 \\
   & Luiando & 0.12448 & 2/1507 \\
   & Oiartzun & 2.80356 & 37/1311 \\
  \midrule\multirow{4}{*}{\specialcell{SVM \\ Car}} & Laukiz1 & 16.76686 & 210/1249 \\
   & Laukiz2 & 40.77700 & 554/1357 \\
   & Luiando & 43.80604 & 661/1507 \\
   & Oiartzun & 81.23205 & 1065/1311 \\
  \midrule\multirow{4}{*}{\specialcell{XGB \\ Borda}} & Laukiz1 & 79.54091 & 994/1249 \\
   & Laukiz2 & 0.96545 & 14/1357 \\
   & Luiando & 66.27871 & 999/1507 \\
   & Oiartzun & 41.89759 & 550/1311 \\
   \bottomrule
\end{tabular}
\end{specialtable}

\vspace{-18pt}
% plot performance results
\begin{figure}[H]
	%\centering
	%\begin{center}
		\includegraphics[width=0.7\textwidth] {performance-results-1.pdf}
		\caption{Predictive performance in RMSE (p.p.) of models across tasks. Different feature sets are shown on the y-axis. Labels show the feature selection method (e.g., NF = no filter, Car = "Carscore", Info Gain = "Information Gain", Borda = "Borda").}\label{fig:perf-result}
	%\end{center}
\end{figure}
\vspace{-15pt}

% plot no filter vs all other filters for each model and task
\begin{figure}[H]
	%\centering
	%\begin{center}
		\includegraphics[width=0.67\textwidth] {filter-effect-all-vs-no-filter-1.pdf}
		\caption{Model performances in RMSE across all tasks, split up in facets, when using no filter method (blue dot) compared to any other filter method (red cross) for learners RF, SVM, and XGBoost~(XG).}\label{fig:filter-effects-no-filter}
	%\end{center}
\end{figure}
\vspace{-18pt}

% plot Borda vs all other filters for each model and task
\begin{figure}[H]
	%\centering
	%\begin{center}
		\includegraphics[width=0.67\textwidth] {filter-effect-all-vs-borda-filter-1.pdf}
		\caption{Predictive performances in RMSE (p.p.) when using the Borda filter method (blue dot) compared to any other filter (red cross) for each learner across all tasks.}\label{fig:filter-effects-borda}
	%\end{center}
\end{figure}

\subsection{Variable Importance}

\subsubsection*{Permutation-Based Variable Importance}

% Variable Importance
% - VI: Vogelmann indices are best
% - All: Clustering around the red edge (700 - 750 nm)
% - Overall highest: 1.7 RMSE decrease - not that much

\textls[-15]{The most important features for datasets HR and VI showed an average decrease in RMSE of 1.06 p.p. (HR, B65) and 1.93 p.p. (VI, Vogelmann2) when permuted (Figure~\ref{fig:fi-permut-vi-hr}).}
For the HR dataset, most (i.e., six out of ten) relevant features clustered around the infrared region (920~nm--1000~nm), while for VI, eight out of ten concentrate within the wavelength range of 700~nm--750~nm (the so\-/called \enquote{red edge}).
For HR, four features in the infrared region (920~nm--1000~nm) were identified by the model to be most important, being associated with a mean decrease in RMSE of around 1 p.p.
Overall, apart from the top five features, the vast majority of features showed only a small importance with average decreases in RMSE below 0.5 p.p.
\vspace{-10pt}
% start a new page without indent 4.6cm

\end{paracol}
\nointerlineskip
% permutation based var imp for datasets HR and VI
\begin{figure} [H]
\widefigure
	%\centering
	%\begin{center}
		\includegraphics[width=0.75\textwidth] {fi-permut-vi-hr-1.pdf}
		\caption{Variable importance for feature sets HR and VI: Mean decrease in RMSE for one\ hundred feature permutations using the SVM learner. The wavelength range on the x-axis matches the range of the hyperspectral sensor (400~nm--1000~nm). For each dataset, the ten most important features are highlighted as black dots and labeled by name. Gray dots represent features from importance rank~11 to last. The spectral signature (mean) of each plot was added as a reference on a normalized reflectance scale [0, 1] (secondary y-axis). VI features were decomposed into their individual formula parts, all instances being connected via dashed lines. Each VI feature is composed out of at least two instances.}\label{fig:fi-permut-vi-hr}
	%\end{center}
\end{figure}
\begin{paracol}{2}
%\linenumbers
\switchcolumn



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

\subsection{Predictive Performance}

The best overall performance achieved in this study (SVM with the \enquote{Info Gain} filter, RMSE~27.915 p.p.) has to be seen in the light of model overfitting (see Section~\ref{subsec:perf-plot-char}).
Leaving out the performance on Laukiz2 when aggregating results, the mean RMSE would be around 19 p.p.
However, leaving out a single plot would also change the prediction results for the other plots because the observations from Laukiz2 would not be available for model training.
Due to the apparent presence of model overfitting in this study, it is suggested that more training data representing a greater variety of situations are needed.
A model can only make robust predictions if it has learned relationships across the whole range of the response.
Hence, care should be taken when predicting on the landscape scale using models fitted on this dataset due to their lack of generalizability caused by the limitations of the available training data.
However, when inspecting the fold\-/level performances, it can be concluded that the models performed reasonably well, predicting defoliation greater than 50\%, but they failed for lower levels.
This applied to all learners of this study.
In this study, the overall performance across all learners can be classified as ``poor'' given that only the SVM learner was able to substantially outperform the featureless learner \linebreak (Table~\ref{tab:best-learner-perf}).
It is worth noting that data quality issues may have affected model performances, as discussed below in detail  (Section~\ref{subsec:data-quality}).

\subsubsection{Model Differences}

An interesting finding is the strength of the SVM algorithm when comparing its predictive performance to its competitors (Table~\ref{tab:best-learner-perf}).
These cluster around a performance of 31 p.p., while SVM scored about 3 p.p. better than all other methods.
However, we refrain from comparing these results (both relatively and absolute) to other studies since many study design points have an influence on the final result (optimization strategy, data characteristics, feature selection methods, etc.).

A potential limiting factor in this study could be the upper limit of 70 iterations used for the XGBoost algorithm (hyperparameter \texttt{nrounds}), especially for feature sets including NRIs (Table~\ref{tab:hyperparameter_limits}).
This setting was a compromise between runtime and tuning space extension with the goal to work well for most feature sets.
It may be recommendable to increase this upper limit to a value closer to the number of features in the dataset in order to be able to exploit the full potential of this hyperparameter.

\subsubsection{Feature Set Differences}

One objective of this study was to determine whether expert-based and data-driven feature engineering have a positive influence on model performance.
With respect to Figure~\ref{fig:perf-result}, no overall positive or negative pattern related to specific feature sets was found that would be valid across all models.
The performance of RF and XGBoost on the VI feature set was around 4 to 6 p.p. lower than on others.
One reason could be the lack of coverage in the wavelength range between 810~nm and 1000~nm (Figure~\ref{fig:fi-permut-vi-hr}).
In addition, for all learners but SVM, a better performance was observed when NRI indices were included in the feature set (i.e., NRI-VI, HR-NRI, and HR-NRI-VI).

\subsection{Performance vs.\ Plot Characteristics}
\label{subsec:perf-plot-char}

The large differences in RMSE obtained on different test folds can be attributed to model overfitting (Table~\ref{tab:svm-single-fold-perf}).
An RMSE of 54.26 p.p. reveals the model's inability to predict tree defoliation on this plot (Laukiz2).
Laukiz2 differs highly in the distribution of the response variable defoliation compared to all other plots (Figure~\ref{fig:defol-distr}).
In the prediction scenario for Laukiz2, the model was trained on data containing mostly medium-to-high defoliation values and only few low ones.
This caused overfitting on the medium-to-high values, degrading the model's predictive performance in other scenarios.
When Laukiz2 was in the training set, the overall mean RMSE was reduced by up to 50\% with single fold performances as good as 9 p.p. RMSE (with Luiando as test set).

There was also no clear pattern in the percentage of features selected based on hyperparameter tuning (Table~\ref{tab:tune-perc-sel-features}).
The optimal value for the number of features (interpreted as a\ percentage of available features), which are selected from the ranked filter results, is determined by the internal surrogate learner of the MBO tuning method using the results from the previous tuning iterations.
Due to this iterative approach, MBO is in some ways able to evaluate how well a learning algorithm plays together with a certain amount of selected features and is subsequently able to adjust the number of variables to an optimal value.
In general, considering SVM's relative success, the use of at least a few hundred features from the combined feature set appears to be beneficial, or at least not harmful when the model's built-in regularization is capable of dealing with the resulting high-dimensional situation.

% filters always reduce computation time
Realizing early during hyperparameter optimization that only a few features are needed to reach adequate performances can reduce the overall computational runtime substantially.
Hence, regardless of the potential advantage of using filters for increased predictive performance, these can have a strong positive effect on runtime, especially of models making use of hyperparameters that depend on the available number of features, such as RF with $m_\textrm{try}$.

Ultimately, the results of Table~\ref{tab:tune-perc-sel-features} should be taken with care, as they rely on single model--filter combinations and are subject to random variation.
More in-depth research is needed to investigate the effect of filters on criteria other than performance (such as runtime), leading to a multi-criteria optimization problem.

\subsection{Feature Selection Methods}

% - Effect varies across learners -> suggest to use FS even on feature sets with p < 100?
% - Filters can also have a negative impact
% - Filters are interesting for reducing runtime and improve interpretability, not only for performance
The usefulness of filters with respect to predictive performance in this study varied.
While the performance of some models (up to 5 p.p. for RF and XGBoost) was improved by specific filters, some models achieved a poorer performance with filters than without them (Figure~\ref{fig:filter-effects-no-filter}).
There was no pattern of specific filters consistently resulting in better scores.
Hence, it is recommended to test multiple filters in a study if it is intended to use filters.
While filters can improve the performance of models, they may be more appealing based on other aspects than performance. Reducing variables can reduce computational efforts in high-dimensional scenarios and may enhance the interpretability of models.
Filters are a lot cheaper to compute than wrapper methods, and the final feature subset selection can be integrated as an additional hyperparameter into the model optimization~stage.

% - Ensemble FS shows no advantage to simple methods -> always one simple filter which is better. Ensemble filters are good across many different datasets
Models that used the Borda ensemble method in this study did not perform better on average than models that used a single filter or no filter at all.
Ensemble methods have higher stability and robustness than single ones and have shown promising results in~\cite{drotar2017}.
Hence, their expected main advantage is stable performances across datasets with varying characteristics.
Single filter methods might yield better model performances on certain datasets but fail on others.
The fact that this study used multiple feature sets but only one dataset and tested many single filters could be a potential explanation of why, in all cases, a single filter outperformed the ensemble filter.
However, studies that use ensemble filters are still rare, and these are usually not compared against single filters~\cite{ghosh2019}.
In summary, in this study, Borda did\ not\ perform better than a randomly selected filter method.
More case studies applying ensemble filter methods are needed to verify this finding.
Nevertheless, ensemble filters can be a promising addition to an ML feature selection portfolio.

% - PCA shows similar performance but probably requires tuning of main components -> check runtime!
PCA, acting as a filter method, more often showed less than optimal results, especially for algorithms RF and XGBoost.
XGBoost in particular had substantial problems when using PCA as a filter method and accounted for four of the six worst results (Table~\ref{tab:perf-worst-10}).
However, PCA was able to reduce model fitting times substantially across all algorithms.
Depending on the use case, PCA can be an interesting option to reduce dimensionality while keeping runtime low.
However, information about the total number of features used by the model is lost when applying this technique.
Since filter scores only need to be calculated once for a given dataset in a benchmark setting, the runtime advantage of a PCA vs. filter methods might in fact be negligible in practice.

\subsection{Linking Feature Importance to Spectral Characteristics}

Unsurprisingly, the most important features for both HR and VI datasets were identified around the red edge of the spectra, specifically in the range of 680~nm to 750~nm.

This area has the highest ability to distinguish between reflectances related to a high density/high foliage density and thus the health status of vegetation and its respective counterpart~\cite{horler1983}.
However, four out of ten of the most important features of dataset HR are located between 920~nm and 1000~nm.
Looking at the spectral curves of the plots, apparent reflectance differences can be observed in this spectral range---especially for plot Oiartzun---which might explain why these features were considered important by the~model.

A possible explanation for the poor performances of most models scored on the VI dataset compared to all other feature sets could be the lack of features covering the area between 850~nm and 1000~nm (Figure~\ref{fig:fi-permut-vi-hr}).
The majority of VI features covers the range between 550~nm--800~nm.
Only one index (PWI) covers information in the range beyond 900~nm.

\subsection{Data Quality}
\label{subsec:data-quality}

Environmental datasets always come with some constraints that can have potential influence on the modeling process and its outcome.
Finding a suitable approach to extract the remote sensing information from each tree was a complex process.
Due to the reported geometric offset of up to one meter within the hyperspectral data, the risk of assigning a value to an observation that would actually refer to a different, possibly non-tree, pixel was reasonably high.
It was concluded that using a buffer radius of one meter can be a good compromise between the inclusion of information from too many surrounding trees and an under-coverage of the tree crown.
With the chosen radius, we are confident that we were able to map individual tree crowns while accounting for a possible geometric offset.
% The applied buffer only included a pixel value if the distance to the centroid of a pixel was smaller than the buffer radius (i.e., $<= 1m$).
This results in all cases in four contributing pixels (=four square meters) for the extraction of hyperspectral information for a given tree.
Even though no results showing the influence of different buffer values on the extraction were provided, it is hypothesized that the relationships between features would not change substantially, leading to almost identical model results.
Instead of using a buffer to extract the hyperspectral information, segmentation could have been considered.
However, this method would have required more effort for no clear added value in our view and would have moved the focus of this manuscript more to data preprocessing and away from feature selection methods.

Trees located within grid cells on the border of a plot are a notable exception where the exact number of pixels contributing to the observation's feature value may be reduced since the image was cropped to the plot's extent.
Cropping was applied to avoid the accidental inclusion of background data such as forest roads.
However, this effect was deemed to be of negligible importance.

The available hyperspectral data covered a wavelength between 400~nm and 1000~nm.
Hence, the spectral range of the shortwave infrared (SWIR) region is not covered in this study.
Given that this range is often used in forest health studies~\cite{hais2019}, e.g., when calculating the \ac{NDMI} index~\cite{gao1996}, this marks a clear limitation of the dataset at hand.

The dataset consists of in situ data collected during September 2016, which was matched against remote sensing data acquired at the end of September 2016.
A multi-temporal dataset consisting of in situ data from different phenology stages would possibly improve the achieved model performances.
However, this would also require the costly acquisition of hyperspectral data of these additional timestamps.

The R package \texttt{hsdar} was used for the calculation of vegetation indices~\cite{lehnert2016}.
All indices that could be calculated with the given spectral range of the data (400~nm--1000~nm) were used.
This means that even though Table~\ref{tab:vegindices} lists all indices available in the package, not all listed indices were used in this study.
Although this selection included a large number of indices, some possibly helpful indices might have been missed due to the restriction of the hyperspectral data.

Overall, the magnitude of uncertainty introduced by the mentioned effects during index derivation cannot be quantified.
Such limitations and uncertainties apply to most environmental studies and cannot be completely avoided.

\subsection{Practical Implications on Defoliation and Tree Health Mapping}
Even though this work has a strong methodological focus by comparing different benchmark settings on highly\ correlated feature sets, implications on tree health should be briefly discussed in the following.
Due to the outlined dataset issues in Section~\ref{subsec:data-quality}, which are mainly responsible for the resulting poor model performances, the trained models are not suited for practical use, e.g., to predict defoliation in unknown areas, due to the high mapping uncertainty.
However, the general approach of utilizing hyperspectral data to classify the health status of trees partly led to promising results.
For example, due to the narrow bandwidth of the hyperspectral sensor, small parts of the spectrum (mainly in the infrared region) were of higher importance to the models (e.g., see Figure~\ref{fig:fi-permut-vi-hr}), meaning that they helped the models to distinguish between low and high tree defoliation.
If spatial offset errors of the image data and possible background noise can be reduced (possibly by making use of image segmentation), we believe that model performances could be substantially enhanced.
Such improved models, starting around an RMSE of 20\% and less, should be able to provide added value to support the long-term monitoring of forest health and early detection of fungi-affected tree plots.
Nevertheless, overall the use of defoliation as a proxy for forest health should be critically reconsidered as it comes with various practical issues, starting from potential offsets during data collection, varying leaf density due to tree age, and differing effects between tree species, to  name just a few.

\subsection{Comparison to Other Studies}
% - no environmental studies use filter methods, some use FFS
% - only few studies analyze defoliation
% - most FS is done in bioinformatics

% defoliation studies and what data they used
While most defoliation studies operate on the plot level using coarser\ resolution multispectral satellite data~\cite{townsend2012, debeurs2008, rengarajan2016}, there are also several recent studies using airborne or ground-based sensors at the tree level.
Among these, refs.~\cite{meng2018, kalin2019} used ground-level methods, such as \ac{ALS} and \ac{LiDAR}.

% defoliation studies and what
Studies focusing on tree-level defoliation mainly used ground-level methods, such as \ac{ALS} or \ac{LiDAR}~\cite{meng2018, kalin2019}.
Ref.~\cite{meng2018} used \ac{OLS} regression methods while~\cite{kalin2019} retrieving information from ground-level RGB photos using \ac{CNN}.
However, neither of them used spatial \ac{CV} for model assessment, and~\cite{kalin2019} did not perform \ac{FS}.
The authors of \cite{goodbody2018} used a \ac{PLS} model with high-resolution \ac{DAP} to predict cumulative defoliation caused by the spruce budworm.
Study results indicated that spectral features were found to be most helpful for the model.
Incorporating such features (both spectral and structural) could be a possible enhancement for future works.
No studies were found to model defoliation caused by \textit{Diplodia sapinea (Fr.) Fuckel} with remote sensing data, and most studies focused on describing the tree conditions based on local sampling~\cite{hlebarska2018, kaya2019}.

% write about hyperspectral remote sensing
The field of (hyperspectral) remote sensing has had a strong focus on using RF for modeling in recent years~\cite{belgiu2016}.
However, in high-dimensional scenarios, tuning the parameter \texttt{\(m_{try}\)} becomes computationally expensive.
To account for this and the high dimensionality in general, studies used feature selection approaches, such as  semi-supervised feature extraction~\cite{xia2015}, wrapper methods~\cite{fassnacht2014, feng2016, georganos2018}, PCA, and adjusted feature selection~\cite{rochac2016}.
In general, applying feature selection methods on hyperspectral datasets has shown to be effective, regardless of the method used~\cite{pal2010, keller2016}.
However, no studies were found that made explicit use of filter methods in combination with hyperparameter tuning in the field of (hyperspectral) remote sensing.
Potential reasons for this absence could be an easier programmatic access to wrapper methods and a higher general awareness of these compared to filter methods.
Applying the filter-based feature selection methodology shown in this study and its related code provided in the research compendium might be a helpful reference for future studies using hyperspectral remote sensing data.

% RS studies with more/other algorithms than RF
When looking for remote sensing studies that compare multiple models, it turned out that these often operate in a low-dimensional predictor space~\cite{xu2019} or use wrapper methods explicitly~\cite{georganos2018}.

Refs.~\cite{shendryk2016, ludwig2019} are more similar in their methodology but focus on a different response variable (woody cover).
Ref.~\cite{shendryk2016} used machine learning with \ac{ALS} data to study dieback of trees in eucalyptus forests.
A grid search was used for hyperparameter tuning and \ac{FFS} for variable selection.
Ref.~\cite{ludwig2019} analyzed woody cover in South Africa using a spatial \ac{CV} and \ac{FS} approach~\cite{meyer2018} with an RF classifier.
Ref.~\cite{zandler2015} shows a similar setup; they used hyperspectral vegetation indices and a nested CV approach for performance estimation, and they estimated variable importance targeting woody biomass as the response.
In the results, lasso showed the best performance among the chosen methods.
However, the authors did not optimize the hyperparameters of RF, which makes a fair comparison problematic since the other models used internal optimization.
The discussion section of~\cite{zandler2015} lists additional studies that made use of shrinkage models for high-dimensional remote sensing~modeling.

In summary, no studies could be found that used filter methods for \ac{FS} or made use of \ac{NRI} indices in their work and had a relation to tree health.
This might relate to the fact that most environmental datasets are not high dimensional.
In fact, many studies use fewer than ten features, and issues related to correlations are often solved manually instead of relying on an automated approach.
These manual approaches might suffer from subjectivity and may limit the reproducibility of results.

Other fields (e.g., bioinformatics) encounter high-dimensional datasets more often.
Hence, more studies using (filter-based) feature selection approaches can be found in this \linebreak field~\cite{guo2019, radovic2017}.
However, bioinformatics differs conceptually in many ways from environmental modeling, and, therefore, no greater focus was put into comparing studies of this field.
The availability of high-dimensional feature sets will increase in the future due to higher temporal and spectral resolutions of sensors.
In addition, a high-spatial resolution comes with the possibility of calculating many textural features.
Hence, the ability to deal with high-dimensional datasets becomes more important, and unbiased robust approaches are needed.
We hope that this work and its methodology raise awareness about the application of filter methods to tackle high-dimensional problems in the environmental modeling~field.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

This study analyzed the effectiveness of filter-based feature selection in improving various machine-learning models of defoliation of trees in northern Spain based on hyperspectral remote-sensing data.
Substantial differences in performance occurred depending on which feature selection and machine learning methods were combined.
SVM showed the most robust behavior across all highly\ correlated datasets and was able to predict the response variable of this study substantially better than other methods.

% RQ3: How are feature-selection methods influencing the predictive performance of the models?
Filter methods were able to improve the predictive performance on datasets in some instances, although there was no clear and systematic pattern.
Their effectiveness depends on the algorithm and the dataset characteristics.
Ensemble filter methods did not show a substantial improvement over individual filter methods in this study.

% RQ2: Does combining feature sets have an substantial effect on predictive performance?
% RQ1: Do different environmental feature sets show differences in performance when modeling defoliation at trees?
The addition of derived feature sets was, in most cases, able to improve predictive performance.
In contrast, feature sets that focused on only a small fraction of the available spectral range (i.e., dataset~VI) showed a worse performance than the ones that covered a\ wider range (400~nm--1000~nm; HR, NRI).
NRIs can be seen as a valuable addition to the optimization of predictive performance in the remote sensing of vegetation.

% RQ4: Which features are most important for the models and how can these be interpreted in an ecological context?
Features along the red-edge wavelength region were most important for models during prediction.
With respect to dedicated vegetation indices, all versions of the Vogelmann index were seen as the most important indices for the best performing SVM model.
This matches well with the actual purpose of these indices---they were invented to detect defoliation on sugar maple trees (\textit{Acer saccharum Marsh.}) caused by pear thrips (\textit{Taeniothrips inconsequens Uzel})~\cite{vogelmann1993}.
However, assessing feature importance for highly correlated features remains a challenging task.
Results might be biased and should be taken with care to avoid overgeneralizing from individual studies.

% What about the general potential of analyzing defol with hyperspectral data?
Finally, the potential of predicting defoliation with the given study design was rather limited with respect to the average RMSE of 28 p.p. scored by the best performing model.
More training data covering a wider range of defoliation values in a larger number of forest plantations are needed to train better models that can create more robust predictions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{Conceptualization, P.S. and A.B.; data curation, P.S. and E.I.; formal analysis, P.S., J.C., B.B., and A.B.; funding acquisition, A.B.; investigation, P.S.; methodology, P.S., J.M., J.C., and B.B.; project administration, A.B.; resources, E.I.; software, P.S.; supervision, B.B. and A.B.; validation, P.S. and A.B.; visualization, P.S.; writing---original draft, P.S.; Writing---review and editing, J.M., J.C., and A.B. All authors have read and agreed to the published version of the manuscript.}

\funding{This work was funded by the EU LIFE Healthy Forest project (LIFE14 ENV/ES/000179) and the German Scholars Organization/Carl Zeiss Foundation.}

\institutionalreview{\hl{ } %MDPI: In this section, please add the Institutional Review Board Statement and approval number for studies involving humans or animals. Please note that the Editorial Office might ask you for further information. Please add ``The study was conducted according to the guidelines of the Declaration of Helsinki, and approved by the Institutional Review Board (or Ethics Committee) of NAME OF INSTITUTE (protocol code XXX and date of approval).'' OR ``Ethical review and approval were waived for this study, due to REASON (please provide a detailed justification).'' OR ``Not applicable'' for studies not involving humans or animals. You might also choose to exclude this statement if the study did not involve humans or animals.
}

\informedconsent{\hl{ } %MDPI: Any research article describing a study involving humans should contain this statement. Please add ``Informed consent was obtained from all subjects involved in the study.'' OR ``Patient consent was waived due to REASON (please provide a detailed justification).'' OR ``Not applicable'' for studies not involving humans. You might also choose to exclude this statement if the study did not involve humans.
}

\dataavailability{The data presented in this study are openly available in Zenodo at \url{https://doi.org/10.5281/zenodo.2635403} \hl{(accessed on).} %MDPI: Please add access date after the URL; like “http://xxxxxxx (access on 8 January 2021)”, please check all URL in manuscript and add the specific date.
}


% \acknowledgments{In this section you can acknowledge any support given which is not covered by the author contribution or funding sections. This may include administrative and technical support, or donations in kind (e.g., materials used for experiments).}

\conflictsofinterest{The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the~results.}

%% Optional
% \sampleavailability{Samples of the compounds ... are available from the authors.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Only for journal Encyclopedia
%\entrylink{The Link to this entry published on the encyclopedia platform.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional
\abbreviations{Abbreviations}{
	The following abbreviations are used in this manuscript:\\

	\noindent
	\begin{tabular}{@{}ll}
		AGB     & above-ground biomass                                    \\
		ALE     & accumulated local effects                               \\
		ALS     & airborne laser scanning                                 \\
		ANN     & artificial neural network                               \\
		AUROC   & area under the receiver operating characteristics curve \\
		BRT     & boosted regression trees                                \\
		CART    & classification and regression trees                     \\
		CNN     & convolutional neural networks                           \\
		CV      & cross-validation                                        \\
		DAP     & digital aerial photogrammetry                           \\
		ENM     & environmental niche modeling                            \\
		FFS     & forward feature selection                               \\
		FPR     & false positive rate                                     \\
		FS      & feature selection                                       \\
		GAM     & generalized additive model                              \\
		GBM     & gradient boosting machine                               \\
		GLM     & generalized linear model                                \\
		ICGC    & Institut Cartografic i Geologic de Catalunya            \\
		IQR     & interquartile range                                     \\
		LiDAR   & light detection and ranging                             \\
		LOWESS  & locally weighted scatter plot smoothing                 \\
		MARS    & multivariate adaptive regression splines                \\
		MBO     & model-based optimization                                \\
		MEM     & maximum entropy model                                   \\
		ML      & machine learning                                        \\
		NDII    & normalized difference infrared index                    \\
%		NDMI    & normalized difference moisture index                    \\
%		NIR     & near-infrared                                           \\
%		NRI     & normalized ratio index                                  \\
%		OLS     & ordinary least squares                                  \\
%		OMNBR   & optimized multiple narrow-band reflectance              \\
%		PCA     & principal component analysis                            \\
%		PDP     & partial dependence plots                                \\
%		PISR    & potential incoming solar radiation                      \\
%		PLS     & partial least-squares                                   \\
%		POV     & proportion of variance explained                        \\
%		RBF     & radial basis function                                   \\
%		RF      & random forest                                           \\
%		RMSE    & root mean square error                                  \\
%		RR      & ridge regression                                        \\
%		RSS     & residual sum of squares                                 \\
%		SAR     & synthetic aperture radar                                \\
%		SDM     & species distribution modeling                           \\
%		SMBO    & sequential-based model optimization                     \\
%		SVM     & support vector machine                                  \\
%		TPR     & true positive rate                                      \\
%		VI      & vegetation index                                        \\
%		XGBoost & extreme gradient boosting
	\end{tabular}

	\noindent
	\begin{tabular}{@{}ll}
%		AGB     & above-ground biomass                                    \\
%		ALE     & accumulated local effects                               \\
%		ALS     & airborne laser scanning                                 \\
%		ANN     & artificial neural network                               \\
%		AUROC   & area under the receiver operating characteristics curve \\
%		BRT     & boosted regression trees                                \\
%		CART    & classification and regression trees                     \\
%		CNN     & convolutional neural networks                           \\
%		CV      & cross-validation                                        \\
%		DAP     & digital aerial photogrammetry                           \\
%		ENM     & environmental niche modeling                            \\
%		FFS     & forward feature selection                               \\
%		FPR     & false positive rate                                     \\
%		FS      & feature selection                                       \\
%		GAM     & generalized additive model                              \\
%		GBM     & gradient boosting machine                               \\
%		GLM     & generalized linear model                                \\
%		ICGC    & Institut Cartografic i Geologic de Catalunya            \\
%		IQR     & interquartile range                                     \\
%		LiDAR   & light detection and ranging                             \\
%		LOWESS  & locally weighted scatter plot smoothing                 \\
%		MARS    & multivariate adaptive regression splines                \\
%		MBO     & model-based optimization                                \\
%%		MEM     & maximum entropy model                                   \\
%		ML      & machine learning                                        \\
%		NDII    & normalized difference infrared index                    \\
		NDMI    & normalized difference moisture index                    \\
		NIR     & near-infrared                                           \\
		NRI     & normalized ratio index                                  \\
		OLS     & ordinary least squares                                  \\
		OMNBR   & optimized multiple narrow-band reflectance              \\
		PCA     & principal component analysis                            \\
		PDP     & partial dependence plots                                \\
		PISR    & potential incoming solar radiation                      \\
		PLS     & partial least-squares                                   \\
		POV     & proportion of variance explained                        \\
		RBF     & radial basis function                                   \\
		RF      & random forest                                           \\
		RMSE    & root mean square error                                  \\
		RR      & ridge regression                                        \\
		RSS     & residual sum of squares                                 \\
		SAR     & synthetic aperture radar                                \\
		SDM     & species distribution modeling                           \\
		SMBO    & sequential-based model optimization                     \\
		SVM     & support vector machine                                  \\
		TPR     & true positive rate                                      \\
		VI      & vegetation index                                        \\
		XGBoost & extreme gradient boosting
	\end{tabular}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional
%\pagebreak
\appendixtitles{no} % Leave argument "no" if all appendix headings stay EMPTY (then no dot is printed after "Appendix A"). If the appendix sections contain a heading then change the argument to "yes".
\appendixstart
\appendix
\section{}\label{appendixA}
\subsection{}

\vspace{-12pt}
\begin{figure} [H]
	%\begin{center}
		\includegraphics[width=0.7\textwidth] {correlation-filter-nri-1.pdf}
		\caption{Spearman correlations of NRI feature rankings obtained with different filters. Filter names refer to the nomenclature used by the \texttt{mlr} R package. Underscores in names divide the terminology into their upstream R package and the actual filter name.}\label{fig:correlation-filters}
	%\end{center}
\end{figure}

%\pagebreak

\subsection{}
\vspace{-12pt}
\begin{specialtable}[H]
\setlength{\tabcolsep}{5mm}
	%\centering
	\caption[H]{Hyperparameter ranges and types for each model.
		Hyperparameter notations from the respective R packages are shown.}
	\begingroup\scriptsize
	\begin{tabularx}{.75\textwidth}{llXXrXX}
		\toprule
		\textbf{\specialcell{Model                                              \\ (Package)}}                & \textbf{Hyperparameter}              & \textbf{Type}    & \textbf{Start}     & \textbf{End}      & \textbf{Default} \\
	\midrule
		\multirow{3}{*}{\specialcell{RF                                 \\ (ranger)}}       & \texttt{$x_{try}$}    & dbl & 0         & 0.5      & -       \\
		 & \texttt{min.node.size}      & int & 1        & 10      & 1   \\
		 & \texttt{sample.fraction}    & dbl & 0.2      & 0.9     & 1   \\
		\midrule
		\multirow{2}{*}{\specialcell{SVM                                \\ (kernlab)}}     & \texttt{C}                  & dbl & $2^{-10}$ & $2^{10}$ & 1       \\
		 & \texttt{$\sigma$}           & dbl & $2^{-5}$ & $2^{5}$ & 1   \\
		\midrule
		\multirow{7}{*}{\specialcell{XGBoost                            \\ (xgboost)}} & \texttt{nrounds}            & int & 10        & 70      & -       \\
		 & \texttt{colsample\_bytree}  & dbl & 0.6      & 1       & 1   \\
		 & \texttt{subsample}          & dbl & 0.6      & 1       & 1   \\
		 & \texttt{max\_depth}         & int & 3        & 15      & 6   \\
		 & \texttt{gamma}              & int & 0.05     & 10      & 0   \\
		 & \texttt{eta}                & dbl & 0.1      & 1       & 0.3 \\
		 & \texttt{min\_child\_weight} & int & 1        & 7       & 1   \\
		\bottomrule
	\end{tabularx}
	\endgroup
	\label{tab:hyperparameter_limits}
\end{specialtable}
\vspace{-6pt}
%\pagebreak

\subsection{}
\vspace{-12pt}
% \begingroup\scriptsize
% \setlength\tabcolsep{4pt}  % default value: 6pt
% vertical spacing between rows (default is 1)
\renewcommand{\thespecialtable}{A\arabic{specialtable}}
\begin{specialtable}[H]
\setlength{\tabcolsep}{10.4mm}
{
	\caption{List of available vegetation indices in the \texttt{hsdar} package.}
	\begin{tabular}{lll}\toprule
		% \begin{supertabular}{0.3\textwidth}{lll}
		\bfseries{Name} & \bfseries{Formula}                                                                                   & \bfseries{Reference~\hl{*}} %MDPI: there is no explanation for *.
         \\		\midrule
		Boochs          & $D_{703}$                                                                                            &~\cite{boochs1990}             \\
		Boochs2         & $D_{720}$                                                                                            &~\cite{boochs1990}             \\
		CAI             & $0.5 \times (R_{2000} + R_{2200}) -R_{2100}$                                                         &~\cite{nagler2003}             \\
		\midrule
		CARI            & $a = (R_{700}-R_{550}) / 150$                                                                        &~\cite{walthall1994}           \\
		& $b = R_{550}-(a\times 550)$                                                                          &                               \\
		& $\frac{R_{700}\,\times\, | (a\,\times\,670\,+\,R_{670}\,+\,b)}{R_{670}\,\times\,(a^2+1)| ^{0.5}}$                       &                               \\
		\midrule
		Carter          & $R_{695}/R_{420}$                                                                                    &~\cite{carter1994}             \\
		Carter2         & $R_{695}/R_{760}$                                                                                    &~\cite{carter1994}             \\
		Carter3         & $R_{605}/R_{760}$                                                                                    &~\cite{carter1994}             \\
		Carter4         & $R_{710}/R_{760}$                                                                                    &~\cite{carter1994}             \\
		Carter5         & $R_{695}/R_{670}$                                                                                    &~\cite{carter1994}             \\
		Carter6         & $R_{550}$                                                                                            &~\cite{carter1994}             \\
		CI              & $R_{675}\times R_{690}/R_{683}^2$                                                                    &~\cite{zarco-tejada2003}      \\
		CI~2             & $R_{760}/R_{700}-1$                                                                                  &~\cite{gitelson2003}           \\
		ClAInt          & $\int_{600 nm}^{735 nm} R$                                                                           &~\cite{oppelt2004}             \\
		CRI1            & $1/R_{515}-1/R_{550}$                                                                                &~\cite{gitelson2003}           \\
		CRI2            & $1/R_{515}-1/R_{770}$                                                                                &~\cite{gitelson2003}           \\
		CRI3            & $1/R_{515}-1/R_{550}\times R_{770}$                                                                  &~\cite{gitelson2003}           \\
		CRI4            & $1/R_{515}-1/R_{700}\times R_{770}$                                                                  &~\cite{gitelson2003}           \\
		D1              & $D_{730}/D_{706}$                                                                                    &~\cite{zarco-tejada2003}      \\
		D2              & $D_{705}/D_{722}$                                                                                    &~\cite{zarco-tejada2003}      \\
		Datt            & $(R_{850}-R_{710})/(R_{850}-R_{680})$                                                                &~\cite{datt1999}               \\
		Datt2           & $R_{850}/R_{710}$                                                                                    &~\cite{datt1999}               \\
		Datt3           & $D_{754}/D_{704}$                                                                                    &~\cite{datt1999}               \\
		Datt4           & $R_{672}/(R_{550} \times R_{708})$                                                                   &~\cite{datt1998}               \\
		Datt5           & $R_{672}/R_{550}$                                                                                    &~\cite{datt1998}               \\
		Datt6           & $(R_{860})/(R_{550}\times R_{708})$                                                                  &~\cite{datt1998}               \\
%		Datt7           & $(R_{860} - R_{2218})/(R_{860} - R_{1928})$                                                          &~\cite{datt1999a}              \\
%		Datt8           & $(R_{860} - R_{1788})/(R_{860} - R_{1928})$                                                          &~\cite{datt1999a}              \\
%		DD              & $(R_{749}-R_{720})-(R_{701}-R_{672})$                                                                &~\cite{maire2004}              \\
%		DDn             & $2\times (R_{710}-R_{660}-R_{760})$                                                                  &~\cite{lemaire2008}            \\
%		DPI             & $(D_{688}*D_{710})/D_{697}^2$                                                                        &~\cite{zarco-tejada2003}      \\
%		DWSI1           & $R_{80}/R_{1660}$                                                                                    &~\cite{apan2004}               \\
%		DWSI2           & $R_{1660}/R_{550}$                                                                                   &~\cite{apan2004}               \\
%		DWSI3           & $R_{1660}/R_{680}$                                                                                   &~\cite{apan2004}               \\
%		DWSI4           & $R_{550}/R_{680}$                                                                                    &~\cite{apan2004}               \\
%		DWSI5           & $(R_{800} + R_{550})/(R_{1660} + R_{680})$                                                           &~\cite{apan2004}               \\
%		EGFN            & $\frac{(\max(D_{650:750})-\max(D_{500:550}))}{(\max(D_{650:750})+\max(D_{500:550}))}$                &~\cite{penuelas1994}           \\
%		EGFR            & $\max(D_{650:750})/\max(D_{500:550})$                                                                &~\cite{penuelas1994}           \\
%		EVI             & $\frac{2.5\times ((R_{800}-R_{670}) }{ (R_{800}-(6\times R_{670})-(7.5\times R_{475})+1)}$           &~\cite{huete1997}             \\
%		GDVI            & $(R_{800}^n-R_{680}^n) / (R_{800}^n+R_{680}^n)$**                                                    &~\cite{wu2014}                 \\
%		GI              & $R_{554}/R_{677}$                                                                                    &~\cite{smith1995}              \\
%		Gitelson        & $1/R_{700}$                                                                                          &~\cite{gitelson1999}           \\
%		Gitelson2       & $(R_{750}-R_{800}/R_{695}-R_{740})-1$                                                                &~\cite{gitelson2003}           \\
%		GMI1            & $R_{750}/R_{550}$                                                                                    &~\cite{gitelson2003}           \\
%		GMI2            & $R_{750}/R_{700}$                                                                                    &~\cite{gitelson2003}           \\
%		Green NDVI      & $\frac{R_{800}-R_{550}}{R_{800}+R_{550}}$                                                            &~\cite{gitelson1996}           \\
%		LWVI\_1         & $\frac{(R_{1094}-R_{983})}{(R_{1094}+R_{983})}$                                                      &~\cite{galvao2005}             \\
%		LWVI\_2         & $\frac{R_{1094}-R_{1205}}{R_{1094}+R_{1205}}$                                                        &~\cite{galvao2005}             \\
%		Maccioni        & $\frac{R_{780}-R_{710})}{R_{780}-R_{680}}$                                                           &~\cite{maccioni2001}           \\
%		\midrule
%		MCARI           & \parbox{5.5cm}{$((R_{700}-R_{670})-0.2\times (R_{700}-R_{550})) \times (R_{700}/R_{670})$}           &~\cite{daughtry2000}           \\
%		\midrule
%		MCARI2          & \parbox{5.5cm}{$((R_{750}-R_{705})-0.2 \times (R_{750}-R_{550})) \times (R_{750}/R_{705})$}          &~\cite{wu2008}                \\
%		\midrule
%		mND705          & $\frac{(R_{750}-R_{705})}{R_{750}+R_{705}-2\times R_{445}}$                                          &~\cite{sims2002}              \\
%		mNDVI           & $\frac{(R_{800}-R_{680})}{R_{800}+R_{680}-2 \times R_{445}}$                                         &~\cite{sims2002}              \\
%		MPRI            & $\frac{R_{515}-R_{530}}{R_{515}+R_{530}}$                                                            &~\cite{hernandez-clemente2011} \\
%		MSAVI           & \parbox{5.5cm}{$0.5 \times ((2\times R_{800}+1)^2-8\times (R_{800}-R_{670}))^{0.5}$}                 &~\cite{qi1994}                 \\
%		MSI             & $\frac{R_{1600}}{R_{817}}$                                                                           &~\cite{hunt1989}               \\
%		mSR             & $\frac{R_{800}-R_{445}}{R_{680}-R_{445}}$                                                            &~\cite{sims2002}              \\
%		mSR2            & $\frac{(R_{750}/R_{705})-1}{R_{750}/R_{705}+1)^{0.5}}$                                               &~\cite{chen1996}               \\
%		mSR705          & $\frac{R_{750}-R_{445}}{R_{705}-R_{445}}$                                                            &~\cite{sims2002}              \\
%		MTCI            & $\frac{R_{754}-R_{709}}{R_{709}-R_{681}}$                                                            &~\cite{dash2007}               \\
%		\midrule
%		MTVI            & \parbox{3.8cm}{$1.2 \times (1.2 \times (R_{800}-R_{550})-2.5 \times (R_{670}-R_{550}))$}             &~\cite{haboudane2002}          \\
%		\midrule
%		NDLI            & $\frac{log(1/R_{1754}) - log(1/R_{1680})}{log(1/R_{1754}) + log(1/R_{1680})}$                        &~\cite{serrano2002}            \\
%		NDNI            & $\frac{log(1/R_{1510}) - log(1/R_{1680})}{log(1/R_{1510}) + log(1/R_{1680})}$                        &~\cite{serrano2002}            \\
%		NDVI            & $\frac{R_{800}-R_{680}}{R_{800}+R_{680}}$                                                            &~\cite{tucker1979}             \\
%		NDVI2           & $\frac{R_{750}-R_{705}}{R_{750}+R_{705}}$                                                            &~\cite{gitelson1994}           \\
%		NDVI3           & $\frac{R_{682}-R_{553}}{R_{682}+R_{553}}$                                                            &~\cite{guanter2005}            \\
%		NDWI            & $\frac{R_{860}-R_{1240}}{R_{860}+R_{1240}}$                                                          &~\cite{gao1996}                \\
%		NPCI            & $\frac{R_{680}-R_{430}}{R_{680}+R_{430}}$                                                            &~\cite{penuelas1994}           \\
%		OSAVI           & $\frac{(1+0.16) \times (R_{800}-R_{670})}{R_{800}+R_{670}+0.16 }$                                    &~\cite{rondeaux1996}           \\
%		OSAVI2          & $\frac{(1+0.16)\times (R_{750}-R_{705})}{R_{750}+R_{705}+0.16) }$                                    &~\cite{wu2008}                \\
%		PARS            & $\frac{R_{746}}{R_{513}}$                                                                            &~\cite{chappelle1992}          \\
%		PRI             & $\frac{R_{531}-R_{570}}{R_{531}+R_{570}}$                                                            &~\cite{gamon1992}              \\
%		PRI\_norm       & $\frac{PRI \times (-1) }{RDVI\times R_{700}/R_{670}}$                                                &~\cite{zarco-tejada2013}      \\
%		PRI*CI2         & $PRI*CI2$                                                                                            &~\cite{garrity2011}            \\
%		PSRI            & $\frac{R_{678}-R_{500}}{R_{750}}$                                                                    &~\cite{merzlyak1999}           \\
%		PSSR            & $\frac{R_{800}}{R_{635}}$                                                                            &~\cite{blackburn1998}          \\
%		PSND            & $\frac{R_{800}-R_{470}}{R_{800}-R_{470})}$                                                           &~\cite{blackburn1998}          \\
%		PWI             & $\frac{R_{900}}{R_{970}}$                                                                            &~\cite{penuelas1997}           \\
%		RDVI            & $\frac{R_{800}-R_{670}}{ \sqrt{R_{800}+R_{670}}}$                                                    &~\cite{roujean1995}            \\
%		\midrule
%		REP\_LE         & \parbox{3.8cm}{Red-edge position through linear extrapolation}                                       &~\cite{cho2006}                \\
%		\midrule
%		REP\_Li         & $R_{re}=\frac{R_{670}+R_{780})}{2}$                                                                  &~\cite{guyot1988}              \\
%		& $\frac{700 + 40 \times ((R_{re} -R_{700})}{(R_{740}-R_{700}))}$                                      &                               \\
%		\midrule
%		SAVI            & $\frac{(1+L)\times (R_{800}-R_{670})}{(R_{800}+R_{670}+L)}$                                          &~\cite{huete1988}              \\
%		SIPI            & $\frac{R_{800}-R_{445}}{R_{800}-R_{680}}$                                                            &~\cite{penuelasj.1995}         \\
%		\midrule
%		SPVI            & \parbox{3.8cm}{$0.4 \times 3.7 \times (R_{800}-R_{670})-1.2 \times ((R_{530}-R_{670})^2)^{0.5}$}     &~\cite{vincini2006}            \\
%		\midrule
%		SR              & $\frac{R_{800}}{R_{680}}$                                                                            &~\cite{jordan1969}             \\
%		SR1             & $\frac{R_{750}}{R_{700}}$                                                                            &~\cite{gitelson1997}           \\
%		SR2             & $\frac{R_{752}}{R_{690}}$                                                                            &~\cite{gitelson1997}           \\
%		SR3             & $\frac{R_{750}}{R_{550}}$                                                                            &~\cite{gitelson1997}           \\
%		SR4             & $\frac{R_{700}}{R_{670}}$                                                                            &~\cite{mcmurtrey1994}          \\
%		SR5             & $\frac{R_{675}}{R_{700}}$                                                                            &~\cite{chappelle1992}          \\
%		SR6             & $\frac{R_{750}}{R_{710}}$                                                                            &~\cite{zarco-tejada1999}       \\
%		SR7             & $\frac{R_{440}}{R_{690}}$                                                                            &~\cite{lichtenthaler1996}      \\
%		SR8             & $\frac{R_{515}}{R_{550}}$                                                                            &~\cite{hernandez-clemente2012} \\
%		SRPI            & $\frac{R_{430}}{R_{680}}$                                                                            &~\cite{penuelasj.1995}         \\
%		SRWI            & $\frac{R_{850}}{R_{1240}}$                                                                           &~\cite{zarco-tejada2003}      \\
%		Sum\_Dr1        & $\sum_{i=626}^{795} D1_i$                                                                            &~\cite{elvidge1995}            \\
%		Sum\_Dr2        & $\sum_{i=680}^{780} D1_i$                                                                            &~\cite{filella1994}            \\
%		SWIR FI         & $\frac{R_{2133}^2}{R_{2225} \times R_{2209}^3}$                                                      &~\cite{levin2007}              \\
%		\midrule
%		SWIR LI         & \parbox{3.8cm}{$3.87  \times (R_{2210} - R_{2090}) - 27.51 \times (R_{2280} - R_{2090}) - 0.2$}      &~\cite{lobell2001}             \\
%		\midrule
%		SWIR SI         & \parbox{3.8cm}{$-41.59 \times (R_{2210} - R_{2090}) + 1.24 \times (R_{2280} - R_{2090}) + 0.64 $}    &~\cite{lobell2001}             \\
%		\midrule
%		SWIR VI         & \parbox{3.8cm}{$37.72  \times (R_{2210} - R_{2090}) + 6.27 \times (R_{2280} - R_{2090}) + 0.57$}     &~\cite{lobell2001}             \\
%		\midrule
%		TCARI           & \parbox{3.8cm}{$3*((R_{700}-R_{670})-0.2\times R_{700}-R_{550})\times (R_{700}/R_{670}))$}           &~\cite{haboudane2002}          \\
%		\midrule
%		TCARI/OSAVI     & TCARI/OSAVI                                                                                          &~\cite{haboudane2002}          \\
%		\midrule
%		TCARI2          & \parbox{3.8cm}{$3 \times ((R_{750}-R_{705})-0.2 \times (R_{750}-R_{550}) \times (R_{750}/R_{705}))$} &~\cite{wu2008}                \\
%		\midrule
%		TCARI2/OSAVI2   & TCARI2/OSAVI2                                                                                        &~\cite{wu2008}                \\
%		\midrule
%		TGI             & \parbox{3.8cm}{$-0.5 (190 (R_{670} - R_{550} ) - 120 (R_{670} - R_{480}))$}                          &~\cite{hunt2013}               \\
%		\midrule
%		TVI             & \parbox{3.8cm}{$0.5\times (120 \times (R_{750}-R_{550})-200 \times (R_{670}-R_{550}))$}              &~\cite{broge2001}              \\
%		\midrule
%		Vogelmann       & $\frac{R_{740}}{R_{720}}$                                                                            &~\cite{vogelmann1993}          \\
%		Vogelmann2      & $\frac{R_{734}-R_{747}}{R_{715}+R_{726})}$                                                           &~\cite{vogelmann1993}          \\
%		Vogelmann3      & $\frac{D_{715}}{D_{705}}$                                                                            &~\cite{vogelmann1993}          \\
%		Vogelmann4      & $\frac{R_{734}-R_{747}}{R_{715}+R_{720}}$                                                            &~\cite{vogelmann1993}     \\
\bottomrule
	\end{tabular}
	\label{tab:vegindices}}
	\end{specialtable}



\begin{specialtable}[H]\ContinuedFloat
\setlength{\tabcolsep}{7.5mm}
{
	\caption{{\em Cont.}}
	\begin{tabular}{lll}\toprule
		% \begin{supertabular}{0.3\textwidth}{lll}
		\bfseries{Name} & \bfseries{Formula}                                                                                   & \bfseries{Reference~*}         \\		\midrule
%		Boochs          & $D_{703}$                                                                                            &~\cite{boochs1990}             \\
%		Boochs2         & $D_{720}$                                                                                            &~\cite{boochs1990}             \\
%		CAI             & $0.5 \times (R_{2000} + R_{2200}) -R_{2100}$                                                         &~\cite{nagler2003}             \\
%		\midrule
%		CARI            & $a = (R_{700}-R_{550}) / 150$                                                                        &~\cite{walthall1994}           \\
%		& $b = R_{550}-(a\times 550)$                                                                          &                               \\
%		& $\frac{R_{700}\times | (a\times 670+R_{670}+b)}{R_{670}\times(a^2+1)| ^{0.5}}$                       &                               \\
%		\midrule
%		Carter          & $R_{695}/R_{420}$                                                                                    &~\cite{carter1994}             \\
%		Carter2         & $R_{695}/R_{760}$                                                                                    &~\cite{carter1994}             \\
%		Carter3         & $R_{605}/R_{760}$                                                                                    &~\cite{carter1994}             \\
%		Carter4         & $R_{710}/R_{760}$                                                                                    &~\cite{carter1994}             \\
%		Carter5         & $R_{695}/R_{670}$                                                                                    &~\cite{carter1994}             \\
%		Carter6         & $R_{550}$                                                                                            &~\cite{carter1994}             \\
%		CI              & $R_{675}\times R_{690}/R_{683}^2$                                                                    &~\cite{zarco-tejada2003}      \\
%		CI~2             & $R_{760}/R_{700}-1$                                                                                  &~\cite{gitelson2003}           \\
%		ClAInt          & $\int_{600 nm}^{735 nm} R$                                                                           &~\cite{oppelt2004}             \\
%		CRI1            & $1/R_{515}-1/R_{550}$                                                                                &~\cite{gitelson2003}           \\
%		CRI2            & $1/R_{515}-1/R_{770}$                                                                                &~\cite{gitelson2003}           \\
%		CRI3            & $1/R_{515}-1/R_{550}\times R_{770}$                                                                  &~\cite{gitelson2003}           \\
%		CRI4            & $1/R_{515}-1/R_{700}\times R_{770}$                                                                  &~\cite{gitelson2003}           \\
%		D1              & $D_{730}/D_{706}$                                                                                    &~\cite{zarco-tejada2003}      \\
%		D2              & $D_{705}/D_{722}$                                                                                    &~\cite{zarco-tejada2003}      \\
%		Datt            & $(R_{850}-R_{710})/(R_{850}-R_{680})$                                                                &~\cite{datt1999}               \\
%		Datt2           & $R_{850}/R_{710}$                                                                                    &~\cite{datt1999}               \\
%		Datt3           & $D_{754}/D_{704}$                                                                                    &~\cite{datt1999}               \\
%		Datt4           & $R_{672}/(R_{550} \times R_{708})$                                                                   &~\cite{datt1998}               \\
%		Datt5           & $R_{672}/R_{550}$                                                                                    &~\cite{datt1998}               \\
%		Datt6           & $(R_{860})/(R_{550}\times R_{708})$                                                                  &~\cite{datt1998}               \\
		Datt7           & $(R_{860} - R_{2218})/(R_{860} - R_{1928})$                                                          &~\cite{datt1999a}              \\
		Datt8           & $(R_{860} - R_{1788})/(R_{860} - R_{1928})$                                                          &~\cite{datt1999a}              \\
		DD              & $(R_{749}-R_{720})-(R_{701}-R_{672})$                                                                &~\cite{maire2004}              \\
		DDn             & $2\times (R_{710}-R_{660}-R_{760})$                                                                  &~\cite{lemaire2008}            \\
		DPI             & $(D_{688}*D_{710})/D_{697}^2$                                                                        &~\cite{zarco-tejada2003}      \\
		DWSI1           & $R_{80}/R_{1660}$                                                                                    &~\cite{apan2004}               \\
		DWSI2           & $R_{1660}/R_{550}$                                                                                   &~\cite{apan2004}               \\
		DWSI3           & $R_{1660}/R_{680}$                                                                                   &~\cite{apan2004}               \\
		DWSI4           & $R_{550}/R_{680}$                                                                                    &~\cite{apan2004}               \\
		DWSI5           & $(R_{800} + R_{550})/(R_{1660} + R_{680})$                                                           &~\cite{apan2004}               \\
		EGFN            & $\frac{(\max(D_{650:750})\,-\,\max(D_{500:550}))}{(\max(D_{650:750})\,+\,\max(D_{500:550}))}$                &~\cite{penuelas1994}           \\
		EGFR            & $\max(D_{650:750})/\max(D_{500:550})$                                                                &~\cite{penuelas1994}           \\
		EVI             & $\frac{2.5\,\times\, (R_{800}\,-\,R_{670}) }{ (R_{800}\,-\,(6\,\times\, R_{670})\,-\,(7.5\,\times\, R_{475})\,+\,1)}$           &~\cite{huete1997}             \\
		GDVI            & $(R_{800}^n-R_{680}^n) / (R_{800}^n+R_{680}^n)$ \hl{**                                                   } %MDPI: there is no explanation for **.
 &~\cite{wu2014}                 \\
		GI              & $R_{554}/R_{677}$                                                                                    &~\cite{smith1995}              \\
		Gitelson        & $1/R_{700}$                                                                                          &~\cite{gitelson1999}           \\
		Gitelson2       & $(R_{750}-R_{800}/R_{695}-R_{740})-1$                                                                &~\cite{gitelson2003}           \\
		GMI1            & $R_{750}/R_{550}$                                                                                    &~\cite{gitelson2003}           \\
		GMI2            & $R_{750}/R_{700}$                                                                                    &~\cite{gitelson2003}           \\
		Green NDVI      & $\frac{R_{800}\,-\,R_{550}}{R_{800}\,+\,R_{550}}$                                                            &~\cite{gitelson1996}           \\
		LWVI\_1         & $\frac{(R_{1094}\,-\,R_{983})}{(R_{1094}\,+\,R_{983})}$                                                      &~\cite{galvao2005}             \\
		LWVI\_2         & $\frac{R_{1094}\,-\,R_{1205}}{R_{1094}\,+\,R_{1205}}$                                                        &~\cite{galvao2005}             \\
		Maccioni        & $\frac{R_{780}\,-\,R_{710})}{R_{780}\,-\,R_{680}}$                                                           &~\cite{maccioni2001}           \\
		\midrule
		MCARI           & \parbox{5.5cm}{$((R_{700}-R_{670})-0.2\times (R_{700}-R_{550})) \times (R_{700}/R_{670})$}           &~\cite{daughtry2000}           \\
		\midrule
		MCARI2          & \parbox{5.5cm}{$((R_{750}-R_{705})-0.2 \times (R_{750}-R_{550})) \times (R_{750}/R_{705})$}          &~\cite{wu2008}                \\
		\midrule
		mND705          & $\frac{(R_{750}\,-\,R_{705})}{R_{750}\,+\,R_{705}\,-\,2\,\times\, R_{445}}$                                          &~\cite{sims2002}              \\
		mNDVI           & $\frac{(R_{800}\,-\,R_{680})}{R_{800}\,+\,R_{680}\,-\,2 \,\times\, R_{445}}$                                         &~\cite{sims2002}              \\
		MPRI            & $\frac{R_{515}\,-\,R_{530}}{R_{515}\,+\,R_{530}}$                                                            &~\cite{hernandez-clemente2011} \\
		MSAVI           & \parbox{5.5cm}{$0.5 \times ((2\times R_{800}+1)^2-8\times (R_{800}-R_{670}))^{0.5}$}                 &~\cite{qi1994}                 \\
		MSI             & $\frac{R_{1600}}{R_{817}}$                                                                           &~\cite{hunt1989}               \\
		mSR             & $\frac{R_{800}\,-\,R_{445}}{R_{680}\,-\,R_{445}}$                                                            &~\cite{sims2002}              \\
		mSR2            & $\frac{(R_{750}/R_{705})\,-\,1}{R_{750}/R_{705}\,+\,1)^{0.5}}$                                               &~\cite{chen1996}               \\
		mSR705          & $\frac{R_{750}\,-\,R_{445}}{R_{705}\,-\,R_{445}}$                                                            &~\cite{sims2002}              \\
		MTCI            & $\frac{R_{754}\,-\,R_{709}}{R_{709}\,-\,R_{681}}$                                                            &~\cite{dash2007}               \\
		\midrule
		MTVI            & \parbox{3.8cm}{$1.2 \times (1.2 \times (R_{800}-R_{550})-2.5 \times (R_{670}-R_{550}))$}             &~\cite{haboudane2002}          \\
		\midrule
		NDLI            & $\frac{log(1/R_{1754}) \,-\, log(1/R_{1680})}{log(1/R_{1754}) \,+\, log(1/R_{1680})}$                        &~\cite{serrano2002}            \\
		NDNI            & $\frac{log(1/R_{1510}) \,-\, log(1/R_{1680})}{log(1/R_{1510}) \,+\, log(1/R_{1680})}$                        &~\cite{serrano2002}            \\
		NDVI            & $\frac{R_{800}\,-\,R_{680}}{R_{800}\,+\,R_{680}}$                                                            &~\cite{tucker1979}             \\
%		NDVI2           & $\frac{R_{750}-R_{705}}{R_{750}+R_{705}}$                                                            &~\cite{gitelson1994}           \\
%		NDVI3           & $\frac{R_{682}-R_{553}}{R_{682}+R_{553}}$                                                            &~\cite{guanter2005}            \\
%		NDWI            & $\frac{R_{860}-R_{1240}}{R_{860}+R_{1240}}$                                                          &~\cite{gao1996}                \\
%		NPCI            & $\frac{R_{680}-R_{430}}{R_{680}+R_{430}}$                                                            &~\cite{penuelas1994}           \\
%		OSAVI           & $\frac{(1+0.16) \times (R_{800}-R_{670})}{R_{800}+R_{670}+0.16 }$                                    &~\cite{rondeaux1996}           \\
%		OSAVI2          & $\frac{(1+0.16)\times (R_{750}-R_{705})}{R_{750}+R_{705}+0.16) }$                                    &~\cite{wu2008}                \\
%		PARS            & $\frac{R_{746}}{R_{513}}$                                                                            &~\cite{chappelle1992}          \\
%		PRI             & $\frac{R_{531}-R_{570}}{R_{531}+R_{570}}$                                                            &~\cite{gamon1992}              \\
%		PRI\_norm       & $\frac{PRI \times (-1) }{RDVI\times R_{700}/R_{670}}$                                                &~\cite{zarco-tejada2013}      \\
%		PRI*CI2         & $PRI*CI2$                                                                                            &~\cite{garrity2011}            \\
%		PSRI            & $\frac{R_{678}-R_{500}}{R_{750}}$                                                                    &~\cite{merzlyak1999}           \\
%		PSSR            & $\frac{R_{800}}{R_{635}}$                                                                            &~\cite{blackburn1998}          \\
%		PSND            & $\frac{R_{800}-R_{470}}{R_{800}-R_{470})}$                                                           &~\cite{blackburn1998}          \\
%		PWI             & $\frac{R_{900}}{R_{970}}$                                                                            &~\cite{penuelas1997}           \\
%		RDVI            & $\frac{R_{800}-R_{670}}{ \sqrt{R_{800}+R_{670}}}$                                                    &~\cite{roujean1995}            \\
%		\midrule
%		REP\_LE         & \parbox{3.8cm}{Red-edge position through linear extrapolation}                                       &~\cite{cho2006}                \\
%		\midrule
%		REP\_Li         & $R_{re}=\frac{R_{670}+R_{780})}{2}$                                                                  &~\cite{guyot1988}              \\
%		& $\frac{700 + 40 \times ((R_{re} -R_{700})}{(R_{740}-R_{700}))}$                                      &                               \\
%		\midrule
%		SAVI            & $\frac{(1+L)\times (R_{800}-R_{670})}{(R_{800}+R_{670}+L)}$                                          &~\cite{huete1988}              \\
%		SIPI            & $\frac{R_{800}-R_{445}}{R_{800}-R_{680}}$                                                            &~\cite{penuelasj.1995}         \\
%		\midrule
%		SPVI            & \parbox{3.8cm}{$0.4 \times 3.7 \times (R_{800}-R_{670})-1.2 \times ((R_{530}-R_{670})^2)^{0.5}$}     &~\cite{vincini2006}            \\
%		\midrule
%		SR              & $\frac{R_{800}}{R_{680}}$                                                                            &~\cite{jordan1969}             \\
%		SR1             & $\frac{R_{750}}{R_{700}}$                                                                            &~\cite{gitelson1997}           \\
%		SR2             & $\frac{R_{752}}{R_{690}}$                                                                            &~\cite{gitelson1997}           \\
%		SR3             & $\frac{R_{750}}{R_{550}}$                                                                            &~\cite{gitelson1997}           \\
%		SR4             & $\frac{R_{700}}{R_{670}}$                                                                            &~\cite{mcmurtrey1994}          \\
%		SR5             & $\frac{R_{675}}{R_{700}}$                                                                            &~\cite{chappelle1992}          \\
%		SR6             & $\frac{R_{750}}{R_{710}}$                                                                            &~\cite{zarco-tejada1999}       \\
%		SR7             & $\frac{R_{440}}{R_{690}}$                                                                            &~\cite{lichtenthaler1996}      \\
%		SR8             & $\frac{R_{515}}{R_{550}}$                                                                            &~\cite{hernandez-clemente2012} \\
%		SRPI            & $\frac{R_{430}}{R_{680}}$                                                                            &~\cite{penuelasj.1995}         \\
%		SRWI            & $\frac{R_{850}}{R_{1240}}$                                                                           &~\cite{zarco-tejada2003}      \\
%		Sum\_Dr1        & $\sum_{i=626}^{795} D1_i$                                                                            &~\cite{elvidge1995}            \\
%		Sum\_Dr2        & $\sum_{i=680}^{780} D1_i$                                                                            &~\cite{filella1994}            \\
%		SWIR FI         & $\frac{R_{2133}^2}{R_{2225} \times R_{2209}^3}$                                                      &~\cite{levin2007}              \\
%		\midrule
%		SWIR LI         & \parbox{3.8cm}{$3.87  \times (R_{2210} - R_{2090}) - 27.51 \times (R_{2280} - R_{2090}) - 0.2$}      &~\cite{lobell2001}             \\
%		\midrule
%		SWIR SI         & \parbox{3.8cm}{$-41.59 \times (R_{2210} - R_{2090}) + 1.24 \times (R_{2280} - R_{2090}) + 0.64 $}    &~\cite{lobell2001}             \\
%		\midrule
%		SWIR VI         & \parbox{3.8cm}{$37.72  \times (R_{2210} - R_{2090}) + 6.27 \times (R_{2280} - R_{2090}) + 0.57$}     &~\cite{lobell2001}             \\
%		\midrule
%		TCARI           & \parbox{3.8cm}{$3*((R_{700}-R_{670})-0.2\times R_{700}-R_{550})\times (R_{700}/R_{670}))$}           &~\cite{haboudane2002}          \\
%		\midrule
%		TCARI/OSAVI     & TCARI/OSAVI                                                                                          &~\cite{haboudane2002}          \\
%		\midrule
%		TCARI2          & \parbox{3.8cm}{$3 \times ((R_{750}-R_{705})-0.2 \times (R_{750}-R_{550}) \times (R_{750}/R_{705}))$} &~\cite{wu2008}                \\
%		\midrule
%		TCARI2/OSAVI2   & TCARI2/OSAVI2                                                                                        &~\cite{wu2008}                \\
%		\midrule
%		TGI             & \parbox{3.8cm}{$-0.5 (190 (R_{670} - R_{550} ) - 120 (R_{670} - R_{480}))$}                          &~\cite{hunt2013}               \\
%		\midrule
%		TVI             & \parbox{3.8cm}{$0.5\times (120 \times (R_{750}-R_{550})-200 \times (R_{670}-R_{550}))$}              &~\cite{broge2001}              \\
%		\midrule
%		Vogelmann       & $\frac{R_{740}}{R_{720}}$                                                                            &~\cite{vogelmann1993}          \\
%		Vogelmann2      & $\frac{R_{734}-R_{747}}{R_{715}+R_{726})}$                                                           &~\cite{vogelmann1993}          \\
%		Vogelmann3      & $\frac{D_{715}}{D_{705}}$                                                                            &~\cite{vogelmann1993}          \\
%		Vogelmann4      & $\frac{R_{734}-R_{747}}{R_{715}+R_{720}}$                                                            &~\cite{vogelmann1993}     \\
		\bottomrule
	\end{tabular}
	\label{tab:vegindices}}
	\end{specialtable}


\begin{specialtable}[H]\ContinuedFloat
\setlength{\tabcolsep}{11mm}
{
	\caption{{\em Cont.}}
	\begin{tabular}{lll}\toprule
		% \begin{supertabular}{0.3\textwidth}{lll}
		\bfseries{Name} & \bfseries{Formula}                                                                                   & \bfseries{Reference~*}         \\		\midrule
%		Boochs          & $D_{703}$                                                                                            &~\cite{boochs1990}             \\
%		Boochs2         & $D_{720}$                                                                                            &~\cite{boochs1990}             \\
%		CAI             & $0.5 \times (R_{2000} + R_{2200}) -R_{2100}$                                                         &~\cite{nagler2003}             \\
%		\midrule
%		CARI            & $a = (R_{700}-R_{550}) / 150$                                                                        &~\cite{walthall1994}           \\
%		& $b = R_{550}-(a\times 550)$                                                                          &                               \\
%		& $\frac{R_{700}\times | (a\times 670+R_{670}+b)}{R_{670}\times(a^2+1)| ^{0.5}}$                       &                               \\
%		\midrule
%		Carter          & $R_{695}/R_{420}$                                                                                    &~\cite{carter1994}             \\
%		Carter2         & $R_{695}/R_{760}$                                                                                    &~\cite{carter1994}             \\
%		Carter3         & $R_{605}/R_{760}$                                                                                    &~\cite{carter1994}             \\
%		Carter4         & $R_{710}/R_{760}$                                                                                    &~\cite{carter1994}             \\
%		Carter5         & $R_{695}/R_{670}$                                                                                    &~\cite{carter1994}             \\
%		Carter6         & $R_{550}$                                                                                            &~\cite{carter1994}             \\
%		CI              & $R_{675}\times R_{690}/R_{683}^2$                                                                    &~\cite{zarco-tejada2003}      \\
%		CI~2             & $R_{760}/R_{700}-1$                                                                                  &~\cite{gitelson2003}           \\
%		ClAInt          & $\int_{600 nm}^{735 nm} R$                                                                           &~\cite{oppelt2004}             \\
%		CRI1            & $1/R_{515}-1/R_{550}$                                                                                &~\cite{gitelson2003}           \\
%		CRI2            & $1/R_{515}-1/R_{770}$                                                                                &~\cite{gitelson2003}           \\
%		CRI3            & $1/R_{515}-1/R_{550}\times R_{770}$                                                                  &~\cite{gitelson2003}           \\
%		CRI4            & $1/R_{515}-1/R_{700}\times R_{770}$                                                                  &~\cite{gitelson2003}           \\
%		D1              & $D_{730}/D_{706}$                                                                                    &~\cite{zarco-tejada2003}      \\
%		D2              & $D_{705}/D_{722}$                                                                                    &~\cite{zarco-tejada2003}      \\
%		Datt            & $(R_{850}-R_{710})/(R_{850}-R_{680})$                                                                &~\cite{datt1999}               \\
%		Datt2           & $R_{850}/R_{710}$                                                                                    &~\cite{datt1999}               \\
%		Datt3           & $D_{754}/D_{704}$                                                                                    &~\cite{datt1999}               \\
%		Datt4           & $R_{672}/(R_{550} \times R_{708})$                                                                   &~\cite{datt1998}               \\
%		Datt5           & $R_{672}/R_{550}$                                                                                    &~\cite{datt1998}               \\
%		Datt6           & $(R_{860})/(R_{550}\times R_{708})$                                                                  &~\cite{datt1998}               \\
%		Datt7           & $(R_{860} - R_{2218})/(R_{860} - R_{1928})$                                                          &~\cite{datt1999a}              \\
%		Datt8           & $(R_{860} - R_{1788})/(R_{860} - R_{1928})$                                                          &~\cite{datt1999a}              \\
%		DD              & $(R_{749}-R_{720})-(R_{701}-R_{672})$                                                                &~\cite{maire2004}              \\
%		DDn             & $2\times (R_{710}-R_{660}-R_{760})$                                                                  &~\cite{lemaire2008}            \\
%		DPI             & $(D_{688}*D_{710})/D_{697}^2$                                                                        &~\cite{zarco-tejada2003}      \\
%		DWSI1           & $R_{80}/R_{1660}$                                                                                    &~\cite{apan2004}               \\
%		DWSI2           & $R_{1660}/R_{550}$                                                                                   &~\cite{apan2004}               \\
%		DWSI3           & $R_{1660}/R_{680}$                                                                                   &~\cite{apan2004}               \\
%		DWSI4           & $R_{550}/R_{680}$                                                                                    &~\cite{apan2004}               \\
%		DWSI5           & $(R_{800} + R_{550})/(R_{1660} + R_{680})$                                                           &~\cite{apan2004}               \\
%		EGFN            & $\frac{(\max(D_{650:750})-\max(D_{500:550}))}{(\max(D_{650:750})+\max(D_{500:550}))}$                &~\cite{penuelas1994}           \\
%		EGFR            & $\max(D_{650:750})/\max(D_{500:550})$                                                                &~\cite{penuelas1994}           \\
%		EVI             & $\frac{2.5\times ((R_{800}-R_{670}) }{ (R_{800}-(6\times R_{670})-(7.5\times R_{475})+1)}$           &~\cite{huete1997}             \\
%		GDVI            & $(R_{800}^n-R_{680}^n) / (R_{800}^n+R_{680}^n)$**                                                    &~\cite{wu2014}                 \\
%		GI              & $R_{554}/R_{677}$                                                                                    &~\cite{smith1995}              \\
%		Gitelson        & $1/R_{700}$                                                                                          &~\cite{gitelson1999}           \\
%		Gitelson2       & $(R_{750}-R_{800}/R_{695}-R_{740})-1$                                                                &~\cite{gitelson2003}           \\
%		GMI1            & $R_{750}/R_{550}$                                                                                    &~\cite{gitelson2003}           \\
%		GMI2            & $R_{750}/R_{700}$                                                                                    &~\cite{gitelson2003}           \\
%		Green NDVI      & $\frac{R_{800}-R_{550}}{R_{800}+R_{550}}$                                                            &~\cite{gitelson1996}           \\
%		LWVI\_1         & $\frac{(R_{1094}-R_{983})}{(R_{1094}+R_{983})}$                                                      &~\cite{galvao2005}             \\
%		LWVI\_2         & $\frac{R_{1094}-R_{1205}}{R_{1094}+R_{1205}}$                                                        &~\cite{galvao2005}             \\
%		Maccioni        & $\frac{R_{780}-R_{710})}{R_{780}-R_{680}}$                                                           &~\cite{maccioni2001}           \\
%		\midrule
%		MCARI           & \parbox{5.5cm}{$((R_{700}-R_{670})-0.2\times (R_{700}-R_{550})) \times (R_{700}/R_{670})$}           &~\cite{daughtry2000}           \\
%		\midrule
%		MCARI2          & \parbox{5.5cm}{$((R_{750}-R_{705})-0.2 \times (R_{750}-R_{550})) \times (R_{750}/R_{705})$}          &~\cite{wu2008}                \\
%		\midrule
%		mND705          & $\frac{(R_{750}-R_{705})}{R_{750}+R_{705}-2\times R_{445}}$                                          &~\cite{sims2002}              \\
%		mNDVI           & $\frac{(R_{800}-R_{680})}{R_{800}+R_{680}-2 \times R_{445}}$                                         &~\cite{sims2002}              \\
%		MPRI            & $\frac{R_{515}-R_{530}}{R_{515}+R_{530}}$                                                            &~\cite{hernandez-clemente2011} \\
%		MSAVI           & \parbox{5.5cm}{$0.5 \times ((2\times R_{800}+1)^2-8\times (R_{800}-R_{670}))^{0.5}$}                 &~\cite{qi1994}                 \\
%		MSI             & $\frac{R_{1600}}{R_{817}}$                                                                           &~\cite{hunt1989}               \\
%		mSR             & $\frac{R_{800}-R_{445}}{R_{680}-R_{445}}$                                                            &~\cite{sims2002}              \\
%		mSR2            & $\frac{(R_{750}/R_{705})-1}{R_{750}/R_{705}+1)^{0.5}}$                                               &~\cite{chen1996}               \\
%		mSR705          & $\frac{R_{750}-R_{445}}{R_{705}-R_{445}}$                                                            &~\cite{sims2002}              \\
%		MTCI            & $\frac{R_{754}-R_{709}}{R_{709}-R_{681}}$                                                            &~\cite{dash2007}               \\
%		\midrule
%		MTVI            & \parbox{3.8cm}{$1.2 \times (1.2 \times (R_{800}-R_{550})-2.5 \times (R_{670}-R_{550}))$}             &~\cite{haboudane2002}          \\
%		\midrule
%		NDLI            & $\frac{log(1/R_{1754}) - log(1/R_{1680})}{log(1/R_{1754}) + log(1/R_{1680})}$                        &~\cite{serrano2002}            \\
%		NDNI            & $\frac{log(1/R_{1510}) - log(1/R_{1680})}{log(1/R_{1510}) + log(1/R_{1680})}$                        &~\cite{serrano2002}            \\
%		NDVI            & $\frac{R_{800}-R_{680}}{R_{800}+R_{680}}$                                                            &~\cite{tucker1979}             \\
		NDVI2           & $\frac{R_{750}\,-\,R_{705}}{R_{750}\,+\,R_{705}}$                                                            &~\cite{gitelson1994}           \\
		NDVI3           & $\frac{R_{682}\,-\,R_{553}}{R_{682}\,+\,R_{553}}$                                                            &~\cite{guanter2005}            \\
		NDWI            & $\frac{R_{860}\,-\,R_{1240}}{R_{860}\,+\,R_{1240}}$                                                          &~\cite{gao1996}                \\
		NPCI            & $\frac{R_{680}\,-\,R_{430}}{R_{680}\,+\,R_{430}}$                                                            &~\cite{penuelas1994}           \\
		OSAVI           & $\frac{(1\,+\,0.16) \,\times\, (R_{800}\,-\,R_{670})}{R_{800}\,+\,R_{670}\,+\,0.16 }$                                    &~\cite{rondeaux1996}           \\
		OSAVI2          & $\frac{(1\,+\,0.16)\,\times\, (R_{750}\,-\,R_{705})}{R_{750}\,+\,R_{705}\,+\,0.16) }$                                    &~\cite{wu2008}                \\
		PARS            & $\frac{R_{746}}{R_{513}}$                                                                            &~\cite{chappelle1992}          \\
		PRI             & $\frac{R_{531}\,-\,R_{570}}{R_{531}\,+\,R_{570}}$                                                            &~\cite{gamon1992}              \\
		PRI\_norm       & $\frac{PRI \,\times\, (-1) }{RDVI\,\times\, R_{700}/R_{670}}$                                                &~\cite{zarco-tejada2013}      \\
		PRI\,*\,CI2         & $PRI*CI2$                                                                                            &~\cite{garrity2011}            \\
		PSRI            & $\frac{R_{678}\,-\,R_{500}}{R_{750}}$                                                                    &~\cite{merzlyak1999}           \\
		PSSR            & $\frac{R_{800}}{R_{635}}$                                                                            &~\cite{blackburn1998}          \\
		PSND            & $\frac{R_{800}\,-\,R_{470}}{R_{800}-R_{470}}$                                                           &~\cite{blackburn1998}          \\
		PWI             & $\frac{R_{900}}{R_{970}}$                                                                            &~\cite{penuelas1997}           \\
		RDVI            & $\frac{R_{800}\,-\,R_{670}}{ \sqrt{R_{800}\,+\,R_{670}}}$                                                    &~\cite{roujean1995}            \\
		\midrule
		REP\_LE         & \parbox{3.8cm}{Red-edge position through linear extrapolation}                                       &~\cite{cho2006}                \\
		\midrule
		REP\_Li         & $R_{re}=\frac{R_{670}\,+\,R_{780}}{2}$                                                                  &~\cite{guyot1988}              \\
		& $\frac{700 \,+\, 40 \,\times\, (R_{re} \,-\,R_{700})}{(R_{740}\,-\,R_{700}))}$                                      &                               \\
		\midrule
		SAVI            & $\frac{(1\,+\,L)\,\times\, (R_{800}\,-\,R_{670})}{(R_{800}\,+\,R_{670}\,+\,L)}$                                          &~\cite{huete1988}              \\
		SIPI            & $\frac{R_{800}\,-\,R_{445}}{R_{800}\,-\,R_{680}}$                                                            &~\cite{penuelasj.1995}         \\
		\midrule
		SPVI            & \parbox{3.8cm}{$0.4 \,\times\, 3.7 \,\times\, (R_{800}\,-\,R_{670})\,-\,1.2 \,\times\, ((R_{530}-R_{670})^2)^{0.5}$}     &~\cite{vincini2006}            \\
		\midrule
		SR              & $\frac{R_{800}}{R_{680}}$                                                                            &~\cite{jordan1969}             \\
		SR1             & $\frac{R_{750}}{R_{700}}$                                                                            &~\cite{gitelson1997}           \\
		SR2             & $\frac{R_{752}}{R_{690}}$                                                                            &~\cite{gitelson1997}           \\
		SR3             & $\frac{R_{750}}{R_{550}}$                                                                            &~\cite{gitelson1997}           \\
		SR4             & $\frac{R_{700}}{R_{670}}$                                                                            &~\cite{mcmurtrey1994}          \\
		SR5             & $\frac{R_{675}}{R_{700}}$                                                                            &~\cite{chappelle1992}          \\
		SR6             & $\frac{R_{750}}{R_{710}}$                                                                            &~\cite{zarco-tejada1999}       \\
		SR7             & $\frac{R_{440}}{R_{690}}$                                                                            &~\cite{lichtenthaler1996}      \\
		SR8             & $\frac{R_{515}}{R_{550}}$                                                                            &~\cite{hernandez-clemente2012} \\
		SRPI            & $\frac{R_{430}}{R_{680}}$                                                                            &~\cite{penuelasj.1995}         \\
		SRWI            & $\frac{R_{850}}{R_{1240}}$\vspace{6pt}                                                                                                                                                      &~\cite{zarco-tejada2003}      \\
		Sum\_Dr1        & $\sum_{i=626}^{795} D1_i$\vspace{6pt}                                                                            &~\cite{elvidge1995}            \\
		Sum\_Dr2        & $\sum_{i=680}^{780} D1_i$                                                                            &~\cite{filella1994}            \\
		SWIR FI         & $\frac{R_{2133}^2}{R_{2225} \,\times\, R_{2209}^3}$                                                      &~\cite{levin2007}              \\
%		\midrule
%		SWIR LI         & \parbox{3.8cm}{$3.87  \times (R_{2210} - R_{2090}) - 27.51 \times (R_{2280} - R_{2090}) - 0.2$}      &~\cite{lobell2001}             \\
%		\midrule
%		SWIR SI         & \parbox{3.8cm}{$-41.59 \times (R_{2210} - R_{2090}) + 1.24 \times (R_{2280} - R_{2090}) + 0.64 $}    &~\cite{lobell2001}             \\
%		\midrule
%		SWIR VI         & \parbox{3.8cm}{$37.72  \times (R_{2210} - R_{2090}) + 6.27 \times (R_{2280} - R_{2090}) + 0.57$}     &~\cite{lobell2001}             \\
%		\midrule
%		TCARI           & \parbox{3.8cm}{$3*((R_{700}-R_{670})-0.2\times R_{700}-R_{550})\times (R_{700}/R_{670}))$}           &~\cite{haboudane2002}          \\
%		\midrule
%		TCARI/OSAVI     & TCARI/OSAVI                                                                                          &~\cite{haboudane2002}          \\
%		\midrule
%		TCARI2          & \parbox{3.8cm}{$3 \times ((R_{750}-R_{705})-0.2 \times (R_{750}-R_{550}) \times (R_{750}/R_{705}))$} &~\cite{wu2008}                \\
%		\midrule
%		TCARI2/OSAVI2   & TCARI2/OSAVI2                                                                                        &~\cite{wu2008}                \\
%		\midrule
%		TGI             & \parbox{3.8cm}{$-0.5 (190 (R_{670} - R_{550} ) - 120 (R_{670} - R_{480}))$}                          &~\cite{hunt2013}               \\
%		\midrule
%		TVI             & \parbox{3.8cm}{$0.5\times (120 \times (R_{750}-R_{550})-200 \times (R_{670}-R_{550}))$}              &~\cite{broge2001}              \\
%		\midrule
%		Vogelmann       & $\frac{R_{740}}{R_{720}}$                                                                            &~\cite{vogelmann1993}          \\
%		Vogelmann2      & $\frac{R_{734}-R_{747}}{R_{715}+R_{726})}$                                                           &~\cite{vogelmann1993}          \\
%		Vogelmann3      & $\frac{D_{715}}{D_{705}}$                                                                            &~\cite{vogelmann1993}          \\
%		Vogelmann4      & $\frac{R_{734}-R_{747}}{R_{715}+R_{720}}$                                                            &~\cite{vogelmann1993}     \\
		\bottomrule
	\end{tabular}
	\label{tab:vegindices}}
	\end{specialtable}

\begin{specialtable}[H]\ContinuedFloat
\setlength{\tabcolsep}{9mm}
{
	\caption{{\em Cont.}}
	\begin{tabular}{lll}\toprule
		% \begin{supertabular}{0.3\textwidth}{lll}
		\bfseries{Name} & \bfseries{Formula}                                                                                   & \bfseries{Reference~*}         \\		\midrule
%		Boochs          & $D_{703}$                                                                                            &~\cite{boochs1990}             \\
%		Boochs2         & $D_{720}$                                                                                            &~\cite{boochs1990}             \\
%		CAI             & $0.5 \times (R_{2000} + R_{2200}) -R_{2100}$                                                         &~\cite{nagler2003}             \\
%		\midrule
%		CARI            & $a = (R_{700}-R_{550}) / 150$                                                                        &~\cite{walthall1994}           \\
%		& $b = R_{550}-(a\times 550)$                                                                          &                               \\
%		& $\frac{R_{700}\times | (a\times 670+R_{670}+b)}{R_{670}\times(a^2+1)| ^{0.5}}$                       &                               \\
%		\midrule
%		Carter          & $R_{695}/R_{420}$                                                                                    &~\cite{carter1994}             \\
%		Carter2         & $R_{695}/R_{760}$                                                                                    &~\cite{carter1994}             \\
%		Carter3         & $R_{605}/R_{760}$                                                                                    &~\cite{carter1994}             \\
%		Carter4         & $R_{710}/R_{760}$                                                                                    &~\cite{carter1994}             \\
%		Carter5         & $R_{695}/R_{670}$                                                                                    &~\cite{carter1994}             \\
%		Carter6         & $R_{550}$                                                                                            &~\cite{carter1994}             \\
%		CI              & $R_{675}\times R_{690}/R_{683}^2$                                                                    &~\cite{zarco-tejada2003}      \\
%		CI~2             & $R_{760}/R_{700}-1$                                                                                  &~\cite{gitelson2003}           \\
%		ClAInt          & $\int_{600 nm}^{735 nm} R$                                                                           &~\cite{oppelt2004}             \\
%		CRI1            & $1/R_{515}-1/R_{550}$                                                                                &~\cite{gitelson2003}           \\
%		CRI2            & $1/R_{515}-1/R_{770}$                                                                                &~\cite{gitelson2003}           \\
%		CRI3            & $1/R_{515}-1/R_{550}\times R_{770}$                                                                  &~\cite{gitelson2003}           \\
%		CRI4            & $1/R_{515}-1/R_{700}\times R_{770}$                                                                  &~\cite{gitelson2003}           \\
%		D1              & $D_{730}/D_{706}$                                                                                    &~\cite{zarco-tejada2003}      \\
%		D2              & $D_{705}/D_{722}$                                                                                    &~\cite{zarco-tejada2003}      \\
%		Datt            & $(R_{850}-R_{710})/(R_{850}-R_{680})$                                                                &~\cite{datt1999}               \\
%		Datt2           & $R_{850}/R_{710}$                                                                                    &~\cite{datt1999}               \\
%		Datt3           & $D_{754}/D_{704}$                                                                                    &~\cite{datt1999}               \\
%		Datt4           & $R_{672}/(R_{550} \times R_{708})$                                                                   &~\cite{datt1998}               \\
%		Datt5           & $R_{672}/R_{550}$                                                                                    &~\cite{datt1998}               \\
%		Datt6           & $(R_{860})/(R_{550}\times R_{708})$                                                                  &~\cite{datt1998}               \\
%		Datt7           & $(R_{860} - R_{2218})/(R_{860} - R_{1928})$                                                          &~\cite{datt1999a}              \\
%		Datt8           & $(R_{860} - R_{1788})/(R_{860} - R_{1928})$                                                          &~\cite{datt1999a}              \\
%		DD              & $(R_{749}-R_{720})-(R_{701}-R_{672})$                                                                &~\cite{maire2004}              \\
%		DDn             & $2\times (R_{710}-R_{660}-R_{760})$                                                                  &~\cite{lemaire2008}            \\
%		DPI             & $(D_{688}*D_{710})/D_{697}^2$                                                                        &~\cite{zarco-tejada2003}      \\
%		DWSI1           & $R_{80}/R_{1660}$                                                                                    &~\cite{apan2004}               \\
%		DWSI2           & $R_{1660}/R_{550}$                                                                                   &~\cite{apan2004}               \\
%		DWSI3           & $R_{1660}/R_{680}$                                                                                   &~\cite{apan2004}               \\
%		DWSI4           & $R_{550}/R_{680}$                                                                                    &~\cite{apan2004}               \\
%		DWSI5           & $(R_{800} + R_{550})/(R_{1660} + R_{680})$                                                           &~\cite{apan2004}               \\
%		EGFN            & $\frac{(\max(D_{650:750})-\max(D_{500:550}))}{(\max(D_{650:750})+\max(D_{500:550}))}$                &~\cite{penuelas1994}           \\
%		EGFR            & $\max(D_{650:750})/\max(D_{500:550})$                                                                &~\cite{penuelas1994}           \\
%		EVI             & $\frac{2.5\times ((R_{800}-R_{670}) }{ (R_{800}-(6\times R_{670})-(7.5\times R_{475})+1)}$           &~\cite{huete1997}             \\
%		GDVI            & $(R_{800}^n-R_{680}^n) / (R_{800}^n+R_{680}^n)$**                                                    &~\cite{wu2014}                 \\
%		GI              & $R_{554}/R_{677}$                                                                                    &~\cite{smith1995}              \\
%		Gitelson        & $1/R_{700}$                                                                                          &~\cite{gitelson1999}           \\
%		Gitelson2       & $(R_{750}-R_{800}/R_{695}-R_{740})-1$                                                                &~\cite{gitelson2003}           \\
%		GMI1            & $R_{750}/R_{550}$                                                                                    &~\cite{gitelson2003}           \\
%		GMI2            & $R_{750}/R_{700}$                                                                                    &~\cite{gitelson2003}           \\
%		Green NDVI      & $\frac{R_{800}-R_{550}}{R_{800}+R_{550}}$                                                            &~\cite{gitelson1996}           \\
%		LWVI\_1         & $\frac{(R_{1094}-R_{983})}{(R_{1094}+R_{983})}$                                                      &~\cite{galvao2005}             \\
%		LWVI\_2         & $\frac{R_{1094}-R_{1205}}{R_{1094}+R_{1205}}$                                                        &~\cite{galvao2005}             \\
%		Maccioni        & $\frac{R_{780}-R_{710})}{R_{780}-R_{680}}$                                                           &~\cite{maccioni2001}           \\
%		\midrule
%		MCARI           & \parbox{5.5cm}{$((R_{700}-R_{670})-0.2\times (R_{700}-R_{550})) \times (R_{700}/R_{670})$}           &~\cite{daughtry2000}           \\
%		\midrule
%		MCARI2          & \parbox{5.5cm}{$((R_{750}-R_{705})-0.2 \times (R_{750}-R_{550})) \times (R_{750}/R_{705})$}          &~\cite{wu2008}                \\
%		\midrule
%		mND705          & $\frac{(R_{750}-R_{705})}{R_{750}+R_{705}-2\times R_{445}}$                                          &~\cite{sims2002}              \\
%		mNDVI           & $\frac{(R_{800}-R_{680})}{R_{800}+R_{680}-2 \times R_{445}}$                                         &~\cite{sims2002}              \\
%		MPRI            & $\frac{R_{515}-R_{530}}{R_{515}+R_{530}}$                                                            &~\cite{hernandez-clemente2011} \\
%		MSAVI           & \parbox{5.5cm}{$0.5 \times ((2\times R_{800}+1)^2-8\times (R_{800}-R_{670}))^{0.5}$}                 &~\cite{qi1994}                 \\
%		MSI             & $\frac{R_{1600}}{R_{817}}$                                                                           &~\cite{hunt1989}               \\
%		mSR             & $\frac{R_{800}-R_{445}}{R_{680}-R_{445}}$                                                            &~\cite{sims2002}              \\
%		mSR2            & $\frac{(R_{750}/R_{705})-1}{R_{750}/R_{705}+1)^{0.5}}$                                               &~\cite{chen1996}               \\
%		mSR705          & $\frac{R_{750}-R_{445}}{R_{705}-R_{445}}$                                                            &~\cite{sims2002}              \\
%		MTCI            & $\frac{R_{754}-R_{709}}{R_{709}-R_{681}}$                                                            &~\cite{dash2007}               \\
%		\midrule
%		MTVI            & \parbox{3.8cm}{$1.2 \times (1.2 \times (R_{800}-R_{550})-2.5 \times (R_{670}-R_{550}))$}             &~\cite{haboudane2002}          \\
%		\midrule
%		NDLI            & $\frac{log(1/R_{1754}) - log(1/R_{1680})}{log(1/R_{1754}) + log(1/R_{1680})}$                        &~\cite{serrano2002}            \\
%		NDNI            & $\frac{log(1/R_{1510}) - log(1/R_{1680})}{log(1/R_{1510}) + log(1/R_{1680})}$                        &~\cite{serrano2002}            \\
%		NDVI            & $\frac{R_{800}-R_{680}}{R_{800}+R_{680}}$                                                            &~\cite{tucker1979}             \\
%		NDVI2           & $\frac{R_{750}-R_{705}}{R_{750}+R_{705}}$                                                            &~\cite{gitelson1994}           \\
%		NDVI3           & $\frac{R_{682}-R_{553}}{R_{682}+R_{553}}$                                                            &~\cite{guanter2005}            \\
%		NDWI            & $\frac{R_{860}-R_{1240}}{R_{860}+R_{1240}}$                                                          &~\cite{gao1996}                \\
%		NPCI            & $\frac{R_{680}-R_{430}}{R_{680}+R_{430}}$                                                            &~\cite{penuelas1994}           \\
%		OSAVI           & $\frac{(1+0.16) \times (R_{800}-R_{670})}{R_{800}+R_{670}+0.16 }$                                    &~\cite{rondeaux1996}           \\
%		OSAVI2          & $\frac{(1+0.16)\times (R_{750}-R_{705})}{R_{750}+R_{705}+0.16) }$                                    &~\cite{wu2008}                \\
%		PARS            & $\frac{R_{746}}{R_{513}}$                                                                            &~\cite{chappelle1992}          \\
%		PRI             & $\frac{R_{531}-R_{570}}{R_{531}+R_{570}}$                                                            &~\cite{gamon1992}              \\
%		PRI\_norm       & $\frac{PRI \times (-1) }{RDVI\times R_{700}/R_{670}}$                                                &~\cite{zarco-tejada2013}      \\
%		PRI*CI2         & $PRI*CI2$                                                                                            &~\cite{garrity2011}            \\
%		PSRI            & $\frac{R_{678}-R_{500}}{R_{750}}$                                                                    &~\cite{merzlyak1999}           \\
%		PSSR            & $\frac{R_{800}}{R_{635}}$                                                                            &~\cite{blackburn1998}          \\
%		PSND            & $\frac{R_{800}-R_{470}}{R_{800}-R_{470})}$                                                           &~\cite{blackburn1998}          \\
%		PWI             & $\frac{R_{900}}{R_{970}}$                                                                            &~\cite{penuelas1997}           \\
%		RDVI            & $\frac{R_{800}-R_{670}}{ \sqrt{R_{800}+R_{670}}}$                                                    &~\cite{roujean1995}            \\
%		\midrule
%		REP\_LE         & \parbox{3.8cm}{Red-edge position through linear extrapolation}                                       &~\cite{cho2006}                \\
%		\midrule
%		REP\_Li         & $R_{re}=\frac{R_{670}+R_{780})}{2}$                                                                  &~\cite{guyot1988}              \\
%		& $\frac{700 + 40 \times ((R_{re} -R_{700})}{(R_{740}-R_{700}))}$                                      &                               \\
%		\midrule
%		SAVI            & $\frac{(1+L)\times (R_{800}-R_{670})}{(R_{800}+R_{670}+L)}$                                          &~\cite{huete1988}              \\
%		SIPI            & $\frac{R_{800}-R_{445}}{R_{800}-R_{680}}$                                                            &~\cite{penuelasj.1995}         \\
%		\midrule
%		SPVI            & \parbox{3.8cm}{$0.4 \times 3.7 \times (R_{800}-R_{670})-1.2 \times ((R_{530}-R_{670})^2)^{0.5}$}     &~\cite{vincini2006}            \\
%		\midrule
%		SR              & $\frac{R_{800}}{R_{680}}$                                                                            &~\cite{jordan1969}             \\
%		SR1             & $\frac{R_{750}}{R_{700}}$                                                                            &~\cite{gitelson1997}           \\
%		SR2             & $\frac{R_{752}}{R_{690}}$                                                                            &~\cite{gitelson1997}           \\
%		SR3             & $\frac{R_{750}}{R_{550}}$                                                                            &~\cite{gitelson1997}           \\
%		SR4             & $\frac{R_{700}}{R_{670}}$                                                                            &~\cite{mcmurtrey1994}          \\
%		SR5             & $\frac{R_{675}}{R_{700}}$                                                                            &~\cite{chappelle1992}          \\
%		SR6             & $\frac{R_{750}}{R_{710}}$                                                                            &~\cite{zarco-tejada1999}       \\
%		SR7             & $\frac{R_{440}}{R_{690}}$                                                                            &~\cite{lichtenthaler1996}      \\
%		SR8             & $\frac{R_{515}}{R_{550}}$                                                                            &~\cite{hernandez-clemente2012} \\
%		SRPI            & $\frac{R_{430}}{R_{680}}$                                                                            &~\cite{penuelasj.1995}         \\
%		SRWI            & $\frac{R_{850}}{R_{1240}}$                                                                           &~\cite{zarco-tejada2003}      \\
%		Sum\_Dr1        & $\sum_{i=626}^{795} D1_i$                                                                            &~\cite{elvidge1995}            \\
%		Sum\_Dr2        & $\sum_{i=680}^{780} D1_i$                                                                            &~\cite{filella1994}            \\
%		SWIR FI         & $\frac{R_{2133}^2}{R_{2225} \times R_{2209}^3}$                                                      &~\cite{levin2007}              \\
%		\midrule
		SWIR LI         & \parbox{3.8cm}{$3.87  \times (R_{2210} - R_{2090}) - 27.51 \times (R_{2280} - R_{2090}) - 0.2$}      &~\cite{lobell2001}             \\
		\midrule
		SWIR SI         & \parbox{3.8cm}{$-41.59 \times (R_{2210} - R_{2090}) + 1.24 \times (R_{2280} - R_{2090}) + 0.64 $}    &~\cite{lobell2001}             \\
		\midrule
		SWIR VI         & \parbox{3.8cm}{$37.72  \times (R_{2210} - R_{2090}) + 6.27 \times (R_{2280} - R_{2090}) + 0.57$}     &~\cite{lobell2001}             \\
		\midrule
		TCARI           & \parbox{3.8cm}{$3*((R_{700}-R_{670})-0.2\times R_{700}-R_{550})\times (R_{700}/R_{670}))$}           &~\cite{haboudane2002}          \\
		\midrule
		TCARI/OSAVI     & TCARI/OSAVI                                                                                          &~\cite{haboudane2002}          \\
		\midrule
		TCARI2          & \parbox{3.8cm}{$3 \times ((R_{750}-R_{705})-0.2 \times (R_{750}-R_{550}) \times (R_{750}/R_{705}))$} &~\cite{wu2008}                \\
		\midrule
		TCARI2/OSAVI2   & TCARI2/OSAVI2                                                                                        &~\cite{wu2008}                \\
		\midrule
		TGI             & \parbox{3.8cm}{$-0.5 (190 (R_{670} - R_{550} ) - 120 (R_{670} - R_{480}))$}                          &~\cite{hunt2013}               \\
		\midrule
		TVI             & \parbox{3.8cm}{$0.5\times (120 \times (R_{750}-R_{550})-200 \times (R_{670}-R_{550}))$}              &~\cite{broge2001}              \\
		\midrule
		Vogelmann       & $\frac{R_{740}}{R_{720}}$                                                                            &~\cite{vogelmann1993}          \\
		Vogelmann2      & $\frac{R_{734}\,-\,R_{747}}{R_{715}\,+\,R_{726})}$                                                           &~\cite{vogelmann1993}          \\
		Vogelmann3      & $\frac{D_{715}}{D_{705}}$                                                                            &~\cite{vogelmann1993}          \\
		Vogelmann4      & $\frac{R_{734}\,-\,R_{747}}{R_{715}\,+\,R_{720}}$                                                            &~\cite{vogelmann1993}     \\
		\bottomrule
	\end{tabular}
	\label{tab:vegindices}}
	\end{specialtable}
%\pagebreak

\subsection{}

The following information was provided by the Institut Carogràfic i Geològic de Catalunya, which was in charge of image acquisition and data preprocessing.

The AISA EAGLE-II sensor was used for airborne image acquisition with a field of view of 37.7~°.
Its spectral resolution is 2.4~nm and ranges from 400~nm\ to 1000~nm.

The conversion of digital numbers (DN) to spectral radiance was made  using software designed for the instrument.
Images were originally scaled in 12 bits but were radiometrically calibrated to 16 bits, reserving the highest value (65,535) for null values.
The procedure was applied to the 23 previously selected images.
Finally, the geometric and atmospheric corrections were applied to the images.

The aim of this procedure was to reduce the positional errors of the images.
The cartographic reference system in use was EPSG~25830.
Positioning was achieved by coupling an Applanix POS AV 410 system to the sensor, integrating GPS and IMU systems.
The system provides geographic coordinates of the terrain and relative coordinates of the aircraft (attitude) at each scanned line.
Additionally a DSM from GeoEuskadi with a spatial resolution of 1~m was used.
The orthorectified hyperspectral images were compared to orthoimages (1:5000) from GeoEuskadi.
This comparison was used as the base to calculate RMSE, which was below the ground sampling distance in the across and along track~directions.

The radiance measured by an instrument depends on the illumination geometry and the reflective properties of the observed surface.
Radiation may be absorbed or scattered (Rayleigh and Mie scattering).
Scattering is responsible for the adjacency effect, i.e., radiation coming from neighbors' areas to the target pixel.
The MODTRAN algorithm was used to model the effect of the atmosphere on the radiation.
To represent the aerosols of the study area, the rural model was used.
In addition, optical thickness was estimated on pixels with a high vegetation cover.
Columnar water vapor was estimated by a linear regression ratio where the spectral radiance of each pixel at the band of the maximum water absorption (906~nm) is compared to its theoretical value in the absence of absorption.
Nonetheless, this technique is unreliable in the presence of a spectral resolution as in this case.
To resolve this, the water vapor parameter was selected manually according to the smoothness observed on the reflectance peak at 960~nm.
This was combined with a mid-latitude summer atmosphere model.
The output of this procedure was reflectance from the target pixel scaled between 0 and 10,000.

The image acquisitions were originally attempted during one day (29 October 2016).
Due to the variable meteorological conditions, some stands had to be imaged one day later.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{paracol}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% To add notes in main text, please use \endnote{} and un-comment the codes below.
%\begin{adjustwidth}{-5.0cm}{0cm}
%\printendnotes[custom]
%\end{adjustwidth}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\reftitle{References}

% Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here.

%=====================================
% References, variant A: external bibliography
%=====================================
%\externalbibliography{yes}
%\bibliography{Biblio}
\begin{thebibliography}{999}

\bibitem[Lary \em{et~al.}(2016)Lary, Alavi, Gandomi, and Walker]{lary2016}
Lary, D.J.; Alavi, A.H.; Gandomi, A.H.; Walker, A.L.
\newblock Machine Learning in Geosciences and Remote Sensing.
\newblock {\em Geosci. Front.} {\bf 2016}, {\em 7},~3--10, doi:10.1016/j.gsf.2015.07.003.

\bibitem[Ma \em{et~al.}(2015)Ma, Wu, Wang, Huang, Ranjan, Zomaya, and
  Jie]{ma2015}
Ma, Y.; Wu, H.; Wang, L.; Huang, B.; Ranjan, R.; Zomaya, A.; Jie, W.
\newblock Remote Sensing Big Data Computing: Challenges and Opportunities.
\newblock {\em Future Gener. Comput. Syst.} {\bf 2015}, {\em
  51},~47--60.

\bibitem[Mascaro \em{et~al.}(2014)Mascaro, Asner, Knapp, {Kennedy-Bowdoin},
  Martin, Anderson, Higgins, and Chadwick]{mascaro2014}
Mascaro, J.; Asner, G.P.; Knapp, D.E.; {Kennedy-Bowdoin}, T.; Martin, R.E.;
  Anderson, C.; Higgins, M.; Chadwick, K.D.
\newblock A {{Tale}} of {{Two}} ``{{Forests}}'': Random {{Forest Machine
  Learning Aids Tropical Forest Carbon Mapping}}.
\newblock {\em PLoS ONE} {\bf 2014}, {\em 9},~e85993.

\bibitem[Urban \em{et~al.}(2018)Urban, Berger, Mudau, Heckel, Truckenbrodt,
  Onyango~Odipo, Smit, and Schmullius]{urban2018}
Urban, M.; Berger, C.; Mudau, T.E.; Heckel, K.; Truckenbrodt, J.;
  Onyango~Odipo, V.; Smit, I.P.J.; Schmullius, C.
\newblock Surface {{Moisture}} and {{Vegetation Cover Analysis}} for {{Drought
  Monitoring}} in the {{Southern Kruger National Park Using Sentinel}}-1,
  {{Sentinel}}-2, and {{Landsat}}-8.
\newblock {\em Remote Sens.} {\bf 2018}, {\em 10},~1482.

\bibitem[Hawry{\l}o \em{et~al.}(2018)Hawry{\l}o, Bednarz, Wezyk, and
  Szostak]{hawrylo2018}
Hawry{\l}o, P.; Bednarz, B.; Wezyk, P.; Szostak, M.
\newblock Estimating Defoliation of {{Scots}} Pine Stands Using Machine
  Learning Methods and Vegetation Indices of {{Sentinel}}-2.
\newblock {\em Eur. J. Remote Sens.} {\bf 2018}, {\em
  51},~194--204.

\bibitem[Pollastrini \em{et~al.}(2016)Pollastrini, Feducci, Bonal, Fotelli,
  Gessler, Grossiord, Guyot, Jactel, Nguyen, Radoglou, and
  Bussotti]{pollastrini2016}
Pollastrini, M.; Feducci, M.; Bonal, D.; Fotelli, M.; Gessler, A.; Grossiord,
  C.; Guyot, V.; Jactel, H.; Nguyen, D.; Radoglou, K.; et~al.
\newblock Physiological Significance of Forest Tree Defoliation: Results from a
  Survey in a Mixed Forest in {{Tuscany}} (Central {{Italy}}).
\newblock {\em For. Ecol. Manag.} {\bf 2016}, {\em 361},~170--178, doi:10.1016/j.foreco.2015.11.018.

\bibitem[Gottardini \em{et~al.}(2020)Gottardini, Cristofolini, Cristofori,
  Pollastrini, Camin, and Ferretti]{gottardini2020}
Gottardini, E.; Cristofolini, F.; Cristofori, A.; Pollastrini, M.; Camin, F.;
  Ferretti, M.
\newblock A Multi-Proxy Approach Reveals Common and Species-Specific Features
  Associated with Tree Defoliation in Broadleaved Species.
\newblock {\em For. Ecol. Manag.} {\bf 2020}, {\em 467},~118151, doi:10.1016/j.foreco.2020.118151.

\bibitem[Oliva \em{et~al.}(2016)Oliva, Stenlid, {Gr{\"o}nkvist-Wichmann},
  Wahlstr{\"o}m, Jonsson, Drobyshev, and Stenstr{\"o}m]{oliva2016}
Oliva, J.; Stenlid, J.; {Gr{\"o}nkvist-Wichmann}, L.; Wahlstr{\"o}m, K.;
  Jonsson, M.; Drobyshev, I.; Stenstr{\"o}m, E.
\newblock Pathogen-Induced Defoliation of {{Pinus}} Sylvestris Leads to Tree
  Decline and Death from Secondary Biotic Factors.
\newblock {\em For. Ecol. Manag.} {\bf 2016}, {\em 379},~273--280, doi:10.1016/j.foreco.2016.08.011.

\bibitem[Zhang \em{et~al.}(2016)Zhang, Thapa, Ross, and Gann]{zhang2016}
Zhang, K.; Thapa, B.; Ross, M.; Gann, D.
\newblock Remote Sensing of Seasonal Changes and Disturbances in Mangrove
  Forest: A Case Study from {{South Florida}}.
\newblock {\em Ecosphere} {\bf 2016}, \emph{7}, e01366, doi:10.1002/ecs2.1366.

\bibitem[Townsend \em{et~al.}(2012)Townsend, Singh, Foster, Rehberg, Kingdon,
  Eshleman, and Seagle]{townsend2012}
Townsend, P.A.; Singh, A.; Foster, J.R.; Rehberg, N.J.; Kingdon, C.C.;
  Eshleman, K.N.; Seagle, S.W.
\newblock A General {{Landsat}} Model to Predict Canopy Defoliation in
  Broadleaf Deciduous Forests.
\newblock {\em Remote Sens. Environ.} {\bf 2012}, {\em 119},~255--265, doi:10.1016/j.rse.2011.12.023.

\bibitem[Jiang \em{et~al.}(2014)Jiang, Wang, {de Bie}, Skidmore, Liu, Song,
  Zhang, Wang, and Shao]{jiang2014}
Jiang, Y.; Wang, T.; {de Bie}, C.A.J.M.; Skidmore, A.K.; Liu, X.; Song, S.;
  Zhang, L.; Wang, J.; Shao, X.
\newblock Satellite-Derived Vegetation Indices Contribute Significantly to the
  Prediction of Epiphyllous Liverworts.
\newblock {\em Ecol. Indic.} {\bf 2014}, {\em 38},~72--80, doi:10.1016/j.ecolind.2013.10.024.

\bibitem[Adamczyk and Osberger(2015)]{adamczyk2015}
Adamczyk, J.; Osberger, A.
\newblock Red-Edge Vegetation Indices for Detecting and Assessing Disturbances
  in {{Norway}} Spruce Dominated Mountain Forests.
\newblock {\em Int. J. Appl. Earth Obs. Geoinf.} {\bf 2015}, {\em 37},~90--99, doi:10.1016/j.jag.2014.10.013.

\bibitem[Thenkabail \em{et~al.}(2018)Thenkabail, Lyon, and
  Huete]{thenkabail2018}
Thenkabail, P.S.; Lyon, J.G.; Huete, A. (Eds.)
\newblock {\em Hyperspectral Indices and Image Classifications for Agriculture
  and Vegetation}; {CRC Press}:  \hl{Boca Raton, FL, USA,} %newly added information, please confirm
 2018, doi:10.1201/9781315159331.

\bibitem[Thenkabail \em{et~al.}(2000)Thenkabail, Smith, and
  De~Pauw]{thenkabail2000}
Thenkabail, P.S.; Smith, R.B.; De~Pauw, E.
\newblock Hyperspectral Vegetation Indices and Their Relationships with
  Agricultural Crop Characteristics.
\newblock {\em Remote Sens. Environ.} {\bf 2000}, {\em 71},~158--182.

\bibitem[Cai \em{et~al.}(2018)Cai, Luo, Wang, and Yang]{cai2018}
Cai, J.; Luo, J.; Wang, S.; Yang, S.
\newblock Feature Selection in Machine Learning: A New Perspective.
\newblock {\em Neurocomputing} {\bf 2018}, {\em 300},~70--79, doi:10.1016/j.neucom.2017.11.077.

\bibitem[Pinto \em{et~al.}(2020)Pinto, Powell, Peterson, Rosalen, and
  Fernandes]{pinto2020}
Pinto, J.; Powell, S.; Peterson, R.; Rosalen, D.; Fernandes, O.
\newblock Detection of {{Defoliation Injury}} in {{Peanut}} with
  {{Hyperspectral Proximal Remote Sensing}}.
\newblock {\em Remote Sens.} {\bf 2020}, {\em 12},~3828, doi:10.3390/rs12223828.

\bibitem[Yu \em{et~al.}(2021)Yu, Ren, and Luo]{yu2021}
Yu, R.; Ren, L.; Luo, Y.
\newblock Early Detection of Pine Wilt Disease in {{Pinus}} Tabuliformis in
  {{North China}} Using a Field Portable Spectrometer and {{UAV}}-Based
  Hyperspectral Imagery.
\newblock {\em For. Ecosyst.} {\bf 2021}, {\em 8},~44, doi:10.1186/s40663-021-00328-6.

\bibitem[Lin \em{et~al.}(2014)Lin, Yan, Wang, and Song]{lin2014}
Lin, H.; Yan, E.; Wang, G.; Song, R.
\newblock Analysis of Hyperspectral Bands for the Health Diagnosis of Tree
  Species.
\newblock  In Proceedings of the 2014 {{Third International Workshop}} on {{Earth Observation}} and
  {{Remote Sensing Applications}} ({{EORSA}}),  \hl{Changsha, China, 11--14 June} %MDPI: newly added, please confirm. The same to refs. 37, 45, 56, 63, 87, 89, 145
 2014; pp. 448--451, doi:10.1109/EORSA.2014.6927931.

\bibitem[Kayet \em{et~al.}(2019)Kayet, Pathak, Chakrabarty, Singh, Chowdary,
  Kumar, and Sahoo]{kayet2019}
Kayet, N.; Pathak, K.; Chakrabarty, A.; Singh, C.P.; Chowdary, V.M.; Kumar, S.;
  Sahoo, S.
\newblock Forest Health Assessment for Geo-Environmental Planning and
  Management in Hilltop Mining Areas Using {{Hyperion}} and {{Landsat}} Data.
\newblock {\em Ecol. Indic.} {\bf 2019}, {\em 106},~105471, doi:10.1016/j.ecolind.2019.105471.

\bibitem[Dash \em{et~al.}(2017)Dash, Watt, Pearse, Heaphy, and
  Dungey]{dash2017}
Dash, J.P.; Watt, M.S.; Pearse, G.D.; Heaphy, M.; Dungey, H.S.
\newblock Assessing Very High Resolution {{UAV}} Imagery for Monitoring Forest
  Health during a Simulated Disease Outbreak.
\newblock {\em ISPRS J. Photogramm. Remote Sens.} {\bf 2017},
  {\em 131},~1--14, doi:10.1016/j.isprsjprs.2017.07.007.

\bibitem[Trunk(1979)]{trunk1979}
Trunk, G.V.
\newblock A {{Problem}} of {{Dimensionality}}: A {{Simple Example}}.
\newblock {\em IEEE Trans. Pattern Anal. Mach. Intell.}
  {\bf 1979}, {\em \hl{PAMI-1}}, %MDPI: please confirm if this volume is correct.
 306--307.

\bibitem[Xu \em{et~al.}(2016)Xu, Caramanis, and Mannor]{xu2016}
Xu, H.; Caramanis, C.; Mannor, S.
\newblock Statistical {{Optimization}} in {{High Dimensions}}.
\newblock {\em Oper. Res.} {\bf 2016}, {\em 64},~958--979.

\bibitem[Mesanza \em{et~al.}(2016)Mesanza, Iturritxa, and Patten]{mesanza2016}
Mesanza, N.; Iturritxa, E.; Patten, C.L.
\newblock Native Rhizobacteria as Biocontrol Agents of {{Heterobasidion}}
  Annosum s.s. and {{Armillaria}} Mellea Infection of {{Pinus}} Radiata.
\newblock {\em Biol. Control} {\bf 2016}, {\em 101},~8--16, doi:10.1016/j.biocontrol.2016.06.003.

\bibitem[Iturritxa \em{et~al.}(2017)Iturritxa, Trask, Mesanza, Raposo,
  {Elvira-Recuenco}, and Patten]{iturritxa2017}
Iturritxa, E.; Trask, T.; Mesanza, N.; Raposo, R.; {Elvira-Recuenco}, M.;
  Patten, C.L.
\newblock Biocontrol of {{Fusarium}} Circinatum Infection of Young {{Pinus}}
  Radiata Trees.
\newblock {\em Forests} {\bf 2017}, {\em 8},~32, doi:10.3390/f8020032.

\bibitem[Iturritxa \em{et~al.}(2014)Iturritxa, Mesanza, and
  Brenning]{iturritxa2014}
Iturritxa, E.; Mesanza, N.; Brenning, A.
\newblock Spatial Analysis of the Risk of Major Forest Diseases in {{Monterey}}
  Pine Plantations.
\newblock {\em Plant Pathol.} {\bf 2014}, {\em 64},~880--889, doi:10.1111/ppa.12328.

\bibitem[Ganley \em{et~al.}(2009)Ganley, Watt, Manning, and
  Iturritxa]{ganley2009}
Ganley, R.J.; Watt, M.S.; Manning, L.; Iturritxa, E.
\newblock A Global Climatic Risk Assessment of Pitch Canker Disease.
\newblock {\em Can. J. For. Res.} {\bf 2009}, {\em
  39},~2246--2256, doi:10.1139/x09-131.

\bibitem[Innes(1993)]{innes1993}
Innes, J.
\newblock Methods to Estimate Forest Health.
\newblock {\em Silva Fenn.} {\bf 1993}, {\em \hl{27}}, %MDPI: please add page.
 doi:10.14214/sf.a15668.

\bibitem[MacLean and Lidstone(1982)]{maclean1982}
MacLean, D.A.; Lidstone, R.G.
\newblock Defoliation by Spruce Budworm: Estimation by Ocular and Shoot-Count
  Methods and Variability among Branches, Trees, and Stands.
\newblock {\em Can. J. For. Res.} {\bf 1982}, {\em
  12},~582--594, doi:10.1139/x82-090.

\bibitem[{Johnstone Iain M.} and {Titterington D.
  Michael}(2009)]{johnstoneiainm.2009}
{Johnstone Iain M.}.; {Titterington D. Michael}.
\newblock Statistical Challenges of High-Dimensional Data.
\newblock {\em Philos. Trans. R. Soc. A Math. Phys. Eng. Sci.} {\bf 2009}, {\em 367},~4237--4253.

\bibitem[Bommert \em{et~al.}(2020)Bommert, Sun, Bischl, Rahnenf{\"u}hrer, and
  Lang]{bommert2020}
Bommert, A.; Sun, X.; Bischl, B.; Rahnenf{\"u}hrer, J.; Lang, M.
\newblock Benchmark for Filter Methods for Feature Selection in
  High-Dimensional Classification Data.
\newblock {\em Comput. Stat. Data Anal.} {\bf 2020}, {\em
  143},~106839, doi:10.1016/j.csda.2019.106839.

\bibitem[Das(2001)]{das2001}
Das, S.
\newblock \emph{Filters, {{Wrappers}} and a {{Boosting}}-{{Based Hybrid}} for
  {{Feature Selection}}};
\newblock  \hl{{{ICML}}:} %MDPI: please add location of publisher. The same to refs. 39, 45, 56, 62, 76, 125, 142
  2001.

\bibitem[Guyon and Elisseeff(2003)]{guyon2003}
Guyon, I.; Elisseeff, A.
\newblock An Introduction to Variable and Feature Selection.
\newblock {\em J. Mach. Learn. Res.} {\bf 2003}, {\em
  3},~1157--1182.

\bibitem[Jolliffe and Cadima(2016)]{jolliffe2016}
Jolliffe, I.; Cadima, J.
\newblock Principal Component Analysis: A Review and Recent Developments.
\newblock {\em Philos. Trans. R. Soc. A Math. Phys. Eng. Sci.} {\bf 2016}, {\em 374},~20150202.

\bibitem[Drot{\'a}r \em{et~al.}(2017)Drot{\'a}r, {\v S}imo{\v n}{\'a}k,
  Pietrikov{\'a}, Chovanec, Chovancov{\'a}, {\'A}d{\'a}m, Szab{\'o},
  Bal{\'a}{\v z}, and Bi{\v n}as]{drotar2017}
Drot{\'a}r, P.; {\v S}imo{\v n}{\'a}k, S.; Pietrikov{\'a}, E.; Chovanec, M.;
  Chovancov{\'a}, E.; {\'A}d{\'a}m, N.; Szab{\'o}, C.; Bal{\'a}{\v z}, A.;
  Bi{\v n}as, M.
\newblock Comparison of {{Filter Techniques}} for {{Two}}-{{Step Feature
  Selection}}.
\newblock {\em Comput. Inform.} {\bf 2017}, {\em 36},~597--617, doi:10.4149/cai\_2017\_3\_597.

\bibitem[Drot{\'a}r \em{et~al.}(2015)Drot{\'a}r, Gazda, and
  Sm{\'e}kal]{drotar2015}
Drot{\'a}r, P.; Gazda, J.; Sm{\'e}kal, Z.
\newblock An Experimental Comparison of Feature Selection Methods on Two-Class
  Biomedical Datasets.
\newblock {\em Comput. Biol. Med.} {\bf 2015}, {\em 66},~1--10, doi:10.1016/j.compbiomed.2015.08.010.

\bibitem[Abeel \em{et~al.}(2010)Abeel, Helleputte, {Van de Peer}, Dupont, and
  Saeys]{abeel2010}
Abeel, T.; Helleputte, T.; {Van de Peer}, Y.; Dupont, P.; Saeys, Y.
\newblock Robust Biomarker Identification for Cancer Diagnosis with Ensemble
  Feature Selection Methods.
\newblock {\em Bioinformatics} {\bf 2010}, {\em 26},~392--398.

\bibitem[Dietterich(2000)]{dietterich2000}
Dietterich, T.G.
\newblock Ensemble {{Methods}} in {{Machine Learning}}.
\newblock  In Proceedings of the {{First International Workshop}} on {{Multiple
  Classifier Systems}}, \hl{Cagliari, Italy, 21--23 June} 2000; {Springer}:  \hl{Berlin/Heidelberg, Germany,} %newly added information, please confirm
 2000; pp. 1--15.

\bibitem[Polikar(2012)]{polikar2012}
Polikar, R.
\newblock Ensemble {{Learning}}. In {\em Ensemble {{Machine Learning}}: Methods
  and {{Applications}}}; Zhang, C., Ma, Y., Eds.; {Springer}: {Boston, MA, USA},
  2012; pp. 1--34, doi:10.1007/978-1-4419-9326-7\_1.

\bibitem[Feurer \em{et~al.}(2015)Feurer, Klein, Eggensperger, Springenberg,
  Blum, and Hutter]{feurer2015}
Feurer, M.; Klein, A.; Eggensperger, K.; Springenberg, J.; Blum, M.; Hutter, F.
\newblock Efficient and {{Robust Automated Machine Learning}}. In {\em Advances
  in {{Neural Information Processing Systems}} 28}; Cortes, C., Lawrence, N.D.,
  Lee, D.D., Sugiyama, M., Garnett, R., Eds.; {\hl{Curran Associates, Inc.}}:  2015;
  pp. 2962--2970.

\bibitem[{Bol{\'o}n-Canedo} and {Alonso-Betanzos}(2019)]{bolon-canedo2019}
{Bol{\'o}n-Canedo}, V.; {Alonso-Betanzos}, A.
\newblock Ensembles for Feature Selection: A Review and Future Trends.
\newblock {\em Inf. Fusion} {\bf 2019}, {\em 52},~1--12, doi:10.1016/j.inffus.2018.11.008.

\bibitem[Pearson(1901)]{pearson1901}
Pearson, K.
\newblock {{LIII}}. {{On}} Lines and Planes of Closest Fit to Systems of Points
  in Space.
\newblock {\em  Lond. Edinb. Dublin Philos. Mag. J. Sci.} {\bf 1901}, {\em 2},~559--572.

\bibitem[Quinlan(1986)]{quinlan1986}
Quinlan, J.R.
\newblock Induction of Decision Trees.
\newblock {\em Mach. Learn.} {\bf 1986}, {\em 1},~81--106.

\bibitem[Zhao(2013)]{zhao2013}
Zhao, X.M.
\newblock Maximum {{Relevance}}/{{Minimum Redundancy}} ({{MRMR}}). In {\em
  Encyclopedia of {{Systems Biology}}}; Dubitzky, W., Wolkenhauer, O., Cho,
  K.H.,Yokota, H., Eds.; {Springer}: {New York, NY, USA},  2013; pp.
  1191--1192, doi:10.1007/978-1-4419-9863-7\_432.

\bibitem[Zuber and Strimmer(2011)]{zuber2011}
Zuber, V.; Strimmer, K.
\newblock High-{{Dimensional Regression}} and {{Variable Selection Using CAR
  Scores}}.
\newblock {\em Stat. Appl. Genet. Mol. Biol.} {\bf
  2011}, {\em \hl{10}}. %MDPI: please add page. The same to ref. 69


\bibitem[Kira and Rendell(1992)]{kira1992}
Kira, K.; Rendell, L.A.
\newblock The Feature Selection Problem: Traditional Methods and a New
  Algorithm.
\newblock  In Proceedings of the Tenth National Conference on {{Artificial}}
  Intelligence, \hl{San Jose, CA, USA, 12--16 July} 1992; {\hl{AAAI Press}}:  1992; pp. 129--134.

\bibitem[Fleuret(2004)]{fleuret2004}
Fleuret, F.
\newblock Fast {{Binary Feature Selection}} with {{Conditional Mutual
  Information}}.
\newblock {\em  J. Mach. Learn. Res.} {\bf 2004}, {\em
  5},~1531--1555.

\bibitem[Shannon(1948)]{shannon1948}
Shannon, C.E.
\newblock A Mathematical Theory of Communication.
\newblock {\em  Bell Syst. Tech. J.} {\bf 1948}, {\em
  27},~379--423, doi:10.1002/j.1538-7305.1948.tb01338.x.

\bibitem[Hastie \em{et~al.}(2001)Hastie, Friedman, and Tibshirani]{hastie2001}
Hastie, T.; Friedman, J.; Tibshirani, R.
\newblock {\em The Elements of Statistical Learning}; {Springer}: New York, NY, USA,
  2001, doi:10.1007/978-0-387-21606-5.

\bibitem[Pe{\~n}a \em{et~al.}(2017)Pe{\~n}a, Liao, and Brenning]{pena2017}
Pe{\~n}a, M.; Liao, R.; Brenning, A.
\newblock Using Spectrotemporal Indices to Improve the Fruit-Tree Crop
  Classification Accuracy.
\newblock {\em ISPRS J. Photogramm. Remote Sens.} {\bf 2017},
  {\em 128},~158--169.

\bibitem[Bischl \em{et~al.}(2017)Bischl, Richter, Bossek, Horn, Thomas, and
  Lang]{mlrmbo}
Bischl, B.; Richter, J.; Bossek, J.; Horn, D.; Thomas, J.; Lang, M.
\newblock {{mlrMBO}}: A {{Modular Framework}} for {{Model}}-{{Based
  Optimization}} of {{Expensive Black}}-{{Box Functions}}.
\newblock {\em arXiv} {\bf 2017}, arXiv:1703.03373.


\bibitem[Binder \em{et~al.}(2020)Binder, Moosbauer, Thomas, and
  Bischl]{binder2020}
Binder, M.; Moosbauer, J.; Thomas, J.; Bischl, B.
\newblock Multi-{{Objective Hyperparameter Tuning}} and {{Feature Selection}}
  Using {{Filter Ensembles}}.
\newblock {\em arXiv} {\bf 2020},
  arXiv:1912.12912

\bibitem[Schratz \em{et~al.}(2019)Schratz, Muenchow, Iturritxa, Richter, and
  Brenning]{schratz2019}
Schratz, P.; Muenchow, J.; Iturritxa, E.; Richter, J.; Brenning, A.
\newblock Hyperparameter Tuning and Performance Assessment of Statistical and
  Machine-Learning Algorithms Using Spatial Data.
\newblock {\em Ecol. Model.} {\bf 2019}, {\em 406},~109--120, doi:10.1016/j.ecolmodel.2019.06.002.

\bibitem[Hutter \em{et~al.}(2011)Hutter, Hoos, and {Leyton-Brown}]{hutter2011}
Hutter, F.; Hoos, H.H.; {Leyton-Brown}, K.
\newblock \emph{Sequential Model-Based Optimization for General Algorithm
  Configuration}; Lecture {{Notes}} in {{Computer Science}}; {Springer:
  Berlin/Heidelberg, Germany},  2011; pp. 507--523, doi:10.1007/978-3-642-25566-3\_40.

\bibitem[Jones \em{et~al.}(1998)Jones, Schonlau, and Welch]{jones1998}
Jones, D.R.; Schonlau, M.; Welch, W.J.
\newblock Efficient Global Optimization of Expensive Black-Box Functions.
\newblock {\em J. Glob. Optim.} {\bf 1998}, {\em 13},~455--492, doi:10.1023/a:1008306431147.

\bibitem[Bergstra and Bengio(2012)]{bergstra2012}
Bergstra, J.; Bengio, Y.
\newblock Random {{Search}} for {{Hyper}}-Parameter {{Optimization}}.
\newblock {\em J. Mach. Learn. Res.} {\bf 2012}, {\em 13},~281--305.

\bibitem[Brenning(2012)]{sperrorest}
Brenning, A.
\newblock Spatial Cross-Validation and Bootstrap for the Assessment of
  Prediction Rules in Remote Sensing: The {{R}} Package Sperrorest.
\newblock  In Proceedings of the 2012 {{IEEE International Geoscience}} and {{Remote Sensing
  Symposium}}, \hl{Munich, Germany, 22--27 July} 2012; {R} Package Version 2.1.0; {\hl{IEEE}}:  2012,
\newblock
  doi:{\changeurlcolor{black}\href{https://doi.org/10.1109/igarss.2012.6352393}{\detokenize{10.1109/igarss.2012.6352393}}}.

\bibitem[Friedman(2001)]{friedman2001}
Friedman, J.H.
\newblock Greedy Function Approximation: A Gradient Boosting Machine.
\newblock {\em Ann. Stat.} {\bf 2001}, {\em 29},~1189--1232, doi:10.1214/aos/1013203451.

\bibitem[Greenwell \em{et~al.}(2018)Greenwell, Boehmke, and
  McCarthy]{greenwell2018}
Greenwell, B.M.; Boehmke, B.C.; McCarthy, A.J.
\newblock A {{Simple}} and {{Effective Model}}-{{Based Variable Importance
  Measure}}.
\newblock {\em arXiv} {\bf 2018}, arXiv:1805.04755

\bibitem[Molnar(2019)]{molnar2019}
Molnar, C.
\newblock {\em Interpretable Machine Learning---{{A Guide}} for {{Making Black
  Box Models Explainable}}};  \hl{2019.} %MDPI: please add publisher and location. The same to ref. 66, 67, 74, 100


\bibitem[Brenning(2021)]{brenning2021}
Brenning, A.
\newblock Transforming {{Feature Space}} to {{Interpret Machine Learning
  Models}}.
\newblock {\em arXiv} {\bf 2021}, arXiv:2104.04295

\bibitem[Apley and Zhu(2019)]{apley2019}
Apley, D.W.; Zhu, J.
\newblock Visualizing the {{Effects}} of {{Predictor Variables}} in {{Black Box
  Supervised Learning Models}}.
\newblock {\em arXiv} {\bf 2019}, arXiv:1612.08468

\bibitem[{R Core Team}(2019)]{rcoreteam2019}
{R Core Team}.
\newblock {\em R: A {{Language}} and {{Environment}} for {{Statistical
  Computing}}};  \hl{R Core Team:} 2019.

\bibitem[Chen and Guestrin(2016)]{chen2016}
Chen, T.; Guestrin, C.
\newblock {{XGBoost}}: A {{Scalable Tree Boosting System}}.
\newblock  In Proceedings of the {{22nd ACM SIGKDD International Conference}} on
  {{Knowledge Discovery}} and {{Data Mining}}, {{KDD}} '16, \hl{San Francisco, CA, USA, 13--17 August} 2016; {ACM}: {New York, NY, USA},
  2016; pp. 785--794, doi:10.1145/2939672.2939785.

\bibitem[Karatzoglou \em{et~al.}(2004)Karatzoglou, Smola, Hornik, and
  Zeileis]{kernlab}
Karatzoglou, A.; Smola, A.; Hornik, K.; Zeileis, A.
\newblock Kernlab---{{An S4 Package}} for {{Kernel Methods}} in {{R}}.
\newblock {\em J. Stat. Softw.} {\bf 2004}, {\em 11},~1--20, doi:10.18637/jss.v011.i09.

\bibitem[Friedman \em{et~al.}(2010)Friedman, Hastie, and Tibshirani]{glmnet}
Friedman, J.; Hastie, T.; Tibshirani, R.
\newblock Regularization Paths for Generalized Linear Models via Coordinate
  Descent.
\newblock {\em J. Stat. Softw.} {\bf 2010}, {\em 33},~1--22, doi:10.18637/jss.v033.i01.

\bibitem[Kursa(2018)]{praznik}
Kursa, M.B.
\newblock {\em Praznik: Collection of {{Information}}-{{Based Feature Selection
  Filters}}};  \hl{2018.}

\bibitem[Zawadzki and Kosinski(2019)]{fselectorrcpp}
Zawadzki, Z.; Kosinski, M.
\newblock {\em {{FSelectorRcpp}}: '{{Rcpp}}' {{Implementation}} of
  '{{FSelector}}' {{Entropy}}-{{Based Feature Selection Algorithms}} with a
  {{Sparse Matrix Support}}};  \hl{2019.}

\bibitem[Bischl \em{et~al.}(2016)Bischl, Lang, Kotthoff, Schiffner, Richter,
  Studerus, Casalicchio, and Jones]{mlr}
Bischl, B.; Lang, M.; Kotthoff, L.; Schiffner, J.; Richter, J.; Studerus, E.;
  Casalicchio, G.; Jones, Z.M.
\newblock mlr: Machine learning in {R}.
\newblock {\em J. Mach. Learn. Res.} {\bf 2016}, {\em
  17},~1--5.

\bibitem[Landau(2018)]{drake}
Landau, W.M.
\newblock The Drake {{R}} Package: A Pipeline Toolkit for Reproducibility and
  High-Performance Computing.
\newblock {\em J. Open Source Softw.} {\bf 2018}, {\em \hl{3}}.

\bibitem[Ghosh \em{et~al.}(2019)Ghosh, Adhikary, Ghosh, Sardar, Begum, and
  Sarkar]{ghosh2019}
Ghosh, M.; Adhikary, S.; Ghosh, K.K.; Sardar, A.; Begum, S.; Sarkar, R.
\newblock Genetic Algorithm Based Cancerous Gene Identification from Microarray
  Data Using Ensemble of Filter Methods.
\newblock {\em Med. Biol. Eng. Comput.} {\bf 2019}, {\em
  57},~159--176, doi:10.1007/s11517-018-1874-4.

\bibitem[Horler \em{et~al.}(1983)Horler, Dockray, and Barber]{horler1983}
Horler, D.N.H.; Dockray, M.; Barber, J.
\newblock The Red Edge of Plant Leaf Reflectance.
\newblock {\em Int. J. Remote Sens.} {\bf 1983}, {\em
  4},~273--288, doi:10.1080/01431168308948546.

\bibitem[Hais \em{et~al.}(2019)Hais, Hellebrandov{\'a}, and {\v
  S}r{\'a}mek]{hais2019}
Hais, M.; Hellebrandov{\'a}, K.N.; {\v S}r{\'a}mek, V.
\newblock Potential of {{Landsat}} Spectral Indices in Regard to the Detection
  of Forest Health Changes Due to Drought Effects.
\newblock {\em J. For. Sci.} {\bf 2019}, {\em 65},~70--78, doi:10.17221/137/2018-JFS.

\bibitem[Gao(1996)]{gao1996}
Gao, B.C.
\newblock {{NDWI}}\textemdash{{A}} Normalized Difference Water Index for Remote
  Sensing of Vegetation Liquid Water from Space.
\newblock {\em Remote Sens. Environ.} {\bf 1996}, {\em 58},~257--266.

\bibitem[Lehnert \em{et~al.}(2016)Lehnert, Meyer, and Bendix]{lehnert2016}
Lehnert, L.W.; Meyer, H.; Bendix, J.
\newblock {\em Hsdar: Manage, Analyse and Simulate Hyperspectral Data in
  {{R}}};  \hl{2016.}

\bibitem[{de Beurs} and Townsend(2008)]{debeurs2008}
{de Beurs}, K.M.; Townsend, P.A.
\newblock Estimating the Effect of Gypsy Moth Defoliation Using {{MODIS}}.
\newblock {\em Remote Sens. Environ.} {\bf 2008}, {\em
  112},~3983--3990.

\bibitem[Rengarajan and Schott(2016)]{rengarajan2016}
Rengarajan, R.; Schott, J.R.
\newblock Modeling Forest Defoliation Using Simulated {{BRDF}} and Assessing
  Its Effect on Reflectance and Sensor Reaching Radiance.
\newblock  In \emph{Remote {{Sensing}} and {{Modeling}} of {{Ecosystems}} for
  {{Sustainability XIII}}}; {\hl{International Society for Optics and Photonics}}:
  2016; Volume 9975, p. 997503, doi:10.1117/12.2235391.

\bibitem[Meng \em{et~al.}(2018)Meng, Dennison, Zhao, Shendryk, Rickert,
  Hanavan, Cook, and Serbin]{meng2018}
Meng, R.; Dennison, P.E.; Zhao, F.; Shendryk, I.; Rickert, A.; Hanavan, R.P.;
  Cook, B.D.; Serbin, S.P.
\newblock Mapping Canopy Defoliation by Herbivorous Insects at the Individual
  Tree Level Using Bi-Temporal Airborne Imaging Spectroscopy and {{LiDAR}}
  Measurements.
\newblock {\em Remote Sens. Environ.} {\bf 2018}, {\em 215},~170--183.

\bibitem[K{\"a}lin \em{et~al.}(2019)K{\"a}lin, Lang, Hug, Gessler, and
  Wegner]{kalin2019}
K{\"a}lin, U.; Lang, N.; Hug, C.; Gessler, A.; Wegner, J.D.
\newblock Defoliation Estimation of Forest Trees from Ground-Level Images.
\newblock {\em Remote Sens. Environ.} {\bf 2019}, {\em 223},~143--153.

\bibitem[Goodbody \em{et~al.}(2018)Goodbody, Coops, Hermosilla, Tompalski,
  McCartney, and MacLean]{goodbody2018}
Goodbody, T.R.H.; Coops, N.C.; Hermosilla, T.; Tompalski, P.; McCartney, G.;
  MacLean, D.A.
\newblock Digital Aerial Photogrammetry for Assessing Cumulative Spruce Budworm
  Defoliation and Enhancing Forest Inventories at a Landscape-Level.
\newblock {\em ISPRS J. Photogramm. Remote Sens.} {\bf 2018},
  {\em 142},~1--11, doi:10.1016/j.isprsjprs.2018.05.012.

\bibitem[Hlebarska and Georgieva(2018)]{hlebarska2018}
Hlebarska, S.; Georgieva, M.
\newblock {Distribution of the invasive pathogen Diplodia sapinea on \emph{Pinus} spp.
  in Bulgaria.}
\newblock In Proceedings of the 90 Years Forest Research Institute---For
  the Society and Nature, Sofia, Bulgaria, 24--26 October 2018;   pp.
  61--70.

\bibitem[Kaya \em{et~al.}(2019)Kaya, Yeltekin, Lehtijarvi, Lehtijarvi, and
  Woodward]{kaya2019}
Kaya, A.G.A.; Yeltekin, {\c S}.; Lehtijarvi, T.D.; Lehtijarvi, A.; Woodward, S.
\newblock Severity of {{Diplodia}} Shoot Blight (Caused by {{Diplodia}}
  Sapinea) Was Greatest on {{Pinus}} Sylvestris and {{Pinus}} Nigra in a
  Plantation Containing Five Pine Species.
\newblock {\em Phytopathol. Mediterr.} {\bf 2019}, {\em 58},~249--259.

\bibitem[Belgiu and Dr{\u a}gu{\c t}(2016)]{belgiu2016}
Belgiu, M.; Dr{\u a}gu{\c t}, L.
\newblock Random Forest in Remote Sensing: A Review of Applications and Future
  Directions.
\newblock {\em ISPRS J. Photogramm. Remote Sens.} {\bf 2016},
  {\em 114},~24--31, doi:10.1016/j.isprsjprs.2016.01.011.

\bibitem[Xia \em{et~al.}(2015)Xia, Liao, Chanussot, Du, Song, and
  Philips]{xia2015}
Xia, J.; Liao, W.; Chanussot, J.; Du, P.; Song, G.; Philips, W.
\newblock Improving {{Random Forest with Ensemble}} of {{Features}} and
  {{Semisupervised Feature Extraction}}.
\newblock {\em IEEE Geosci. Remote Sens. Lett.} {\bf 2015}, {\em
  12},~1471--1475, doi:10.1109/LGRS.2015.2409112.

\bibitem[Fassnacht \em{et~al.}(2014)Fassnacht, Neumann, F{\"o}rster,
  Buddenbaum, Ghosh, Clasen, Joshi, and Koch]{fassnacht2014}
Fassnacht, F.E.; Neumann, C.; F{\"o}rster, M.; Buddenbaum, H.; Ghosh, A.;
  Clasen, A.; Joshi, P.K.; Koch, B.
\newblock Comparison of {{Feature Reduction Algorithms}} for {{Classifying Tree
  Species With Hyperspectral Data}} on {{Three Central European Test Sites}}.
\newblock {\em IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.} {\bf 2014}, {\em 7},~2547--2561, doi:10.1109/JSTARS.2014.2329390.

\bibitem[Feng \em{et~al.}(2016)Feng, Jiao, Liu, Sun, and Zhang]{feng2016}
Feng, J.; Jiao, L.; Liu, F.; Sun, T.; Zhang, X.
\newblock Unsupervised Feature Selection Based on Maximum Information and
  Minimum Redundancy for Hyperspectral Images.
\newblock {\em Pattern Recognit.} {\bf 2016}, {\em 51},~295--309, doi:10.1016/j.patcog.2015.08.018.

\bibitem[Georganos \em{et~al.}(2018)Georganos, Grippa, Vanhuysse, Lennert,
  Shimoni, Kalogirou, and Wolff]{georganos2018}
Georganos, S.; Grippa, T.; Vanhuysse, S.; Lennert, M.; Shimoni, M.; Kalogirou,
  S.; Wolff, E.
\newblock Less Is More: Optimizing Classification Performance through Feature
  Selection in a Very-High-Resolution Remote Sensing Object-Based Urban
  Application.
\newblock {\em GISci. Remote Sens.} {\bf 2018}, {\em 55},~221--242, doi:10.1080/15481603.2017.1408892.

\bibitem[Rochac and Zhang(2016)]{rochac2016}
Rochac, J.F.R.; Zhang, N.
\newblock Feature Extraction in Hyperspectral Imaging Using Adaptive Feature
  Selection Approach.
\newblock  In Proceedings of the 2016 {{Eighth International Conference}} on {{Advanced Computational
  Intelligence}} ({{ICACI}}),  \hl{Chiang Mai, Thailand, 14--16 February} 2016; pp. 36--40, doi:10.1109/ICACI.2016.7449799.

\bibitem[Pal and Foody(2010)]{pal2010}
Pal, M.; Foody, G.M.
\newblock Feature {{Selection}} for {{Classification}} of {{Hyperspectral
  Data}} by {{SVM}}.
\newblock {\em IEEE Trans. Geosci. Remote Sens.} {\bf 2010},
  {\em 48},~2297--2307, doi:10.1109/TGRS.2009.2039484.

\bibitem[Keller \em{et~al.}(2016)Keller, Braun, Hinz, and Weinmann]{keller2016}
Keller, S.; Braun, A.C.; Hinz, S.; Weinmann, M.
\newblock Investigation of the Impact of Dimensionality Reduction and Feature
  Selection on the Classification of Hyperspectral {{EnMAP}} Data.
\newblock  In Proceedings of the 2016 8th {{Workshop}} on {{Hyperspectral Image}} and {{Signal
  Processing}}: Evolution in {{Remote Sensing}} ({{WHISPERS}}),  \hl{Los Angeles, CA, USA, 21--24 August} 2016; pp.
  1--5, doi:10.1109/WHISPERS.2016.8071759.

\bibitem[Xu \em{et~al.}(2019)Xu, Zhao, Yin, Zhang, Liu, and Yang]{xu2019}
Xu, S.; Zhao, Q.; Yin, K.; Zhang, F.; Liu, D.; Yang, G.
\newblock Combining Random Forest and Support Vector Machines for Object-Based
  Rural-Land-Cover Classification Using High Spatial Resolution Imagery.
\newblock {\em J. Appl. Remote Sens.} {\bf 2019}, {\em 13},~014521, doi:10.1117/1.JRS.13.014521.

\bibitem[Shendryk \em{et~al.}(2016)Shendryk, Broich, Tulbure, McGrath, Keith,
  and Alexandrov]{shendryk2016}
Shendryk, I.; Broich, M.; Tulbure, M.G.; McGrath, A.; Keith, D.; Alexandrov,
  S.V.
\newblock Mapping Individual Tree Health Using Full-Waveform Airborne Laser
  Scans and Imaging Spectroscopy: A Case Study for a Floodplain Eucalypt
  Forest.
\newblock {\em Remote Sens. Environ.} {\bf 2016}, {\em 187},~202--217.

\bibitem[Ludwig \em{et~al.}(2019)Ludwig, Morgenthal, Detsch, Higginbottom,
  Lezama~Valdes, Nau{\ss}, and Meyer]{ludwig2019}
Ludwig, M.; Morgenthal, T.; Detsch, F.; Higginbottom, T.P.; Lezama~Valdes, M.;
  Nau{\ss}, T.; Meyer, H.
\newblock Machine Learning and Multi-Sensor Based Modelling of Woody Vegetation
  in the {{Molopo Area}}, {{South Africa}}.
\newblock {\em Remote Sens. Environ.} {\bf 2019}, {\em 222},~195--203.

\bibitem[Meyer \em{et~al.}(2018)Meyer, Reudenbach, Hengl, Katurji, and
  Nauss]{meyer2018}
Meyer, H.; Reudenbach, C.; Hengl, T.; Katurji, M.; Nauss, T.
\newblock Improving Performance of Spatio-Temporal Machine Learning Models
  Using Forward Feature Selection and Target-Oriented Validation.
\newblock {\em Environ. Model. Softw.} {\bf 2018}, {\em
  101},~1--9, doi:10.1016/j.envsoft.2017.12.001.

\bibitem[Zandler \em{et~al.}(2015)Zandler, Brenning, and Samimi]{zandler2015}
Zandler, H.; Brenning, A.; Samimi, C.
\newblock Quantifying Dwarf Shrub Biomass in an Arid Environment: Comparing
  Empirical Methods in a High Dimensional Setting.
\newblock {\em Remote Sens. Environ.} {\bf 2015}, {\em 158},~140--155, doi:10.1016/j.rse.2014.11.007.

\bibitem[Guo \em{et~al.}(2019)Guo, Chung, Li, and Zhang]{guo2019}
Guo, Y.; Chung, F.L.; Li, G.; Zhang, L.
\newblock Multi-{{Label Bioinformatics Data Classification with Ensemble
  Embedded Feature Selection}}.
\newblock {\em IEEE Access} {\bf 2019}, {\em 7},~103863--103875, doi:10.1109/ACCESS.2019.2931035.

\bibitem[Radovic \em{et~al.}(2017)Radovic, Ghalwash, Filipovic, and
  Obradovic]{radovic2017}
Radovic, M.; Ghalwash, M.; Filipovic, N.; Obradovic, Z.
\newblock Minimum Redundancy Maximum Relevance Feature Selection Approach for
  Temporal Gene Expression Data.
\newblock {\em BMC Bioinform.} {\bf 2017}, {\em 18},~9, doi:10.1186/s12859-016-1423-9.

\bibitem[Vogelmann \em{et~al.}(1993)Vogelmann, Rock, and Moss]{vogelmann1993}
Vogelmann, J.E.; Rock, B.N.; Moss, D.M.
\newblock Red Edge Spectral Measurements from Sugar Maple Leaves.
\newblock {\em Int. J. Remote Sens.} {\bf 1993}, {\em
  14},~1563--1575, doi:10.1080/01431169308953986.

\bibitem[Boochs \em{et~al.}(1990)Boochs, Kupfer, Dockter, and
  K{\"u}hbauch]{boochs1990}
Boochs, F.; Kupfer, G.; Dockter, K.; K{\"u}hbauch, W.
\newblock Shape of the Red Edge as Vitality Indicator for Plants.
\newblock {\em Int. J. Remote Sens.} {\bf 1990}, {\em
  11},~1741--1753, doi:10.1080/01431169008955127.

\bibitem[Nagler \em{et~al.}(2003)Nagler, Inoue, Glenn, Russ, and
  Daughtry]{nagler2003}
Nagler, P.L.; Inoue, Y.; Glenn, E.P.; Russ, A.L.; Daughtry, C.S.T.
\newblock Cellulose Absorption Index ({{CAI}}) to Quantify Mixed
  Soil\textendash Plant Litter Scenes.
\newblock {\em Remote Sens. Environ.} {\bf 2003}, {\em 87},~310--325, doi:10.1016/j.rse.2003.06.001.

\bibitem[Walthall \em{et~al.}(1994)Walthall, Daughtry, Chappelle, Mcmurtrey,
  and Kim]{walthall1994}
Walthall, C.L.; Daughtry, C.S.T.; Chappelle, E.W.; Mcmurtrey, J.E.; Kim, M.S.
\newblock \emph{The {{Use}} of {{High Spectral Resolution Bands}} for {{Estimating
  Absorbed Photosynthetically Active Radiation}} ({{A Par}})}; { \hl{1994}}.

\bibitem[Carter(1994)]{carter1994}
Carter, G.A.
\newblock Ratios of Leaf Reflectances in Narrow Wavebands as Indicators of
  Plant Stress.
\newblock {\em Int. J. Remote Sens.} {\bf 1994}, {\em
  15},~697--703, doi:10.1080/01431169408954109.

\bibitem[{Zarco-Tejada} \em{et~al.}(2003){Zarco-Tejada}, Pushnik, Dobrowski,
  and Ustin]{zarco-tejada2003}
{Zarco-Tejada}, P.J.; Pushnik, J.C.; Dobrowski, S.; Ustin, S.L.
\newblock Steady-State Chlorophyll a Fluorescence Detection from Canopy
  Derivative Reflectance and Double-Peak Red-Edge Effects.
\newblock {\em Remote Sens. Environ.} {\bf 2003}, {\em 84},~283--294, doi:10.1016/s0034-4257(02)00113-x.

\bibitem[Gitelson \em{et~al.}(2003)Gitelson, Gritz~{\textdagger}, and
  Merzlyak]{gitelson2003}
Gitelson, A.A.; Gritz, Y.; Merzlyak, M.N.
\newblock Relationships between Leaf Chlorophyll Content and Spectral
  Reflectance and Algorithms for Non-Destructive Chlorophyll Assessment in
  Higher Plant Leaves.
\newblock {\em J. Plant Physiol.} {\bf 2003}, {\em 160},~271--282, doi:10.1078/0176-1617-00887.

\bibitem[Oppelt and Mauser(2004)]{oppelt2004}
Oppelt, N.; Mauser, W.
\newblock Hyperspectral Monitoring of Physiological Parameters of Wheat during
  a Vegetation Period Using {{AVIS}} Data.
\newblock {\em Int. J. Remote Sens.} {\bf 2004}, {\em
  25},~145--159, doi:10.1080/0143116031000115300.

\bibitem[Datt(1999)]{datt1999}
Datt, B.
\newblock Visible/near Infrared Reflectance and Chlorophyll Content in
  {{Eucalyptus}} Leaves.
\newblock {\em Int. J. Remote Sens.} {\bf 1999}, {\em
  20},~2741--2759, doi:10.1080/014311699211778.

\bibitem[Datt(1998)]{datt1998}
Datt, B.
\newblock Remote {{Sensing}} of {{Chlorophyll}} a, {{Chlorophyll}} b,
  {{Chlorophyll}} A+b, and {{Total Carotenoid
  Content}} in {{Eucalyptus Leaves}}.
\newblock {\em Remote Sens. Environ.} {\bf 1998}, {\em 66},~111--121, doi:10.1016/s0034-4257(98)00046-7.

\bibitem[Datt(1999)]{datt1999a}
Datt, B.
\newblock Remote {{Sensing}} of {{Water Content}} in {{Eucalyptus Leaves}}.
\newblock {\em Aust. J. Bot.} {\bf 1999}, {\em 47},~909, doi:10.1071/bt98042.

\bibitem[le~Maire \em{et~al.}(2004)le~Maire, Fran{\c c}ois, and
  Dufr{\^e}ne]{maire2004}
le~Maire, G.; Fran{\c c}ois, C.; Dufr{\^e}ne, E.
\newblock Towards Universal Broad Leaf Chlorophyll Indices Using {{PROSPECT}}
  Simulated Database and Hyperspectral Reflectance Measurements.
\newblock {\em Remote Sens. Environ.} {\bf 2004}, {\em 89},~1--28, doi:10.1016/j.rse.2003.09.004.

\bibitem[Lemaire \em{et~al.}(2008)Lemaire, Francois, Soudani, Berveiller,
  Pontailler, Breda, Genet, Davi, and Dufrene]{lemaire2008}
Lemaire, G.; Francois, C.; Soudani, K.; Berveiller, D.; Pontailler, J.; Breda,
  N.; Genet, H.; Davi, H.; Dufrene, E.
\newblock Calibration and Validation of Hyperspectral Indices for the
  Estimation of Broadleaved Forest Leaf Chlorophyll Content, Leaf Mass per
  Area, Leaf Area Index and Leaf Canopy Biomass.
\newblock {\em Remote Sens. Environ.} {\bf 2008}, {\em
  112},~3846--3864, doi:10.1016/j.rse.2008.06.005.

\bibitem[Apan \em{et~al.}(2004)Apan, Held, Phinn, and Markley]{apan2004}
Apan, A.; Held, A.; Phinn, S.; Markley, J.
\newblock Detecting Sugarcane `orange Rust' Disease Using {{EO}}-1 {{Hyperion}}
  Hyperspectral Imagery.
\newblock {\em Int. J. Remote Sens.} {\bf 2004}, {\em
  25},~489--498, doi:10.1080/01431160310001618031.

\bibitem[Pe{\~n}uelas \em{et~al.}(1994)Pe{\~n}uelas, Gamon, Fredeen, Merino,
  and Field]{penuelas1994}
Pe{\~n}uelas, J.; Gamon, J.A.; Fredeen, A.L.; Merino, J.; Field, C.B.
\newblock Reflectance Indices Associated with Physiological Changes in
  Nitrogen- and Water-Limited Sunflower Leaves.
\newblock {\em Remote Sens. Environ.} {\bf 1994}, {\em 48},~135--146, doi:10.1016/0034-4257(94)90136-8.

\bibitem[Huete \em{et~al.}(1997)Huete, Liu, Batchily, and {van
  Leeuwen}]{huete1997}
Huete, A.R.; Liu, H.Q.; Batchily, K.; {van Leeuwen}, W.
\newblock A Comparison of Vegetation Indices over a Global Set of {{TM}} Images
  for {{EOS}}-{{MODIS}}.
\newblock {\em Remote Sens. Environ.} {\bf 1997}, {\em 59},~440--451, doi:10.1016/s0034-4257(96)00112-5.

\bibitem[Wu(2014)]{wu2014}
Wu, W.
\newblock The {{Generalized Difference Vegetation Index}} ({{GDVI}}) for
  {{Dryland Characterization}}.
\newblock {\em Remote Sens.} {\bf 2014}, {\em 6},~1211--1233, doi:10.3390/rs6021211.

\bibitem[Smith \em{et~al.}(1995)Smith, Adams, Stephens, and Hick]{smith1995}
Smith, R.C.G.; Adams, J.; Stephens, D.J.; Hick, P.T.
\newblock Forecasting Wheat Yield in a {{Mediterranean}}-Type Environment from
  the {{NOAA}} Satellite.
\newblock {\em Aust. J. Agric. Res.} {\bf 1995}, {\em
  46},~113, doi:10.1071/ar9950113.

\bibitem[Gitelson \em{et~al.}(1999)Gitelson, Buschmann, and
  Lichtenthaler]{gitelson1999}
Gitelson, A.A.; Buschmann, C.; Lichtenthaler, H.K.
\newblock The {{Chlorophyll Fluorescence Ratio F735}}/{{F700}} as an {{Accurate
  Measure}} of the {{Chlorophyll Content}} in {{Plants}}.
\newblock {\em Remote Sens. Environ.} {\bf 1999}, {\em 69},~296--302, doi:10.1016/s0034-4257(99)00023-1.

\bibitem[Gitelson \em{et~al.}(1996)Gitelson, Kaufman, and
  Merzlyak]{gitelson1996}
Gitelson, A.A.; Kaufman, Y.J.; Merzlyak, M.N.
\newblock Use of a Green Channel in Remote Sensing of Global Vegetation from
  {{EOS}}-{{MODIS}}.
\newblock {\em Remote Sens. Environ.} {\bf 1996}, {\em 58},~289--298, doi:10.1016/s0034-4257(96)00072-7.

\bibitem[Galv{\~a}o \em{et~al.}(2005)Galv{\~a}o, Formaggio, and
  Tisot]{galvao2005}
Galv{\~a}o, L.S.; Formaggio, A.R.; Tisot, D.A.
\newblock Discrimination of Sugarcane Varieties in {{Southeastern Brazil}} with
  {{EO}}-1 {{Hyperion}} Data.
\newblock {\em Remote Sens. Environ.} {\bf 2005}, {\em 94},~523--534, doi:10.1016/j.rse.2004.11.012.

\bibitem[Maccioni \em{et~al.}(2001)Maccioni, Agati, and
  Mazzinghi]{maccioni2001}
Maccioni, A.; Agati, G.; Mazzinghi, P.
\newblock New Vegetation Indices for Remote Measurement of Chlorophylls Based
  on Leaf Directional Reflectance Spectra.
\newblock {\em J. Photochem. Photobiol. B Biol.} {\bf
  2001}, {\em 61},~52--61, doi:10.1016/s1011-1344(01)00145-2.

\bibitem[Daughtry(2000)]{daughtry2000}
Daughtry, C.
\newblock Estimating {{Corn Leaf Chlorophyll Concentration}} from {{Leaf}} and
  {{Canopy Reflectance}}.
\newblock {\em Remote Sens. Environ.} {\bf 2000}, {\em 74},~229--239, doi:10.1016/s0034-4257(00)00113-9.

\bibitem[Wu \em{et~al.}(2008)Wu, Niu, Tang, and Huang]{wu2008}
Wu, C.; Niu, Z.; Tang, Q.; Huang, W.
\newblock Estimating Chlorophyll Content from Hyperspectral Vegetation Indices:
  Modeling and Validation.
\newblock {\em Agric. For. Meteorol.} {\bf 2008}, {\em
  148},~1230--1241, doi:10.1016/j.agrformet.2008.03.005.

\bibitem[Sims and Gamon(2002)]{sims2002}
Sims, D.A.; Gamon, J.A.
\newblock Relationships between Leaf Pigment Content and Spectral Reflectance
  across a Wide Range of Species, Leaf Structures and Developmental Stages.
\newblock {\em Remote Sens. Environ.} {\bf 2002}, {\em 81},~337--354, doi:10.1016/s0034-4257(02)00010-x.

\bibitem[{Hern{\'a}ndez-Clemente} \em{et~al.}(2011){Hern{\'a}ndez-Clemente},
  {Navarro-Cerrillo}, Su{\'a}rez, Morales, and
  {Zarco-Tejada}]{hernandez-clemente2011}
{Hern{\'a}ndez-Clemente}, R.; {Navarro-Cerrillo}, R.M.; Su{\'a}rez, L.;
  Morales, F.; {Zarco-Tejada}, P.J.
\newblock Assessing Structural Effects on {{PRI}} for Stress Detection in
  Conifer Forests.
\newblock {\em Remote Sens. Environ.} {\bf 2011}, {\em
  115},~2360--2375, doi:10.1016/j.rse.2011.04.036.

\bibitem[Qi \em{et~al.}(1994)Qi, Chehbouni, Huete, Kerr, and
  Sorooshian]{qi1994}
Qi, J.; Chehbouni, A.; Huete, A.R.; Kerr, Y.H.; Sorooshian, S.
\newblock A Modified Soil Adjusted Vegetation Index.
\newblock {\em Remote Sens. Environ.} {\bf 1994}, {\em 48},~119--126, doi:10.1016/0034-4257(94)90134-1.

\bibitem[Hunt and Rock(1989)]{hunt1989}
Hunt, E.; Rock, B.
\newblock Detection of Changes in Leaf Water Content Using {{Near}}- and
  {{Middle}}-{{Infrared}} Reflectances.
\newblock {\em Remote Sens. Environ.} {\bf 1989}, {\em 30},~43--54, doi:10.1016/0034-4257(89)90046-1.

\bibitem[Chen(1996)]{chen1996}
Chen, J.M.
\newblock \emph{Evaluation of Vegetation Indices and a {{Modified Simple Ratio}} for
  Boreal Applications};
\newblock Technical Report; {\hl{Natural Resources Canada/ESS/Scientific and
  Technical Publishing Services}}:  1996, doi:10.4095/218303.

\bibitem[Dash and Curran(2007)]{dash2007}
Dash, J.; Curran, P.
\newblock Evaluation of the {{MERIS}} Terrestrial Chlorophyll Index ({{MTCI}}).
\newblock {\em Adv. Space Res.} {\bf 2007}, {\em 39},~100--104, doi:10.1016/j.asr.2006.02.034.

\bibitem[Haboudane \em{et~al.}(2002)Haboudane, Miller, Tremblay,
  {Zarco-Tejada}, and Dextraze]{haboudane2002}
Haboudane, D.; Miller, J.R.; Tremblay, N.; {Zarco-Tejada}, P.J.; Dextraze, L.
\newblock Integrated Narrow-Band Vegetation Indices for Prediction of Crop
  Chlorophyll Content for Application to Precision Agriculture.
\newblock {\em Remote Sens. Environ.} {\bf 2002}, {\em 81},~416--426, doi:10.1016/s0034-4257(02)00018-4.

\bibitem[Serrano \em{et~al.}(2002)Serrano, Pe{\~n}uelas, and
  Ustin]{serrano2002}
Serrano, L.; Pe{\~n}uelas, J.; Ustin, S.L.
\newblock Remote Sensing of Nitrogen and Lignin in {{Mediterranean}} Vegetation
  from {{AVIRIS}} Data.
\newblock {\em Remote Sens. Environ.} {\bf 2002}, {\em 81},~355--364, doi:10.1016/s0034-4257(02)00011-1.

\bibitem[Tucker(1979)]{tucker1979}
Tucker, C.J.
\newblock Red and Photographic Infrared Linear Combinations for Monitoring
  Vegetation.
\newblock {\em Remote Sens. Environ.} {\bf 1979}, {\em 8},~127--150, doi:10.1016/0034-4257(79)90013-0.

\bibitem[Gitelson and Merzlyak(1994)]{gitelson1994}
Gitelson, A.; Merzlyak, M.N.
\newblock Quantitative Estimation of Chlorophyll-a Using Reflectance Spectra:
  Experiments with Autumn Chestnut and Maple Leaves.
\newblock {\em J. Photochem. Photobiol. B Biol.} {\bf
  1994}, {\em 22},~247--252, doi:10.1016/1011-1344(93)06963-4.

\bibitem[Guanter \em{et~al.}(2005)Guanter, Alonso, and Moreno]{guanter2005}
Guanter, L.; Alonso, L.; Moreno, J.
\newblock A Method for the Surface Reflectance Retrieval from
  {{PROBA}}/{{CHRIS}} Data over Land: Application to {{ESA SPARC}} Campaigns.
\newblock {\em IEEE Trans. Geosci. Remote Sens.} {\bf 2005},
  {\em 43},~2908--2917, doi:10.1109/tgrs.2005.857915.

\bibitem[Rondeaux \em{et~al.}(1996)Rondeaux, Steven, and Baret]{rondeaux1996}
Rondeaux, G.; Steven, M.; Baret, F.
\newblock Optimization of Soil-Adjusted Vegetation Indices.
\newblock {\em Remote Sens. Environ.} {\bf 1996}, {\em 55},~95--107, doi:10.1016/0034-4257(95)00186-7.

\bibitem[Chappelle \em{et~al.}(1992)Chappelle, Kim, and
  McMurtrey]{chappelle1992}
Chappelle, E.W.; Kim, M.S.; McMurtrey, J.E.
\newblock Ratio Analysis of Reflectance Spectra ({{RARS}}): An Algorithm for
  the Remote Estimation of the Concentrations of Chlorophyll {{A}}, Chlorophyll
  {{B}}, and Carotenoids in Soybean Leaves.
\newblock {\em Remote Sens. Environ.} {\bf 1992}, {\em 39},~239--247, doi:10.1016/0034-4257(92)90089-3.

\bibitem[Gamon \em{et~al.}(1992)Gamon, Pe{\~n}uelas, and Field]{gamon1992}
Gamon, J.; Pe{\~n}uelas, J.; Field, C.
\newblock A Narrow-Waveband Spectral Index That Tracks Diurnal Changes in
  Photosynthetic Efficiency.
\newblock {\em Remote Sens. Environ.} {\bf 1992}, {\em 41},~35--44, doi:10.1016/0034-4257(92)90059-s.

\bibitem[{Zarco-Tejada} \em{et~al.}(2013){Zarco-Tejada}, {Gonz{\'a}lez-Dugo},
  Williams, Su{\'a}rez, Berni, Goldhamer, and Fereres]{zarco-tejada2013}
{Zarco-Tejada}, P.J.; {Gonz{\'a}lez-Dugo}, V.; Williams, L.E.; Su{\'a}rez, L.;
  Berni, J.A.J.; Goldhamer, D.; Fereres, E.
\newblock A {{PRI}}-Based Water Stress Index Combining Structural and
  Chlorophyll Effects: Assessment Using Diurnal Narrow-Band Airborne Imagery
  and the {{CWSI}} Thermal Index.
\newblock {\em Remote Sens. Environ.} {\bf 2013}, {\em 138},~38--50, doi:10.1016/j.rse.2013.07.024.

\bibitem[Garrity \em{et~al.}(2011)Garrity, Eitel, and Vierling]{garrity2011}
Garrity, S.R.; Eitel, J.U.; Vierling, L.A.
\newblock Disentangling the Relationships between Plant Pigments and the
  Photochemical Reflectance Index Reveals a New Approach for Remote Estimation
  of Carotenoid Content.
\newblock {\em Remote Sens. Environ.} {\bf 2011}, {\em 115},~628--635, doi:10.1016/j.rse.2010.10.007.

\bibitem[Merzlyak \em{et~al.}(1999)Merzlyak, Gitelson, Chivkunova, and
  Rakitin]{merzlyak1999}
Merzlyak, M.N.; Gitelson, A.A.; Chivkunova, O.B.; Rakitin, V.Y.
\newblock Non-Destructive Optical Detection of Pigment Changes during Leaf
  Senescence and Fruit Ripening.
\newblock {\em Physiol. Plant.} {\bf 1999}, {\em 106},~135--141, doi:10.1034/j.1399-3054.1999.106119.x.

\bibitem[Blackburn(1998)]{blackburn1998}
Blackburn, G.A.
\newblock Quantifying Chlorophylls and Caroteniods at Leaf and Canopy Scales.
\newblock {\em Remote Sens. Environ.} {\bf 1998}, {\em 66},~273--285, doi:10.1016/s0034-4257(98)00059-5.

\bibitem[Penuelas \em{et~al.}(1997)Penuelas, Pinol, Ogaya, and
  Filella]{penuelas1997}
Penuelas, J.; Pinol, J.; Ogaya, R.; Filella, I.
\newblock Estimation of Plant Water Concentration by the Reflectance {{Water
  Index WI}} ({{R900}}/{{R970}}).
\newblock {\em Int. J. Remote Sens.} {\bf 1997}, {\em
  18},~2869--2875, doi:10.1080/014311697217396.

\bibitem[Roujean and Breon(1995)]{roujean1995}
Roujean, J.L.; Breon, F.M.
\newblock Estimating {{PAR}} Absorbed by Vegetation from Bidirectional
  Reflectance Measurements.
\newblock {\em Remote Sens. Environ.} {\bf 1995}, {\em 51},~375--384, doi:10.1016/0034-4257(94)00114-3.

\bibitem[Cho and Skidmore(2006)]{cho2006}
Cho, M.A.; Skidmore, A.K.
\newblock A New Technique for Extracting the Red Edge Position from
  Hyperspectral Data: The Linear Extrapolation Method.
\newblock {\em Remote Sens. Environ.} {\bf 2006}, {\em 101},~181--193, doi:10.1016/j.rse.2005.12.011.

\bibitem[Guyot and Baret(1988)]{guyot1988}
Guyot, G.; Baret, F.
\newblock Utilisation de La Haute Resolution Spectrale Pour Suivre l'etat Des
  Couverts Vegetaux.
\newblock  In \emph{Spectral Signatures of Objects in Remote Sensing}; Guyenne, T.D.,
  Hunt, J.J., Eds.;  {\hl{ESA Special Publication}}: 1988; Volume 287, p. 279.

\bibitem[Huete(1988)]{huete1988}
Huete, A.
\newblock A Soil-Adjusted Vegetation Index ({{SAVI}}).
\newblock {\em Remote Sens. Environ.} {\bf 1988}, {\em 25},~295--309, doi:10.1016/0034-4257(88)90106-x.

\bibitem[Penuelas \em{et~al.}(1995)Penuelas, Filella, Lloret, Munoz, and
  Vilaleliu]{penuelasj.1995}
Penuelas, J.; Filella, I.; Lloret, P.; Munoz, F.; Vilaleliu, M.
\newblock Reflectance Assessment of Mite Effects on Apple Trees.
\newblock {\em Int. J. Remote Sens.} {\bf 1995}, {\em
  16},~2727--2733, doi:10.1080/01431169508954588.

\bibitem[Vincini \em{et~al.}(2006)Vincini, Frazzi, and D'Alessio]{vincini2006}
Vincini, M.; Frazzi, E.; D'Alessio, P.
\newblock Angular Dependence of Maize and Sugar Beet {{VIs}} from Directional
  {{CHRIS}}/{{Proba}} Data.
\newblock  In Proceedings of the 4th {{ESA CHRIS PROBA}} Workshop,  \hl{Frascati, Italy, 19--21 September} 2006; Volume 2006, pp.
  19--21.

\bibitem[Jordan(1969)]{jordan1969}
Jordan, C.F.
\newblock Derivation of Leaf-Area Index from Quality of Light on the Forest
  Floor.
\newblock {\em Ecology} {\bf 1969}, {\em 50},~663--666, doi:10.2307/1936256.

\bibitem[Gitelson and Merzlyak(1997)]{gitelson1997}
Gitelson, A.A.; Merzlyak, M.N.
\newblock Remote Estimation of Chlorophyll Content in Higher Plant Leaves.
\newblock {\em Int. J. Remote Sens.} {\bf 1997}, {\em
  18},~2691--2697, doi:10.1080/014311697217558.

\bibitem[McMurtrey \em{et~al.}(1994)McMurtrey, Chappelle, Kim, Meisinger, and
  Corp]{mcmurtrey1994}
McMurtrey, J.; Chappelle, E.; Kim, M.; Meisinger, J.; Corp, L.
\newblock Distinguishing Nitrogen Fertilization Levels in Field Corn (\emph{{{Zea}}
  mays} {{L}}.) with Actively Induced Fluorescence and Passive Reflectance
  Measurements.
\newblock {\em Remote Sens. Environ.} {\bf 1994}, {\em 47},~36--44, doi:10.1016/0034-4257(94)90125-2.

\bibitem[{Zarco-Tejada} and Miller(1999)]{zarco-tejada1999}
{Zarco-Tejada}, P.J.; Miller, J.R.
\newblock Land Cover Mapping at {{BOREAS}} Using Red Edge Spectral Parameters
  from {{CASI}} Imagery.
\newblock {\em J. Geophys. Res. Atmos.} {\bf 1999}, {\em
  104},~27921--27933, doi:10.1029/1999jd900161.

\bibitem[Lichtenthaler \em{et~al.}(1996)Lichtenthaler, Lang, Sowinska, Heisel,
  and Mieh{\'e}]{lichtenthaler1996}
Lichtenthaler, H.; Lang, M.; Sowinska, M.; Heisel, F.; Mieh{\'e}, J.
\newblock Detection of Vegetation Stress via a New High Resolution Fluorescence
  Imaging System.
\newblock {\em J. Plant Physiol.} {\bf 1996}, {\em 148},~599--612, doi:10.1016/s0176-1617(96)80081-2.

\bibitem[{Hern{\'a}ndez-Clemente} \em{et~al.}(2012){Hern{\'a}ndez-Clemente},
  {Navarro-Cerrillo}, and {Zarco-Tejada}]{hernandez-clemente2012}
{Hern{\'a}ndez-Clemente}, R.; {Navarro-Cerrillo}, R.M.; {Zarco-Tejada}, P.J.
\newblock Carotenoid Content Estimation in a Heterogeneous Conifer Forest Using
  Narrow-Band Indices and {{PROSPECTDART}} Simulations.
\newblock {\em Remote Sens. Environ.} {\bf 2012}, {\em 127},~298--315, doi:10.1016/j.rse.2012.09.014.

\bibitem[Elvidge and Chen(1995)]{elvidge1995}
Elvidge, C.D.; Chen, Z.
\newblock Comparison of Broad-Band and Narrow-Band Red and near-Infrared
  Vegetation Indices.
\newblock {\em Remote Sens. Environ.} {\bf 1995}, {\em 54},~38--48, doi:10.1016/0034-4257(95)00132-k.

\bibitem[Filella and Penuelas(1994)]{filella1994}
Filella, I.; Penuelas, J.
\newblock The Red Edge Position and Shape as Indicators of Plant Chlorophyll
  Content, Biomass and Hydric Status.
\newblock {\em Int. J. Remote Sens.} {\bf 1994}, {\em
  15},~1459--1470, doi:10.1080/01431169408954177.

\bibitem[Levin \em{et~al.}(2007)Levin, Kidron, and {Ben-Dor}]{levin2007}
Levin, N.; Kidron, G.J.; {Ben-Dor}, E.
\newblock Surface Properties of Stabilizing Coastal Dunes: Combining Spectral
  and Field Analyses.
\newblock {\em Sedimentology} {\bf 2007}, {\em 54},~771--788, doi:10.1111/j.1365-3091.2007.00859.x.

\bibitem[Lobell \em{et~al.}(2001)Lobell, Asner, Law, and Treuhaft]{lobell2001}
Lobell, D.B.; Asner, G.P.; Law, B.E.; Treuhaft, R.N.
\newblock Subpixel Canopy Cover Estimation of Coniferous Forests in {{Oregon}}
  Using {{SWIR}} Imaging Spectrometry.
\newblock {\em J. Geophys. Res. Atmos.} {\bf 2001}, {\em
  106},~5151--5160, doi:10.1029/2000jd900739.

\bibitem[Hunt \em{et~al.}(2013)Hunt, Doraiswamy, McMurtrey, Daughtry, Perry,
  and Akhmedov]{hunt2013}
Hunt, E.R.; Doraiswamy, P.C.; McMurtrey, J.E.; Daughtry, C.S.; Perry, E.M.;
  Akhmedov, B.
\newblock A Visible Band Index for Remote Sensing Leaf Chlorophyll Content at
  the Canopy Scale.
\newblock {\em Int. J. Appl. Earth Obs. Geoinf.} {\bf 2013}, {\em 21},~103--112, doi:10.1016/j.jag.2012.07.020.

\bibitem[Broge and Leblanc(2001)]{broge2001}
Broge, N.; Leblanc, E.
\newblock Comparing Prediction Power and Stability of Broadband and
  Hyperspectral Vegetation Indices for Estimation of Green Leaf Area Index and
  Canopy Chlorophyll Density.
\newblock {\em Remote Sens. Environ.} {\bf 2001}, {\em 76},~156--172, doi:10.1016/s0034-4257(00)00197-8.

\end{thebibliography}

% If authors have biography, please use the format below
%\section*{Short Biography of Authors}
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{pschratz.jpeg}}}
%{\textbf{Patrick Schratz} is a Data Scientist working in Zurich, Switzerland.
%	His area of expertise is applied machine learning, more specifically the field of environmental modeling.
%	He is a Ph.D. candidate in geographic information science in the Department of Geography at Friedrich Schiller University Jena where he conducts research in environmental modeling.
%	At cynkra GmbH in Zurich, Patrick is an R consultant with many years of experience in the following fields: CI/CD, package development, DevOps tasks, machine learning and spatiotemporal data handling.}
%
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{jmuenchow.jpg}}}
%{\textbf{Jannes Muenchow} is a GIScientist working in tropical ecology since 2007 with a special interest in ENSO, biodiversity, species distribution modeling and predictive mapping.
%	He received his Ph.D. from Friedrich-Alexander-Universität Erlangen-Nürnberg (Germany) in 2013.
%	He joined the business location department of a large consulting company as a geo-data scientist for more than two years until the prediction of a strong El Niño event brought him back to academia from 2016 to 2019 (Friedrich Schiller University Jena).
%	His research focuses on developing open-source tools for ecology, geomorphology and qualitative GIS.
%	He is a co-author of the book ``Geocomputation with R''.}
%
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{eiturritxa.jpg}}}
%{\textbf{Eugenia Iturritxa} received the Ecology and Ph.D. degree in Plant Protection from the University of the Basque Country in 2001.
%	Since 1999 her main research has focused on forest health, on the study of diverse species of native and introduced pathogenic fungi in forests and forest plantations.
%	Her research the NEIKER research center includes the distribution of diseases, analysis of predisposing factors for them, genetic and phenotypic studies of populations and their epidemiology.}
%
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{jcortes.png}}}
%{\textbf{José Cortés} received his B.S. in Mathematics and M.S. in Statistics from Arizona State University, Arizona, USA, in 2014 and 2016, respectively.
%	Currently he is a Ph.D. student at Friedrich Schiller University Jena, Germany.
%	He is a member of the International Max Planck Research School on Global Biogeochemical Cycles (IMPRS-gBGC), a joint program with the Max Planck Institute for Biogeochemistry.
%	His research focuses on spatiotemporal trend detection in environmental data.}
%
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{bbischl.jpg}}}
%{\textbf{Bernd Bischl} obtained his Ph.D. from TU Dortmund University in 2013.
%	He is a professor of Statistical Learning and Data Science in the Department of Statistics at Ludwig-Maximilians-Universität in Munich, Germany, and a director of the Munich Center for Machine Learning.
%	His research interests include AutoML, model selection, interpretable ML and XAI.}
%
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{abrenning.jpg}}}
%{\textbf{Alexander Brenning} (Ph.D.~2005) graduated in mathematics at Technical University of Freiberg, Germany, and received his Ph.D. in geography from Humboldt-Universität zu~Berlin.
%	He served as an assistant professor and tenured associate professor in geomatics at the University of Waterloo, Ontario, Canada from 2007 until 2015, when he was appointed as a full professor in geographic information science at Friedrich Schiller University Jena, Germany.
%	Dr.~Brenning's research interests include statistical and machine-learning techniques for environmental modeling and remote sensing.
%	He has also contributed to open-source geocomputation by developing R packages for spatial cross-validation and GIS coupling.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors’ response\\
%Reviewer 2 comments and authors’ response\\
%Reviewer 3 comments and authors’ response
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

